{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-input syll-output transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqjF58zbxeBa",
        "outputId": "52310b00-da78-4681-8c08-ddcb97b644a5"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfQD7yAgxeBn",
        "outputId": "430e000f-bc55-4f4d-e2c1-58f77286f2e7"
      },
      "source": [
        "# TODO\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NNZisZExeBp"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81DRc6NMxeBq"
      },
      "source": [
        "raw_input_text, _, _= load_verses(\n",
        "    input_file, char_level=False, pad=False\n",
        ")\n",
        "raw_target_text, _, _ = load_verses(\n",
        "    target_file, char_level=False, pad=False\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gbbqK58xeBq"
      },
      "source": [
        "def preprocess_target(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace(' ', ' <SEP> ')\n",
        "    x = x.replace('|', ' <SYL> ')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCENLJWxeBr"
      },
      "source": [
        "input_text = [verse.strip() for verse in raw_input_text.split('\\n') if verse.strip() != '']\n",
        "input_text = list(map(preprocess_input, input_text))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvY5HrwqxeBr"
      },
      "source": [
        "target_text = [verse.strip() for verse in raw_target_text.split('\\n') if verse.strip() != '']\n",
        "target_text = list(map(preprocess_target, target_text))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPkwKFkcxeBt"
      },
      "source": [
        "input_tercets = []\n",
        "target_tercets = []\n",
        "\n",
        "for line in range(len(input_text) - 6):\n",
        "    input_tercets.append(' '.join(input_text[line:line+3]) + ' <EOT>')\n",
        "    target_tercets.append(' '.join(target_text[line+3:line+6]) + ' <EOT>')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N37AvekuxeBv"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "input_tokenizer.fit_on_texts(input_tercets)\n",
        "input_text = input_tokenizer.texts_to_sequences(input_tercets)\n",
        "\n",
        "input_vocab = set(input_tokenizer.word_index.keys())\n",
        "input_vocab_size = len(input_vocab) + 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDeugXXJxeBw"
      },
      "source": [
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "target_tokenizer.fit_on_texts(target_tercets)\n",
        "target_text = target_tokenizer.texts_to_sequences(target_tercets)\n",
        "\n",
        "target_vocab = set(target_tokenizer.word_index.keys())\n",
        "target_vocab_size = len(target_vocab) + 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEE2bQTVxeBw"
      },
      "source": [
        "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_text, padding=\"post\"\n",
        ")\n",
        "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_text, padding=\"post\"\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujuvEQLrxeBw"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input, padded_target\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AGR1mb_xeBy"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/word-syll-gen\"\n",
        "\n",
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=checkpoint_path\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQxxCMlxeBz",
        "outputId": "146b9e4b-adce-45cc-d203-3f1b612c0567"
      },
      "source": [
        "transformer_trainer.train(dataset, EPOCHS)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.2300 Accuracy 0.9242\n",
            "Epoch 1 Batch 50 Loss 0.2473 Accuracy 0.9217\n",
            "Epoch 1 Batch 100 Loss 0.2549 Accuracy 0.9194\n",
            "Epoch 1 Batch 150 Loss 0.2612 Accuracy 0.9175\n",
            "Epoch 1 Loss 0.2628 Accuracy 0.9170\n",
            "Time taken for 1 epoch: 79.60 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2285 Accuracy 0.9265\n",
            "Epoch 2 Batch 50 Loss 0.2364 Accuracy 0.9249\n",
            "Epoch 2 Batch 100 Loss 0.2456 Accuracy 0.9220\n",
            "Epoch 2 Batch 150 Loss 0.2510 Accuracy 0.9204\n",
            "Epoch 2 Loss 0.2530 Accuracy 0.9199\n",
            "Time taken for 1 epoch: 79.61 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2260 Accuracy 0.9280\n",
            "Epoch 3 Batch 50 Loss 0.2312 Accuracy 0.9270\n",
            "Epoch 3 Batch 100 Loss 0.2379 Accuracy 0.9247\n",
            "Epoch 3 Batch 150 Loss 0.2435 Accuracy 0.9229\n",
            "Epoch 3 Loss 0.2453 Accuracy 0.9223\n",
            "Time taken for 1 epoch: 79.12 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2040 Accuracy 0.9373\n",
            "Epoch 4 Batch 50 Loss 0.2222 Accuracy 0.9290\n",
            "Epoch 4 Batch 100 Loss 0.2298 Accuracy 0.9268\n",
            "Epoch 4 Batch 150 Loss 0.2354 Accuracy 0.9251\n",
            "Epoch 4 Loss 0.2371 Accuracy 0.9246\n",
            "Time taken for 1 epoch: 79.47 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2094 Accuracy 0.9318\n",
            "Epoch 5 Batch 50 Loss 0.2150 Accuracy 0.9318\n",
            "Epoch 5 Batch 100 Loss 0.2233 Accuracy 0.9288\n",
            "Epoch 5 Batch 150 Loss 0.2299 Accuracy 0.9268\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-syll-gen/ckpt-11\n",
            "Epoch 5 Loss 0.2318 Accuracy 0.9263\n",
            "Time taken for 1 epoch: 79.60 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2051 Accuracy 0.9336\n",
            "Epoch 6 Batch 50 Loss 0.2074 Accuracy 0.9335\n",
            "Epoch 6 Batch 100 Loss 0.2142 Accuracy 0.9314\n",
            "Epoch 6 Batch 150 Loss 0.2208 Accuracy 0.9295\n",
            "Epoch 6 Loss 0.2223 Accuracy 0.9290\n",
            "Time taken for 1 epoch: 79.56 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2000 Accuracy 0.9358\n",
            "Epoch 7 Batch 50 Loss 0.2036 Accuracy 0.9349\n",
            "Epoch 7 Batch 100 Loss 0.2102 Accuracy 0.9328\n",
            "Epoch 7 Batch 150 Loss 0.2158 Accuracy 0.9310\n",
            "Epoch 7 Loss 0.2169 Accuracy 0.9307\n",
            "Time taken for 1 epoch: 79.24 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2023 Accuracy 0.9347\n",
            "Epoch 8 Batch 50 Loss 0.2004 Accuracy 0.9358\n",
            "Epoch 8 Batch 100 Loss 0.2053 Accuracy 0.9341\n",
            "Epoch 8 Batch 150 Loss 0.2100 Accuracy 0.9326\n",
            "Epoch 8 Loss 0.2111 Accuracy 0.9323\n",
            "Time taken for 1 epoch: 79.51 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1847 Accuracy 0.9414\n",
            "Epoch 9 Batch 50 Loss 0.1929 Accuracy 0.9379\n",
            "Epoch 9 Batch 100 Loss 0.1996 Accuracy 0.9358\n",
            "Epoch 9 Batch 150 Loss 0.2041 Accuracy 0.9345\n",
            "Epoch 9 Loss 0.2052 Accuracy 0.9340\n",
            "Time taken for 1 epoch: 79.17 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1839 Accuracy 0.9427\n",
            "Epoch 10 Batch 50 Loss 0.1903 Accuracy 0.9389\n",
            "Epoch 10 Batch 100 Loss 0.1943 Accuracy 0.9377\n",
            "Epoch 10 Batch 150 Loss 0.1990 Accuracy 0.9362\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-syll-gen/ckpt-12\n",
            "Epoch 10 Loss 0.2003 Accuracy 0.9359\n",
            "Time taken for 1 epoch: 80.12 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.1842 Accuracy 0.9430\n",
            "Epoch 11 Batch 50 Loss 0.1827 Accuracy 0.9413\n",
            "Epoch 11 Batch 100 Loss 0.1883 Accuracy 0.9395\n",
            "Epoch 11 Batch 150 Loss 0.1933 Accuracy 0.9378\n",
            "Epoch 11 Loss 0.1946 Accuracy 0.9374\n",
            "Time taken for 1 epoch: 79.32 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1809 Accuracy 0.9413\n",
            "Epoch 12 Batch 50 Loss 0.1779 Accuracy 0.9426\n",
            "Epoch 12 Batch 100 Loss 0.1838 Accuracy 0.9407\n",
            "Epoch 12 Batch 150 Loss 0.1876 Accuracy 0.9398\n",
            "Epoch 12 Loss 0.1892 Accuracy 0.9393\n",
            "Time taken for 1 epoch: 79.54 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1739 Accuracy 0.9447\n",
            "Epoch 13 Batch 50 Loss 0.1741 Accuracy 0.9438\n",
            "Epoch 13 Batch 100 Loss 0.1790 Accuracy 0.9422\n",
            "Epoch 13 Batch 150 Loss 0.1834 Accuracy 0.9408\n",
            "Epoch 13 Loss 0.1842 Accuracy 0.9406\n",
            "Time taken for 1 epoch: 78.80 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1655 Accuracy 0.9457\n",
            "Epoch 14 Batch 50 Loss 0.1674 Accuracy 0.9461\n",
            "Epoch 14 Batch 100 Loss 0.1744 Accuracy 0.9438\n",
            "Epoch 14 Batch 150 Loss 0.1783 Accuracy 0.9425\n",
            "Epoch 14 Loss 0.1793 Accuracy 0.9422\n",
            "Time taken for 1 epoch: 79.38 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1537 Accuracy 0.9484\n",
            "Epoch 15 Batch 50 Loss 0.1664 Accuracy 0.9462\n",
            "Epoch 15 Batch 100 Loss 0.1712 Accuracy 0.9446\n",
            "Epoch 15 Batch 150 Loss 0.1749 Accuracy 0.9436\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/word-syll-gen/ckpt-13\n",
            "Epoch 15 Loss 0.1763 Accuracy 0.9431\n",
            "Time taken for 1 epoch: 79.84 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1604 Accuracy 0.9481\n",
            "Epoch 16 Batch 50 Loss 0.1606 Accuracy 0.9481\n",
            "Epoch 16 Batch 100 Loss 0.1650 Accuracy 0.9466\n",
            "Epoch 16 Batch 150 Loss 0.1694 Accuracy 0.9453\n",
            "Epoch 16 Loss 0.1709 Accuracy 0.9448\n",
            "Time taken for 1 epoch: 79.45 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1704 Accuracy 0.9464\n",
            "Epoch 17 Batch 50 Loss 0.1601 Accuracy 0.9482\n",
            "Epoch 17 Batch 100 Loss 0.1621 Accuracy 0.9473\n",
            "Epoch 17 Batch 150 Loss 0.1667 Accuracy 0.9459\n",
            "Epoch 17 Loss 0.1681 Accuracy 0.9455\n",
            "Time taken for 1 epoch: 79.19 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1543 Accuracy 0.9505\n",
            "Epoch 18 Batch 50 Loss 0.1534 Accuracy 0.9500\n",
            "Epoch 18 Batch 100 Loss 0.1589 Accuracy 0.9485\n",
            "Epoch 18 Batch 150 Loss 0.1629 Accuracy 0.9472\n",
            "Epoch 18 Loss 0.1639 Accuracy 0.9469\n",
            "Time taken for 1 epoch: 79.50 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1455 Accuracy 0.9527\n",
            "Epoch 19 Batch 50 Loss 0.1498 Accuracy 0.9517\n",
            "Epoch 19 Batch 100 Loss 0.1552 Accuracy 0.9498\n",
            "Epoch 19 Batch 150 Loss 0.1588 Accuracy 0.9485\n",
            "Epoch 19 Loss 0.1600 Accuracy 0.9481\n",
            "Time taken for 1 epoch: 78.88 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1413 Accuracy 0.9520\n",
            "Epoch 20 Batch 50 Loss 0.1468 Accuracy 0.9524\n",
            "Epoch 20 Batch 100 Loss 0.1522 Accuracy 0.9504\n",
            "Epoch 20 Batch 150 Loss 0.1556 Accuracy 0.9493\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/word-syll-gen/ckpt-14\n",
            "Epoch 20 Loss 0.1564 Accuracy 0.9490\n",
            "Time taken for 1 epoch: 79.78 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1446 Accuracy 0.9531\n",
            "Epoch 21 Batch 50 Loss 0.1436 Accuracy 0.9532\n",
            "Epoch 21 Batch 100 Loss 0.1480 Accuracy 0.9519\n",
            "Epoch 21 Batch 150 Loss 0.1524 Accuracy 0.9505\n",
            "Epoch 21 Loss 0.1530 Accuracy 0.9503\n",
            "Time taken for 1 epoch: 79.02 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1461 Accuracy 0.9523\n",
            "Epoch 22 Batch 50 Loss 0.1427 Accuracy 0.9539\n",
            "Epoch 22 Batch 100 Loss 0.1463 Accuracy 0.9524\n",
            "Epoch 22 Batch 150 Loss 0.1499 Accuracy 0.9513\n",
            "Epoch 22 Loss 0.1507 Accuracy 0.9510\n",
            "Time taken for 1 epoch: 79.42 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1345 Accuracy 0.9553\n",
            "Epoch 23 Batch 50 Loss 0.1364 Accuracy 0.9558\n",
            "Epoch 23 Batch 100 Loss 0.1420 Accuracy 0.9537\n",
            "Epoch 23 Batch 150 Loss 0.1463 Accuracy 0.9524\n",
            "Epoch 23 Loss 0.1475 Accuracy 0.9520\n",
            "Time taken for 1 epoch: 79.20 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1269 Accuracy 0.9571\n",
            "Epoch 24 Batch 50 Loss 0.1361 Accuracy 0.9558\n",
            "Epoch 24 Batch 100 Loss 0.1392 Accuracy 0.9548\n",
            "Epoch 24 Batch 150 Loss 0.1425 Accuracy 0.9536\n",
            "Epoch 24 Loss 0.1436 Accuracy 0.9532\n",
            "Time taken for 1 epoch: 79.54 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1301 Accuracy 0.9580\n",
            "Epoch 25 Batch 50 Loss 0.1344 Accuracy 0.9563\n",
            "Epoch 25 Batch 100 Loss 0.1376 Accuracy 0.9553\n",
            "Epoch 25 Batch 150 Loss 0.1403 Accuracy 0.9545\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/word-syll-gen/ckpt-15\n",
            "Epoch 25 Loss 0.1412 Accuracy 0.9541\n",
            "Time taken for 1 epoch: 79.71 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.1394 Accuracy 0.9559\n",
            "Epoch 26 Batch 50 Loss 0.1288 Accuracy 0.9578\n",
            "Epoch 26 Batch 100 Loss 0.1336 Accuracy 0.9565\n",
            "Epoch 26 Batch 150 Loss 0.1369 Accuracy 0.9557\n",
            "Epoch 26 Loss 0.1371 Accuracy 0.9556\n",
            "Time taken for 1 epoch: 79.25 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1212 Accuracy 0.9603\n",
            "Epoch 27 Batch 50 Loss 0.1266 Accuracy 0.9589\n",
            "Epoch 27 Batch 100 Loss 0.1308 Accuracy 0.9575\n",
            "Epoch 27 Batch 150 Loss 0.1343 Accuracy 0.9564\n",
            "Epoch 27 Loss 0.1352 Accuracy 0.9561\n",
            "Time taken for 1 epoch: 78.91 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1184 Accuracy 0.9604\n",
            "Epoch 28 Batch 50 Loss 0.1242 Accuracy 0.9595\n",
            "Epoch 28 Batch 100 Loss 0.1282 Accuracy 0.9583\n",
            "Epoch 28 Batch 150 Loss 0.1317 Accuracy 0.9571\n",
            "Epoch 28 Loss 0.1324 Accuracy 0.9569\n",
            "Time taken for 1 epoch: 79.47 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.1188 Accuracy 0.9603\n",
            "Epoch 29 Batch 50 Loss 0.1233 Accuracy 0.9603\n",
            "Epoch 29 Batch 100 Loss 0.1277 Accuracy 0.9587\n",
            "Epoch 29 Batch 150 Loss 0.1302 Accuracy 0.9578\n",
            "Epoch 29 Loss 0.1307 Accuracy 0.9575\n",
            "Time taken for 1 epoch: 79.05 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.1168 Accuracy 0.9631\n",
            "Epoch 30 Batch 50 Loss 0.1239 Accuracy 0.9596\n",
            "Epoch 30 Batch 100 Loss 0.1254 Accuracy 0.9591\n",
            "Epoch 30 Batch 150 Loss 0.1282 Accuracy 0.9582\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/word-syll-gen/ckpt-16\n",
            "Epoch 30 Loss 0.1291 Accuracy 0.9579\n",
            "Time taken for 1 epoch: 79.90 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.1139 Accuracy 0.9624\n",
            "Epoch 31 Batch 50 Loss 0.1190 Accuracy 0.9609\n",
            "Epoch 31 Batch 100 Loss 0.1213 Accuracy 0.9604\n",
            "Epoch 31 Batch 150 Loss 0.1245 Accuracy 0.9594\n",
            "Epoch 31 Loss 0.1253 Accuracy 0.9593\n",
            "Time taken for 1 epoch: 78.93 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.1100 Accuracy 0.9635\n",
            "Epoch 32 Batch 50 Loss 0.1181 Accuracy 0.9612\n",
            "Epoch 32 Batch 100 Loss 0.1217 Accuracy 0.9602\n",
            "Epoch 32 Batch 150 Loss 0.1245 Accuracy 0.9593\n",
            "Epoch 32 Loss 0.1252 Accuracy 0.9591\n",
            "Time taken for 1 epoch: 79.21 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.1155 Accuracy 0.9630\n",
            "Epoch 33 Batch 50 Loss 0.1140 Accuracy 0.9630\n",
            "Epoch 33 Batch 100 Loss 0.1184 Accuracy 0.9617\n",
            "Epoch 33 Batch 150 Loss 0.1215 Accuracy 0.9606\n",
            "Epoch 33 Loss 0.1222 Accuracy 0.9604\n",
            "Time taken for 1 epoch: 79.10 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.1071 Accuracy 0.9672\n",
            "Epoch 34 Batch 50 Loss 0.1136 Accuracy 0.9630\n",
            "Epoch 34 Batch 100 Loss 0.1161 Accuracy 0.9622\n",
            "Epoch 34 Batch 150 Loss 0.1187 Accuracy 0.9614\n",
            "Epoch 34 Loss 0.1192 Accuracy 0.9612\n",
            "Time taken for 1 epoch: 79.06 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.1200 Accuracy 0.9636\n",
            "Epoch 35 Batch 50 Loss 0.1121 Accuracy 0.9633\n",
            "Epoch 35 Batch 100 Loss 0.1135 Accuracy 0.9630\n",
            "Epoch 35 Batch 150 Loss 0.1173 Accuracy 0.9617\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/word-syll-gen/ckpt-17\n",
            "Epoch 35 Loss 0.1181 Accuracy 0.9615\n",
            "Time taken for 1 epoch: 79.24 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0992 Accuracy 0.9651\n",
            "Epoch 36 Batch 50 Loss 0.1098 Accuracy 0.9642\n",
            "Epoch 36 Batch 100 Loss 0.1118 Accuracy 0.9635\n",
            "Epoch 36 Batch 150 Loss 0.1144 Accuracy 0.9627\n",
            "Epoch 36 Loss 0.1152 Accuracy 0.9624\n",
            "Time taken for 1 epoch: 78.51 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.1104 Accuracy 0.9642\n",
            "Epoch 37 Batch 50 Loss 0.1074 Accuracy 0.9646\n",
            "Epoch 37 Batch 100 Loss 0.1106 Accuracy 0.9637\n",
            "Epoch 37 Batch 150 Loss 0.1133 Accuracy 0.9629\n",
            "Epoch 37 Loss 0.1146 Accuracy 0.9626\n",
            "Time taken for 1 epoch: 78.17 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.1041 Accuracy 0.9669\n",
            "Epoch 38 Batch 50 Loss 0.1055 Accuracy 0.9656\n",
            "Epoch 38 Batch 100 Loss 0.1080 Accuracy 0.9648\n",
            "Epoch 38 Batch 150 Loss 0.1111 Accuracy 0.9639\n",
            "Epoch 38 Loss 0.1116 Accuracy 0.9637\n",
            "Time taken for 1 epoch: 78.59 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0968 Accuracy 0.9680\n",
            "Epoch 39 Batch 50 Loss 0.1036 Accuracy 0.9660\n",
            "Epoch 39 Batch 100 Loss 0.1071 Accuracy 0.9648\n",
            "Epoch 39 Batch 150 Loss 0.1094 Accuracy 0.9642\n",
            "Epoch 39 Loss 0.1102 Accuracy 0.9639\n",
            "Time taken for 1 epoch: 78.21 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.1018 Accuracy 0.9668\n",
            "Epoch 40 Batch 50 Loss 0.1030 Accuracy 0.9662\n",
            "Epoch 40 Batch 100 Loss 0.1052 Accuracy 0.9656\n",
            "Epoch 40 Batch 150 Loss 0.1079 Accuracy 0.9646\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/word-syll-gen/ckpt-18\n",
            "Epoch 40 Loss 0.1088 Accuracy 0.9644\n",
            "Time taken for 1 epoch: 78.89 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.1073 Accuracy 0.9669\n",
            "Epoch 41 Batch 50 Loss 0.1018 Accuracy 0.9666\n",
            "Epoch 41 Batch 100 Loss 0.1041 Accuracy 0.9660\n",
            "Epoch 41 Batch 150 Loss 0.1066 Accuracy 0.9651\n",
            "Epoch 41 Loss 0.1074 Accuracy 0.9649\n",
            "Time taken for 1 epoch: 78.16 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0948 Accuracy 0.9700\n",
            "Epoch 42 Batch 50 Loss 0.1019 Accuracy 0.9671\n",
            "Epoch 42 Batch 100 Loss 0.1029 Accuracy 0.9665\n",
            "Epoch 42 Batch 150 Loss 0.1048 Accuracy 0.9659\n",
            "Epoch 42 Loss 0.1057 Accuracy 0.9656\n",
            "Time taken for 1 epoch: 78.44 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.1000 Accuracy 0.9669\n",
            "Epoch 43 Batch 50 Loss 0.0964 Accuracy 0.9683\n",
            "Epoch 43 Batch 100 Loss 0.1002 Accuracy 0.9672\n",
            "Epoch 43 Batch 150 Loss 0.1023 Accuracy 0.9665\n",
            "Epoch 43 Loss 0.1032 Accuracy 0.9662\n",
            "Time taken for 1 epoch: 78.35 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0981 Accuracy 0.9666\n",
            "Epoch 44 Batch 50 Loss 0.0969 Accuracy 0.9682\n",
            "Epoch 44 Batch 100 Loss 0.0991 Accuracy 0.9674\n",
            "Epoch 44 Batch 150 Loss 0.1017 Accuracy 0.9667\n",
            "Epoch 44 Loss 0.1024 Accuracy 0.9665\n",
            "Time taken for 1 epoch: 78.57 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0891 Accuracy 0.9701\n",
            "Epoch 45 Batch 50 Loss 0.0954 Accuracy 0.9688\n",
            "Epoch 45 Batch 100 Loss 0.0981 Accuracy 0.9680\n",
            "Epoch 45 Batch 150 Loss 0.1000 Accuracy 0.9673\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/word-syll-gen/ckpt-19\n",
            "Epoch 45 Loss 0.1006 Accuracy 0.9671\n",
            "Time taken for 1 epoch: 78.86 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0864 Accuracy 0.9711\n",
            "Epoch 46 Batch 50 Loss 0.0935 Accuracy 0.9693\n",
            "Epoch 46 Batch 100 Loss 0.0968 Accuracy 0.9683\n",
            "Epoch 46 Batch 150 Loss 0.0980 Accuracy 0.9679\n",
            "Epoch 46 Loss 0.0983 Accuracy 0.9678\n",
            "Time taken for 1 epoch: 78.57 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0883 Accuracy 0.9715\n",
            "Epoch 47 Batch 50 Loss 0.0928 Accuracy 0.9695\n",
            "Epoch 47 Batch 100 Loss 0.0960 Accuracy 0.9686\n",
            "Epoch 47 Batch 150 Loss 0.0982 Accuracy 0.9679\n",
            "Epoch 47 Loss 0.0987 Accuracy 0.9676\n",
            "Time taken for 1 epoch: 78.28 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0829 Accuracy 0.9742\n",
            "Epoch 48 Batch 50 Loss 0.0921 Accuracy 0.9698\n",
            "Epoch 48 Batch 100 Loss 0.0941 Accuracy 0.9692\n",
            "Epoch 48 Batch 150 Loss 0.0957 Accuracy 0.9687\n",
            "Epoch 48 Loss 0.0963 Accuracy 0.9685\n",
            "Time taken for 1 epoch: 78.61 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0806 Accuracy 0.9756\n",
            "Epoch 49 Batch 50 Loss 0.0913 Accuracy 0.9705\n",
            "Epoch 49 Batch 100 Loss 0.0928 Accuracy 0.9698\n",
            "Epoch 49 Batch 150 Loss 0.0952 Accuracy 0.9690\n",
            "Epoch 49 Loss 0.0955 Accuracy 0.9689\n",
            "Time taken for 1 epoch: 78.14 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.1084 Accuracy 0.9674\n",
            "Epoch 50 Batch 50 Loss 0.0887 Accuracy 0.9709\n",
            "Epoch 50 Batch 100 Loss 0.0900 Accuracy 0.9704\n",
            "Epoch 50 Batch 150 Loss 0.0928 Accuracy 0.9696\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/word-syll-gen/ckpt-20\n",
            "Epoch 50 Loss 0.0939 Accuracy 0.9693\n",
            "Time taken for 1 epoch: 78.96 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "TODO change this :)\n",
        "\n",
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVhkqi-_D1nQ"
      },
      "source": [
        "def postprocess(x):\n",
        "    x = x.replace('<SEP>', ' ')\n",
        "    x = x.replace('<SYL>', '|')\n",
        "    x = x.replace(' <GO> ', '\\n')\n",
        "    return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "0ec136fd-d4a5-4552-dc90-7929b99eb566"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> che <SEP> <SYL> di <SEP> <SYL> que <SYL> sta <SEP> <SYL> fie <SYL> ra <SEP> <SYL> si <SEP> <SYL> di <SYL> ser <SYL> ra <SEP> . <SEP> <GO> <SYL> Quan <SYL> do <SEP> <SYL> li <SEP> <SYL> pie <SYL> di <SEP> <SYL> suoi <SEP> <SYL> re <SYL> scal <SYL> zi <SEP> <SYL> tol <SYL> si <SEP> , <SEP> <GO> <SYL> co <SYL> me <SEP> <SYL> l <SEP> ’ <SEP> <SEP> ul <SYL> ti <SYL> ma <SEP> <SYL> not <SYL> te <SEP> <SYL> già <SEP> <SYL> si <SEP> <SYL> mi <SYL> nac <SYL> cia <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usetPrX-EvPl",
        "outputId": "273977fc-1d4b-459f-c00f-714b5e8076a9"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| e   | che   | di   | que | sta   | fie | ra   | si   | di | ser | ra   .  \n",
            "| Quan | do   | li   | pie | di   | suoi   | re | scal | zi   | tol | si   ,  \n",
            "| co | me   | l   ’     ul | ti | ma   | not | te   | già   | si   | mi | nac | cia <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() #qui potrebbe anche essere [0,0]\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "744cb041-8433-4b18-c92f-d7c5fb51982e"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> de <SEP> <SYL> li <SEP> al <SYL> tri <SEP> <SYL> miei <SEP> <SYL> si <SEP> <SYL> vol <SYL> ge <SEP> il <SEP> <SYL> ge <SYL> lo <SYL> ne <GO> <SYL> che <SEP> <SYL> l <SEP> ’ <SEP> <SEP> a <SYL> ni <SYL> mo <SEP> <SYL> di <SEP> <SYL> Dio <SEP> <SYL> ri <SYL> tor <SYL> nar <SEP> <SYL> più <SEP> <SYL> fe <SYL> de <SEP> . <SEP> <GO> <SYL> Que <SYL> sti <SEP> <SYL> pa <SYL> rea <SEP> <SYL> tra <SEP> <SYL> Dio <SEP> <SYL> a <SEP> <SYL> Ga <SYL> le <SYL> on <SYL> ti <SEP> , <SEP> <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBrcICi_F1P9",
        "outputId": "cd147e6c-50dd-48b3-c904-113ae6a1d8f7"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| e   | de   | li   al | tri   | miei   | si   | vol | ge   il   | ge | lo | ne\n",
            "| che   | l   ’     a | ni | mo   | di   | Dio   | ri | tor | nar   | più   | fe | de   .  \n",
            "| Que | sti   | pa | rea   | tra   | Dio   | a   | Ga | le | on | ti   ,   <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}