{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-input syll-output transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqjF58zbxeBa",
        "outputId": "4dc3efd7-75bd-47e0-e5bc-7b1962eb3cb4"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfQD7yAgxeBn",
        "outputId": "d29b8dca-e47a-4927-ce94-ca304b316a8d"
      },
      "source": [
        "# TODO\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NNZisZExeBp"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81DRc6NMxeBq"
      },
      "source": [
        "raw_input_text, _, _= load_verses(\n",
        "    input_file, char_level=False, pad=False\n",
        ")\n",
        "raw_target_text, _, _ = load_verses(\n",
        "    target_file, char_level=False, pad=False\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gbbqK58xeBq"
      },
      "source": [
        "def preprocess_target(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace(' ', ' <SEP> ')\n",
        "    x = x.replace('|', ' <SYL> ')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCENLJWxeBr"
      },
      "source": [
        "input_text = [verse.strip() for verse in raw_input_text.split('\\n') if verse.strip() != '']\n",
        "input_text = list(map(preprocess_input, input_text))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvY5HrwqxeBr"
      },
      "source": [
        "target_text = [verse.strip() for verse in raw_target_text.split('\\n') if verse.strip() != '']\n",
        "target_text = list(map(preprocess_target, target_text))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPkwKFkcxeBt"
      },
      "source": [
        "input_tercets = []\n",
        "target_tercets = []\n",
        "\n",
        "for line in range(len(input_text) - 6):\n",
        "    input_tercets.append(' '.join(input_text[line:line+3]) + ' <EOT>')\n",
        "    target_tercets.append(' '.join(target_text[line+3:line+6]) + ' <EOT>')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N37AvekuxeBv"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "input_tokenizer.fit_on_texts(input_tercets)\n",
        "input_text = input_tokenizer.texts_to_sequences(input_tercets)\n",
        "\n",
        "input_vocab = set(input_tokenizer.word_index.keys())\n",
        "input_vocab_size = len(input_vocab) + 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDeugXXJxeBw"
      },
      "source": [
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "target_tokenizer.fit_on_texts(target_tercets)\n",
        "target_text = target_tokenizer.texts_to_sequences(target_tercets)\n",
        "\n",
        "target_vocab = set(target_tokenizer.word_index.keys())\n",
        "target_vocab_size = len(target_vocab) + 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEE2bQTVxeBw"
      },
      "source": [
        "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_text, padding=\"post\"\n",
        ")\n",
        "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_text, padding=\"post\"\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujuvEQLrxeBw"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input, padded_target\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AGR1mb_xeBy"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/word-syll-gen\"\n",
        "\n",
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=checkpoint_path\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQxxCMlxeBz",
        "outputId": "eacf26a0-4a0b-4289-d7ba-39fe7aafc0a3"
      },
      "source": [
        "transformer_trainer.train(dataset, EPOCHS)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0858 Accuracy 0.9725\n",
            "Epoch 1 Batch 50 Loss 0.0840 Accuracy 0.9726\n",
            "Epoch 1 Batch 100 Loss 0.0870 Accuracy 0.9715\n",
            "Epoch 1 Batch 150 Loss 0.0897 Accuracy 0.9706\n",
            "Epoch 1 Loss 0.0904 Accuracy 0.9704\n",
            "Time taken for 1 epoch: 36.26 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0925 Accuracy 0.9688\n",
            "Epoch 2 Batch 50 Loss 0.0838 Accuracy 0.9722\n",
            "Epoch 2 Batch 100 Loss 0.0859 Accuracy 0.9717\n",
            "Epoch 2 Batch 150 Loss 0.0870 Accuracy 0.9713\n",
            "Epoch 2 Loss 0.0877 Accuracy 0.9712\n",
            "Time taken for 1 epoch: 36.05 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0946 Accuracy 0.9698\n",
            "Epoch 3 Batch 50 Loss 0.0831 Accuracy 0.9729\n",
            "Epoch 3 Batch 100 Loss 0.0841 Accuracy 0.9724\n",
            "Epoch 3 Batch 150 Loss 0.0859 Accuracy 0.9719\n",
            "Epoch 3 Loss 0.0867 Accuracy 0.9716\n",
            "Time taken for 1 epoch: 35.82 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0849 Accuracy 0.9704\n",
            "Epoch 4 Batch 50 Loss 0.0826 Accuracy 0.9732\n",
            "Epoch 4 Batch 100 Loss 0.0842 Accuracy 0.9726\n",
            "Epoch 4 Batch 150 Loss 0.0859 Accuracy 0.9721\n",
            "Epoch 4 Loss 0.0864 Accuracy 0.9719\n",
            "Time taken for 1 epoch: 36.05 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0838 Accuracy 0.9738\n",
            "Epoch 5 Batch 50 Loss 0.0802 Accuracy 0.9739\n",
            "Epoch 5 Batch 100 Loss 0.0817 Accuracy 0.9732\n",
            "Epoch 5 Batch 150 Loss 0.0839 Accuracy 0.9725\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-syll-gen/ckpt-21\n",
            "Epoch 5 Loss 0.0844 Accuracy 0.9724\n",
            "Time taken for 1 epoch: 36.32 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0662 Accuracy 0.9768\n",
            "Epoch 6 Batch 50 Loss 0.0792 Accuracy 0.9741\n",
            "Epoch 6 Batch 100 Loss 0.0806 Accuracy 0.9737\n",
            "Epoch 6 Batch 150 Loss 0.0832 Accuracy 0.9728\n",
            "Epoch 6 Loss 0.0839 Accuracy 0.9725\n",
            "Time taken for 1 epoch: 36.00 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0717 Accuracy 0.9757\n",
            "Epoch 7 Batch 50 Loss 0.0772 Accuracy 0.9745\n",
            "Epoch 7 Batch 100 Loss 0.0802 Accuracy 0.9737\n",
            "Epoch 7 Batch 150 Loss 0.0821 Accuracy 0.9730\n",
            "Epoch 7 Loss 0.0826 Accuracy 0.9729\n",
            "Time taken for 1 epoch: 36.04 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0822 Accuracy 0.9733\n",
            "Epoch 8 Batch 50 Loss 0.0780 Accuracy 0.9744\n",
            "Epoch 8 Batch 100 Loss 0.0795 Accuracy 0.9739\n",
            "Epoch 8 Batch 150 Loss 0.0810 Accuracy 0.9735\n",
            "Epoch 8 Loss 0.0815 Accuracy 0.9733\n",
            "Time taken for 1 epoch: 35.96 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0685 Accuracy 0.9802\n",
            "Epoch 9 Batch 50 Loss 0.0760 Accuracy 0.9751\n",
            "Epoch 9 Batch 100 Loss 0.0789 Accuracy 0.9743\n",
            "Epoch 9 Batch 150 Loss 0.0809 Accuracy 0.9736\n",
            "Epoch 9 Loss 0.0810 Accuracy 0.9735\n",
            "Time taken for 1 epoch: 35.94 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0721 Accuracy 0.9763\n",
            "Epoch 10 Batch 50 Loss 0.0745 Accuracy 0.9752\n",
            "Epoch 10 Batch 100 Loss 0.0773 Accuracy 0.9745\n",
            "Epoch 10 Batch 150 Loss 0.0795 Accuracy 0.9739\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-syll-gen/ckpt-22\n",
            "Epoch 10 Loss 0.0799 Accuracy 0.9737\n",
            "Time taken for 1 epoch: 36.49 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0631 Accuracy 0.9795\n",
            "Epoch 11 Batch 50 Loss 0.0740 Accuracy 0.9756\n",
            "Epoch 11 Batch 100 Loss 0.0765 Accuracy 0.9747\n",
            "Epoch 11 Batch 150 Loss 0.0781 Accuracy 0.9743\n",
            "Epoch 11 Loss 0.0786 Accuracy 0.9741\n",
            "Time taken for 1 epoch: 36.06 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0657 Accuracy 0.9779\n",
            "Epoch 12 Batch 50 Loss 0.0727 Accuracy 0.9761\n",
            "Epoch 12 Batch 100 Loss 0.0751 Accuracy 0.9752\n",
            "Epoch 12 Batch 150 Loss 0.0768 Accuracy 0.9747\n",
            "Epoch 12 Loss 0.0773 Accuracy 0.9746\n",
            "Time taken for 1 epoch: 35.87 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0706 Accuracy 0.9767\n",
            "Epoch 13 Batch 50 Loss 0.0737 Accuracy 0.9759\n",
            "Epoch 13 Batch 100 Loss 0.0749 Accuracy 0.9755\n",
            "Epoch 13 Batch 150 Loss 0.0768 Accuracy 0.9749\n",
            "Epoch 13 Loss 0.0774 Accuracy 0.9747\n",
            "Time taken for 1 epoch: 36.04 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0725 Accuracy 0.9764\n",
            "Epoch 14 Batch 50 Loss 0.0718 Accuracy 0.9765\n",
            "Epoch 14 Batch 100 Loss 0.0739 Accuracy 0.9758\n",
            "Epoch 14 Batch 150 Loss 0.0753 Accuracy 0.9752\n",
            "Epoch 14 Loss 0.0758 Accuracy 0.9751\n",
            "Time taken for 1 epoch: 36.07 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0660 Accuracy 0.9765\n",
            "Epoch 15 Batch 50 Loss 0.0726 Accuracy 0.9762\n",
            "Epoch 15 Batch 100 Loss 0.0739 Accuracy 0.9759\n",
            "Epoch 15 Batch 150 Loss 0.0750 Accuracy 0.9753\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/word-syll-gen/ckpt-23\n",
            "Epoch 15 Loss 0.0755 Accuracy 0.9752\n",
            "Time taken for 1 epoch: 36.36 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0771 Accuracy 0.9744\n",
            "Epoch 16 Batch 50 Loss 0.0703 Accuracy 0.9766\n",
            "Epoch 16 Batch 100 Loss 0.0722 Accuracy 0.9762\n",
            "Epoch 16 Batch 150 Loss 0.0744 Accuracy 0.9755\n",
            "Epoch 16 Loss 0.0746 Accuracy 0.9755\n",
            "Time taken for 1 epoch: 36.09 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0658 Accuracy 0.9780\n",
            "Epoch 17 Batch 50 Loss 0.0718 Accuracy 0.9765\n",
            "Epoch 17 Batch 100 Loss 0.0729 Accuracy 0.9762\n",
            "Epoch 17 Batch 150 Loss 0.0742 Accuracy 0.9757\n",
            "Epoch 17 Loss 0.0746 Accuracy 0.9756\n",
            "Time taken for 1 epoch: 35.93 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0726 Accuracy 0.9758\n",
            "Epoch 18 Batch 50 Loss 0.0691 Accuracy 0.9772\n",
            "Epoch 18 Batch 100 Loss 0.0716 Accuracy 0.9766\n",
            "Epoch 18 Batch 150 Loss 0.0730 Accuracy 0.9761\n",
            "Epoch 18 Loss 0.0731 Accuracy 0.9760\n",
            "Time taken for 1 epoch: 36.03 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0631 Accuracy 0.9791\n",
            "Epoch 19 Batch 50 Loss 0.0688 Accuracy 0.9775\n",
            "Epoch 19 Batch 100 Loss 0.0700 Accuracy 0.9772\n",
            "Epoch 19 Batch 150 Loss 0.0714 Accuracy 0.9768\n",
            "Epoch 19 Loss 0.0717 Accuracy 0.9766\n",
            "Time taken for 1 epoch: 36.26 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0638 Accuracy 0.9776\n",
            "Epoch 20 Batch 50 Loss 0.0670 Accuracy 0.9780\n",
            "Epoch 20 Batch 100 Loss 0.0697 Accuracy 0.9771\n",
            "Epoch 20 Batch 150 Loss 0.0713 Accuracy 0.9766\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/word-syll-gen/ckpt-24\n",
            "Epoch 20 Loss 0.0716 Accuracy 0.9765\n",
            "Time taken for 1 epoch: 36.79 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0684 Accuracy 0.9789\n",
            "Epoch 21 Batch 50 Loss 0.0667 Accuracy 0.9782\n",
            "Epoch 21 Batch 100 Loss 0.0682 Accuracy 0.9777\n",
            "Epoch 21 Batch 150 Loss 0.0698 Accuracy 0.9772\n",
            "Epoch 21 Loss 0.0704 Accuracy 0.9770\n",
            "Time taken for 1 epoch: 36.11 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0636 Accuracy 0.9790\n",
            "Epoch 22 Batch 50 Loss 0.0661 Accuracy 0.9782\n",
            "Epoch 22 Batch 100 Loss 0.0672 Accuracy 0.9778\n",
            "Epoch 22 Batch 150 Loss 0.0685 Accuracy 0.9775\n",
            "Epoch 22 Loss 0.0689 Accuracy 0.9774\n",
            "Time taken for 1 epoch: 36.08 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0726 Accuracy 0.9754\n",
            "Epoch 23 Batch 50 Loss 0.0670 Accuracy 0.9779\n",
            "Epoch 23 Batch 100 Loss 0.0676 Accuracy 0.9780\n",
            "Epoch 23 Batch 150 Loss 0.0693 Accuracy 0.9774\n",
            "Epoch 23 Loss 0.0696 Accuracy 0.9773\n",
            "Time taken for 1 epoch: 35.98 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0665 Accuracy 0.9770\n",
            "Epoch 24 Batch 50 Loss 0.0652 Accuracy 0.9785\n",
            "Epoch 24 Batch 100 Loss 0.0664 Accuracy 0.9781\n",
            "Epoch 24 Batch 150 Loss 0.0674 Accuracy 0.9779\n",
            "Epoch 24 Loss 0.0680 Accuracy 0.9777\n",
            "Time taken for 1 epoch: 36.02 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0635 Accuracy 0.9790\n",
            "Epoch 25 Batch 50 Loss 0.0658 Accuracy 0.9785\n",
            "Epoch 25 Batch 100 Loss 0.0661 Accuracy 0.9785\n",
            "Epoch 25 Batch 150 Loss 0.0672 Accuracy 0.9781\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/word-syll-gen/ckpt-25\n",
            "Epoch 25 Loss 0.0678 Accuracy 0.9779\n",
            "Time taken for 1 epoch: 36.50 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0723 Accuracy 0.9765\n",
            "Epoch 26 Batch 50 Loss 0.0640 Accuracy 0.9789\n",
            "Epoch 26 Batch 100 Loss 0.0650 Accuracy 0.9785\n",
            "Epoch 26 Batch 150 Loss 0.0664 Accuracy 0.9781\n",
            "Epoch 26 Loss 0.0668 Accuracy 0.9780\n",
            "Time taken for 1 epoch: 36.17 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0637 Accuracy 0.9780\n",
            "Epoch 27 Batch 50 Loss 0.0627 Accuracy 0.9794\n",
            "Epoch 27 Batch 100 Loss 0.0641 Accuracy 0.9789\n",
            "Epoch 27 Batch 150 Loss 0.0658 Accuracy 0.9783\n",
            "Epoch 27 Loss 0.0664 Accuracy 0.9781\n",
            "Time taken for 1 epoch: 35.97 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0573 Accuracy 0.9818\n",
            "Epoch 28 Batch 50 Loss 0.0637 Accuracy 0.9792\n",
            "Epoch 28 Batch 100 Loss 0.0642 Accuracy 0.9790\n",
            "Epoch 28 Batch 150 Loss 0.0652 Accuracy 0.9786\n",
            "Epoch 28 Loss 0.0656 Accuracy 0.9785\n",
            "Time taken for 1 epoch: 36.05 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0597 Accuracy 0.9811\n",
            "Epoch 29 Batch 50 Loss 0.0622 Accuracy 0.9797\n",
            "Epoch 29 Batch 100 Loss 0.0640 Accuracy 0.9790\n",
            "Epoch 29 Batch 150 Loss 0.0647 Accuracy 0.9789\n",
            "Epoch 29 Loss 0.0652 Accuracy 0.9787\n",
            "Time taken for 1 epoch: 36.05 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0613 Accuracy 0.9797\n",
            "Epoch 30 Batch 50 Loss 0.0612 Accuracy 0.9799\n",
            "Epoch 30 Batch 100 Loss 0.0628 Accuracy 0.9794\n",
            "Epoch 30 Batch 150 Loss 0.0639 Accuracy 0.9791\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/word-syll-gen/ckpt-26\n",
            "Epoch 30 Loss 0.0641 Accuracy 0.9790\n",
            "Time taken for 1 epoch: 36.37 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0553 Accuracy 0.9814\n",
            "Epoch 31 Batch 50 Loss 0.0593 Accuracy 0.9806\n",
            "Epoch 31 Batch 100 Loss 0.0619 Accuracy 0.9798\n",
            "Epoch 31 Batch 150 Loss 0.0629 Accuracy 0.9794\n",
            "Epoch 31 Loss 0.0632 Accuracy 0.9794\n",
            "Time taken for 1 epoch: 36.06 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0513 Accuracy 0.9832\n",
            "Epoch 32 Batch 50 Loss 0.0600 Accuracy 0.9802\n",
            "Epoch 32 Batch 100 Loss 0.0624 Accuracy 0.9795\n",
            "Epoch 32 Batch 150 Loss 0.0629 Accuracy 0.9795\n",
            "Epoch 32 Loss 0.0629 Accuracy 0.9794\n",
            "Time taken for 1 epoch: 36.13 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0546 Accuracy 0.9831\n",
            "Epoch 33 Batch 50 Loss 0.0596 Accuracy 0.9805\n",
            "Epoch 33 Batch 100 Loss 0.0605 Accuracy 0.9802\n",
            "Epoch 33 Batch 150 Loss 0.0617 Accuracy 0.9797\n",
            "Epoch 33 Loss 0.0620 Accuracy 0.9796\n",
            "Time taken for 1 epoch: 36.12 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0559 Accuracy 0.9813\n",
            "Epoch 34 Batch 50 Loss 0.0588 Accuracy 0.9806\n",
            "Epoch 34 Batch 100 Loss 0.0604 Accuracy 0.9803\n",
            "Epoch 34 Batch 150 Loss 0.0613 Accuracy 0.9799\n",
            "Epoch 34 Loss 0.0616 Accuracy 0.9799\n",
            "Time taken for 1 epoch: 36.03 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0576 Accuracy 0.9819\n",
            "Epoch 35 Batch 50 Loss 0.0590 Accuracy 0.9806\n",
            "Epoch 35 Batch 100 Loss 0.0607 Accuracy 0.9800\n",
            "Epoch 35 Batch 150 Loss 0.0613 Accuracy 0.9799\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/word-syll-gen/ckpt-27\n",
            "Epoch 35 Loss 0.0615 Accuracy 0.9798\n",
            "Time taken for 1 epoch: 36.43 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0570 Accuracy 0.9807\n",
            "Epoch 36 Batch 50 Loss 0.0568 Accuracy 0.9814\n",
            "Epoch 36 Batch 100 Loss 0.0592 Accuracy 0.9806\n",
            "Epoch 36 Batch 150 Loss 0.0606 Accuracy 0.9802\n",
            "Epoch 36 Loss 0.0611 Accuracy 0.9800\n",
            "Time taken for 1 epoch: 36.03 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0568 Accuracy 0.9820\n",
            "Epoch 37 Batch 50 Loss 0.0581 Accuracy 0.9809\n",
            "Epoch 37 Batch 100 Loss 0.0592 Accuracy 0.9804\n",
            "Epoch 37 Batch 150 Loss 0.0599 Accuracy 0.9803\n",
            "Epoch 37 Loss 0.0603 Accuracy 0.9801\n",
            "Time taken for 1 epoch: 36.12 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0478 Accuracy 0.9840\n",
            "Epoch 38 Batch 50 Loss 0.0578 Accuracy 0.9811\n",
            "Epoch 38 Batch 100 Loss 0.0589 Accuracy 0.9808\n",
            "Epoch 38 Batch 150 Loss 0.0595 Accuracy 0.9806\n",
            "Epoch 38 Loss 0.0596 Accuracy 0.9805\n",
            "Time taken for 1 epoch: 36.17 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0557 Accuracy 0.9828\n",
            "Epoch 39 Batch 50 Loss 0.0556 Accuracy 0.9817\n",
            "Epoch 39 Batch 100 Loss 0.0574 Accuracy 0.9812\n",
            "Epoch 39 Batch 150 Loss 0.0588 Accuracy 0.9808\n",
            "Epoch 39 Loss 0.0592 Accuracy 0.9806\n",
            "Time taken for 1 epoch: 36.11 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0573 Accuracy 0.9798\n",
            "Epoch 40 Batch 50 Loss 0.0567 Accuracy 0.9814\n",
            "Epoch 40 Batch 100 Loss 0.0580 Accuracy 0.9810\n",
            "Epoch 40 Batch 150 Loss 0.0586 Accuracy 0.9808\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/word-syll-gen/ckpt-28\n",
            "Epoch 40 Loss 0.0588 Accuracy 0.9807\n",
            "Time taken for 1 epoch: 36.53 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0551 Accuracy 0.9815\n",
            "Epoch 41 Batch 50 Loss 0.0553 Accuracy 0.9818\n",
            "Epoch 41 Batch 100 Loss 0.0570 Accuracy 0.9813\n",
            "Epoch 41 Batch 150 Loss 0.0580 Accuracy 0.9810\n",
            "Epoch 41 Loss 0.0582 Accuracy 0.9809\n",
            "Time taken for 1 epoch: 36.14 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0568 Accuracy 0.9794\n",
            "Epoch 42 Batch 50 Loss 0.0552 Accuracy 0.9817\n",
            "Epoch 42 Batch 100 Loss 0.0565 Accuracy 0.9813\n",
            "Epoch 42 Batch 150 Loss 0.0577 Accuracy 0.9809\n",
            "Epoch 42 Loss 0.0580 Accuracy 0.9808\n",
            "Time taken for 1 epoch: 36.15 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0454 Accuracy 0.9860\n",
            "Epoch 43 Batch 50 Loss 0.0525 Accuracy 0.9827\n",
            "Epoch 43 Batch 100 Loss 0.0549 Accuracy 0.9820\n",
            "Epoch 43 Batch 150 Loss 0.0564 Accuracy 0.9815\n",
            "Epoch 43 Loss 0.0566 Accuracy 0.9815\n",
            "Time taken for 1 epoch: 35.98 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0516 Accuracy 0.9816\n",
            "Epoch 44 Batch 50 Loss 0.0533 Accuracy 0.9828\n",
            "Epoch 44 Batch 100 Loss 0.0543 Accuracy 0.9823\n",
            "Epoch 44 Batch 150 Loss 0.0557 Accuracy 0.9818\n",
            "Epoch 44 Loss 0.0561 Accuracy 0.9817\n",
            "Time taken for 1 epoch: 36.12 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0460 Accuracy 0.9833\n",
            "Epoch 45 Batch 50 Loss 0.0537 Accuracy 0.9823\n",
            "Epoch 45 Batch 100 Loss 0.0551 Accuracy 0.9818\n",
            "Epoch 45 Batch 150 Loss 0.0562 Accuracy 0.9814\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/word-syll-gen/ckpt-29\n",
            "Epoch 45 Loss 0.0562 Accuracy 0.9814\n",
            "Time taken for 1 epoch: 36.46 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0551 Accuracy 0.9807\n",
            "Epoch 46 Batch 50 Loss 0.0534 Accuracy 0.9823\n",
            "Epoch 46 Batch 100 Loss 0.0554 Accuracy 0.9817\n",
            "Epoch 46 Batch 150 Loss 0.0564 Accuracy 0.9814\n",
            "Epoch 46 Loss 0.0566 Accuracy 0.9814\n",
            "Time taken for 1 epoch: 36.09 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0574 Accuracy 0.9803\n",
            "Epoch 47 Batch 50 Loss 0.0526 Accuracy 0.9828\n",
            "Epoch 47 Batch 100 Loss 0.0541 Accuracy 0.9822\n",
            "Epoch 47 Batch 150 Loss 0.0551 Accuracy 0.9820\n",
            "Epoch 47 Loss 0.0553 Accuracy 0.9819\n",
            "Time taken for 1 epoch: 36.15 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0536 Accuracy 0.9816\n",
            "Epoch 48 Batch 50 Loss 0.0534 Accuracy 0.9826\n",
            "Epoch 48 Batch 100 Loss 0.0538 Accuracy 0.9825\n",
            "Epoch 48 Batch 150 Loss 0.0546 Accuracy 0.9822\n",
            "Epoch 48 Loss 0.0551 Accuracy 0.9820\n",
            "Time taken for 1 epoch: 36.14 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0503 Accuracy 0.9835\n",
            "Epoch 49 Batch 50 Loss 0.0517 Accuracy 0.9829\n",
            "Epoch 49 Batch 100 Loss 0.0537 Accuracy 0.9823\n",
            "Epoch 49 Batch 150 Loss 0.0541 Accuracy 0.9822\n",
            "Epoch 49 Loss 0.0544 Accuracy 0.9821\n",
            "Time taken for 1 epoch: 36.05 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0480 Accuracy 0.9829\n",
            "Epoch 50 Batch 50 Loss 0.0503 Accuracy 0.9837\n",
            "Epoch 50 Batch 100 Loss 0.0526 Accuracy 0.9828\n",
            "Epoch 50 Batch 150 Loss 0.0536 Accuracy 0.9824\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/word-syll-gen/ckpt-30\n",
            "Epoch 50 Loss 0.0540 Accuracy 0.9823\n",
            "Time taken for 1 epoch: 36.51 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "TODO change this :)\n",
        "\n",
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVhkqi-_D1nQ"
      },
      "source": [
        "def postprocess(x):\n",
        "    x = x.replace('<SEP>', ' ')\n",
        "    x = x.replace(' <SYL> ', '|')\n",
        "    x = x.replace('<SYL> ', '|')\n",
        "    x = x.replace(' <SYL>', '|')\n",
        "    x = x.replace('<GO>', '\\n')\n",
        "    x = x.replace('<EOT>', '\\n\\n')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    return x"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hqlF-cEQIpT"
      },
      "source": [
        "def clean_encoder(x):\n",
        "    x = x.replace('<SEP>', ' ')\n",
        "    x = x.replace(' <SYL> ', '')\n",
        "    x = x.replace('<SYL> ', '')\n",
        "    x = x.replace(' <SYL>', '')\n",
        "    x = x.replace('<GO>', ' <GO> ')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    return x"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Fl3mDkQ77j"
      },
      "source": [
        "def clean_decoder(x):\n",
        "    x = x.replace('<GO>', ' <GO> ')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    return x"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "9f320c2e-860d-4198-a827-b060caedfa99"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> dis <SYL> se <SEP> : <SEP> <SEP> « <SEP> <SEP> <SYL> Che <SEP> <SYL> hai <SEP> ? <SEP> <SEP> <SYL> sì <SEP> <SYL> che <SEP> , <SEP> <SEP> <SYL> se <SEP> <SYL> tu <SEP> <SYL> m <SEP> ’ <SEP> <SEP> ac <SYL> cor <SYL> si <GO> <SYL> ne <SEP> <SYL> la <SEP> <SYL> men <SYL> te <SEP> <SYL> sua <SEP> <SYL> se <SYL> men <SYL> za <SEP> <SYL> non <SEP> <SYL> con <SYL> du <SYL> ce <SEP> , <SEP> <GO> <SYL> ma <SEP> <SYL> fia <SEP> <SYL> d <SEP> ’ <SEP> <SEP> un <SEP> <SYL> al <SYL> tro <SEP> <SYL> cir <SYL> cun <SYL> scrit <SYL> to <SEP> , <SEP> <SEP> <SYL> se <SYL> gno <SEP> . <SEP> <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usetPrX-EvPl",
        "outputId": "afca3f5f-1f82-482f-b4ea-bb28380c3834"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|e |dis|se  :   «  |Che |hai  ?  |sì |che  ,  |se |tu |m  ’   ac|cor|si \n",
            "|ne |la |men|te |sua |se|men|za |non |con|du|ce  ,  \n",
            "|ma |fia |d  ’   un |al|tro |cir|cun|scrit|to  ,  |se|gno  .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EHAeaVSRRhJ",
        "outputId": "9b7c4638-9041-4c26-be2b-37bba8d4a40a"
      },
      "source": [
        "print(clean_encoder(generated_text))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e disse  :   «  Che hai  ?  sì che  ,  se tu m  ’   accorsi <GO> ne la mente sua semenza non conduce  ,  <GO> ma fia d  ’   un altro circunscritto  ,  segno  .  <EOT>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-rNRE5xRTtG",
        "outputId": "f576c0b5-5e3f-41a3-aef2-cca03b990c8f"
      },
      "source": [
        "print(clean_decoder(generated_text))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> dis <SYL> se <SEP> : <SEP> <SEP> « <SEP> <SEP> <SYL> Che <SEP> <SYL> hai <SEP> ? <SEP> <SEP> <SYL> sì <SEP> <SYL> che <SEP> , <SEP> <SEP> <SYL> se <SEP> <SYL> tu <SEP> <SYL> m <SEP> ’ <SEP> <SEP> ac <SYL> cor <SYL> si <GO> <SYL> ne <SEP> <SYL> la <SEP> <SYL> men <SYL> te <SEP> <SYL> sua <SEP> <SYL> se <SYL> men <SYL> za <SEP> <SYL> non <SEP> <SYL> con <SYL> du <SYL> ce <SEP> , <SEP> <GO> <SYL> ma <SEP> <SYL> fia <SEP> <SYL> d <SEP> ’ <SEP> <SEP> un <SEP> <SYL> al <SYL> tro <SEP> <SYL> cir <SYL> cun <SYL> scrit <SYL> to <SEP> , <SEP> <SEP> <SYL> se <SYL> gno <SEP> . <SEP> <EOT>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuuX9RiRRyJb",
        "outputId": "f42d451d-eba1-4bf7-c230-a1a496c53ef4"
      },
      "source": [
        "tokenized_enc = [input_tokenizer.texts_to_sequences(clean_encoder(generated_text))]\n",
        "tokenized_enc = tokenized_enc[0]\n",
        "tokenized_enc = list(chain.from_iterable(tokenized_enc))\n",
        "generated_text_1 = generate_greedy(tokenized_enc, decoder_input)\n",
        "print(generated_text_1)\n",
        "print(postprocess(generated_text_1))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> di <SEP> <SYL> quel <SEP> <SYL> che <SEP> <SYL> cre <SYL> di <SEP> <SYL> set <SYL> ta <SEP> il <SEP> <SYL> cer <SYL> chio <SEP> <SYL> pri <SYL> ma <GO> <SYL> il <SEP> <SYL> qual <SEP> <SYL> tu <SEP> <SYL> se <SEP> ’ <SEP> <SEP> <SYL> di <SEP> <SYL> quel <SEP> <SYL> che <SEP> <SYL> ti <SEP> <SYL> ri <SYL> ce <SYL> ve <SEP> » <SEP> <SEP> . <SEP> <GO> <SYL> Co <SYL> sì <SEP> <SYL> par <SYL> lar <SEP> , <SEP> <SEP> <SYL> co <SYL> me <SEP> il <SEP> <SYL> be <SYL> ne <SEP> <SYL> det <SYL> to <EOT> \n",
            "|di |quel |che |cre|di |set|ta  il |cer|chio |pri|ma \n",
            "|il |qual |tu |se  ’  |di |quel |che |ti |ri|ce|ve  »   .  \n",
            "|Co|sì |par|lar  ,  |co|me  il |be|ne |det|to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvdQKby9Smlt"
      },
      "source": [
        "def last_2_decoder(x):\n",
        "  x = x.split('<GO>')[1:]\n",
        "  x = list(map(lambda x: x.strip(), x))\n",
        "  x = list(map(lambda x: x + '<GO>' , x))\n",
        "  x = ''.join(x)\n",
        "  x = target_tokenizer.texts_to_sequences([x])[0]\n",
        "  return x"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEkQPunJfAE2",
        "outputId": "4f94b5e9-30a0-4ab3-e0a8-1e13c1c050b1"
      },
      "source": [
        "tokenized_dec = last_2_decoder(generated_text)\n",
        "generated_text_2 = generate_greedy(encoder_input, tokenized_dec)\n",
        "print(generated_text_2)\n",
        "print(postprocess(generated_text_2))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<GO> <SYL> Que <SYL> sti <SEP> <SYL> fuor <SEP> <SYL> Lu <SYL> ci <SEP> , <SEP> <SEP> <SYL> ri <SYL> ma <SYL> se <SEP> e <SEP> <SYL> non <SEP> <SYL> spe <SYL> sa <EOT> \n",
            "|Que|sti |fuor |Lu|ci  ,  |ri|ma|se  e |non |spe|sa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MSGeRLSSUi-",
        "outputId": "2d37ba18-41c2-441b-dbe7-4622f0752cdf"
      },
      "source": [
        "generated_text_3 = generate_greedy(tokenized_enc, tokenized_dec)\n",
        "print(generated_text_3)\n",
        "print(postprocess(generated_text_3))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<GO> <SYL> Que <SYL> sti <SEP> <SYL> che <SEP> <SYL> la <SEP> <SYL> Chie <SYL> sa <SEP> , <SEP> <SEP> <SYL> lui <SEP> <SYL> tra <SEP> <SYL> quel <SEP> <SYL> cer <SYL> to <EOT> \n",
            "|Que|sti |che |la |Chie|sa  ,  |lui |tra |quel |cer|to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() #qui potrebbe anche essere [0,0]\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "af4ba1fc-8b3a-41be-bb3b-44134da11908"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> dis <SYL> se <SEP> : <SEP> <SEP> « <SEP> <SEP> <SYL> Che <SEP> <SYL> hai <SEP> ? <SEP> <SEP> <SYL> sì <SEP> <SYL> che <SEP> , <SEP> <SEP> <SYL> se <SEP> <SYL> non <SEP> <SYL> con <SYL> te <GO> <SYL> da <SEP> <SYL> l <SEP> ’ <SEP> <SEP> al <SYL> tra <SEP> <SYL> par <SYL> te <SEP> <SYL> vi <SYL> di <SEP> , <SEP> <SEP> <SYL> si <SEP> <SYL> ri <SYL> tras <SYL> se <GO> <SYL> lo <SEP> <SYL> bel <SYL> lo <SEP> <SYL> d <SEP> ’ <SEP> <SEP> o <SYL> gne <SEP> <SYL> par <SYL> te <SEP> <SYL> si <SEP> <SYL> con <SYL> fes <SYL> sa <SEP> ; <SEP> <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBrcICi_F1P9",
        "outputId": "cbe5b95a-d0df-4cf7-847d-aa52a38af9f6"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|e |dis|se  :   «  |Che |hai  ?  |sì |che  ,  |se |non |con|te \n",
            "|da |l  ’   al|tra |par|te |vi|di  ,  |si |ri|tras|se \n",
            "|lo |bel|lo |d  ’   o|gne |par|te |si |con|fes|sa  ;\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}