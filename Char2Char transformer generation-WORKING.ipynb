{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Char2Char transformer generation-WORKING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcDazsVbL_tS"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "KEs7coKBmIKI",
        "outputId": "c6b4e22a-d29d-423b-a30c-57d39850c5f5"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import files\n",
        "    \n",
        "    files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c1b0caa-717e-4920-bc46-f339184e49f5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c1b0caa-717e-4920-bc46-f339184e49f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.zip to data.zip\n",
            "Saving deepcomedy.zip to deepcomedy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CslaB3BRmImh",
        "outputId": "c0a73ef4-4041-4d6d-851e-7eb490daccb0"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install strsimpy\n",
        "#!tar zxvf deepcomedy.tar.gz\n",
        "#!tar zxvf checkpoints.tar.gz\n",
        "!unzip deepcomedy.zip\n",
        "#!tar zxvf data.tar.gz\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/48/b199e2b3b341ac842108c5db4956091dd75d961cfa77aceb033e99cac20f/wandb-0.10.31-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.2MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.4MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=35c49ebc094cef6ea59f8b9452a21a7cdef6fe0c2ae4e516b154b9a9a833750e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=b6a3795ddcd46cad5705f75662273c201b129a7540859268ad3d6485a260b586\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: configparser, shortuuid, pathtools, subprocess32, sentry-sdk, docker-pycreds, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.31\n",
            "Collecting strsimpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/03/13717aacea2f5c3c2ccb74fa921b0073476810afc079be3534eb17383639/strsimpy-0.2.0-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: strsimpy\n",
            "Successfully installed strsimpy-0.2.0\n",
            "Archive:  deepcomedy.zip\n",
            "   creating: deepcomedy/models/\n",
            "  inflating: deepcomedy/models/layers.py  \n",
            "  inflating: deepcomedy/models/transformer.py  \n",
            "  inflating: deepcomedy/preprocessing.py  \n",
            "   creating: deepcomedy/util/\n",
            "  inflating: deepcomedy/util/predicate.py  \n",
            "  inflating: deepcomedy/utils.py     \n",
            "Archive:  data.zip\n",
            "  inflating: data/divina_syll_textonly.txt  \n",
            "  inflating: data/divina_textonly.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import *\n",
        "from deepcomedy.utils import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7n29oxB0z4"
      },
      "source": [
        "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        "raw_syll_text = (\n",
        "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        ")\n",
        "syll_text = preprocess_text(raw_syll_text, end_of_tercet='')\n",
        "text = preprocess_text(raw_text, end_of_tercet='')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ASHyaMBC84V"
      },
      "source": [
        "Split preprocessed text into verses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IF6sE6FC_4J"
      },
      "source": [
        "sep = \"<EOV>\"\n",
        "input_tercets = [x.lstrip() + sep for x in text.split(sep)][:-1]\n",
        "target_tercets = [x.lstrip() + sep for x in syll_text.split(sep)][:-1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdUmYhUKDEuj"
      },
      "source": [
        "Encode with input and target tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mob1kOzDD4z"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=False, filters=\"\", lower=False\n",
        ")\n",
        "input_tokenizer.fit_on_texts(input_tercets)\n",
        "\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=False, filters=\"\", lower=False\n",
        ")\n",
        "target_tokenizer.fit_on_texts(target_tercets)\n",
        "\n",
        "enc_input_tercets = input_tokenizer.texts_to_sequences(input_tercets)\n",
        "enc_target_tercets = target_tokenizer.texts_to_sequences(target_tercets)\n",
        "\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKv92yAL_t8"
      },
      "source": [
        "input_text = []\n",
        "target_text = []\n",
        "target_text_tercet = []\n",
        "\n",
        "for line in range(len(enc_input_tercets) - 2):\n",
        "    input_text.append(list(chain(*enc_input_tercets[line : line + 3])))\n",
        "    target_text_tercet.append(list(chain(*enc_target_tercets[line : line + 3])))\n",
        "    target_text.append(list(chain(*enc_target_tercets[line : line + 4])))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY44HP5lKz2-"
      },
      "source": [
        "Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq34y57yK3wd"
      },
      "source": [
        "padded_input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_text, padding=\"post\"\n",
        ")\n",
        "padded_target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_text, padding=\"post\"\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v2e1bIcLRZi"
      },
      "source": [
        "dataset = make_dataset(padded_input_text, padded_target_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sGOf__BLZzQ"
      },
      "source": [
        "config = {\n",
        "    \"num_layers\" : 6,\n",
        "    \"d_model\" : 256,\n",
        "    \"num_heads\" : 4,\n",
        "    \"dff\" : 512,\n",
        "}\n",
        "\n",
        "checkpoint_save_path = \"./checkpoints/char-input_char-output_gen\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miEcmOVmL0Rt"
      },
      "source": [
        "transformer, transformer_trainer = make_transformer_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path= None)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaR03YUNL_uF",
        "outputId": "31d4f662-dea9-4aaf-ce1d-e71328c9294a"
      },
      "source": [
        "transformer_trainer.train(dataset, 30)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.0481 Accuracy 0.0003\n",
            "Epoch 1 Batch 50 Loss 3.9581 Accuracy 0.1262\n",
            "Epoch 1 Batch 100 Loss 3.5209 Accuracy 0.1663\n",
            "Epoch 1 Batch 150 Loss 3.3449 Accuracy 0.1823\n",
            "Epoch 1 Batch 200 Loss 3.1761 Accuracy 0.2015\n",
            "Epoch 1 Batch 250 Loss 2.9910 Accuracy 0.2272\n",
            "Epoch 1 Batch 300 Loss 2.8416 Accuracy 0.2484\n",
            "Epoch 1 Batch 350 Loss 2.7246 Accuracy 0.2650\n",
            "Epoch 1 Batch 400 Loss 2.6314 Accuracy 0.2785\n",
            "Epoch 1 Loss 2.5653 Accuracy 0.2883\n",
            "Time taken for 1 epoch: 106.16 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9573 Accuracy 0.3826\n",
            "Epoch 2 Batch 50 Loss 1.9313 Accuracy 0.3835\n",
            "Epoch 2 Batch 100 Loss 1.9163 Accuracy 0.3871\n",
            "Epoch 2 Batch 150 Loss 1.9030 Accuracy 0.3903\n",
            "Epoch 2 Batch 200 Loss 1.8881 Accuracy 0.3943\n",
            "Epoch 2 Batch 250 Loss 1.8720 Accuracy 0.3982\n",
            "Epoch 2 Batch 300 Loss 1.8551 Accuracy 0.4025\n",
            "Epoch 2 Batch 350 Loss 1.8371 Accuracy 0.4074\n",
            "Epoch 2 Batch 400 Loss 1.8186 Accuracy 0.4121\n",
            "Epoch 2 Loss 1.8031 Accuracy 0.4160\n",
            "Time taken for 1 epoch: 95.17 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6437 Accuracy 0.4592\n",
            "Epoch 3 Batch 50 Loss 1.6352 Accuracy 0.4598\n",
            "Epoch 3 Batch 100 Loss 1.6211 Accuracy 0.4640\n",
            "Epoch 3 Batch 150 Loss 1.6087 Accuracy 0.4679\n",
            "Epoch 3 Batch 200 Loss 1.5981 Accuracy 0.4715\n",
            "Epoch 3 Batch 250 Loss 1.5866 Accuracy 0.4754\n",
            "Epoch 3 Batch 300 Loss 1.5757 Accuracy 0.4791\n",
            "Epoch 3 Batch 350 Loss 1.5643 Accuracy 0.4825\n",
            "Epoch 3 Batch 400 Loss 1.5529 Accuracy 0.4861\n",
            "Epoch 3 Loss 1.5441 Accuracy 0.4889\n",
            "Time taken for 1 epoch: 95.03 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4694 Accuracy 0.5145\n",
            "Epoch 4 Batch 50 Loss 1.4366 Accuracy 0.5222\n",
            "Epoch 4 Batch 100 Loss 1.4284 Accuracy 0.5248\n",
            "Epoch 4 Batch 150 Loss 1.4205 Accuracy 0.5267\n",
            "Epoch 4 Batch 200 Loss 1.4131 Accuracy 0.5290\n",
            "Epoch 4 Batch 250 Loss 1.4054 Accuracy 0.5313\n",
            "Epoch 4 Batch 300 Loss 1.3984 Accuracy 0.5336\n",
            "Epoch 4 Batch 350 Loss 1.3917 Accuracy 0.5358\n",
            "Epoch 4 Batch 400 Loss 1.3854 Accuracy 0.5379\n",
            "Epoch 4 Loss 1.3793 Accuracy 0.5398\n",
            "Time taken for 1 epoch: 95.03 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3075 Accuracy 0.5576\n",
            "Epoch 5 Batch 50 Loss 1.3088 Accuracy 0.5621\n",
            "Epoch 5 Batch 100 Loss 1.3013 Accuracy 0.5648\n",
            "Epoch 5 Batch 150 Loss 1.2987 Accuracy 0.5655\n",
            "Epoch 5 Batch 200 Loss 1.2938 Accuracy 0.5673\n",
            "Epoch 5 Batch 250 Loss 1.2876 Accuracy 0.5692\n",
            "Epoch 5 Batch 300 Loss 1.2829 Accuracy 0.5706\n",
            "Epoch 5 Batch 350 Loss 1.2783 Accuracy 0.5722\n",
            "Epoch 5 Batch 400 Loss 1.2730 Accuracy 0.5741\n",
            "Epoch 5 Loss 1.2687 Accuracy 0.5756\n",
            "Time taken for 1 epoch: 95.08 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2279 Accuracy 0.5983\n",
            "Epoch 6 Batch 50 Loss 1.2142 Accuracy 0.5935\n",
            "Epoch 6 Batch 100 Loss 1.2107 Accuracy 0.5947\n",
            "Epoch 6 Batch 150 Loss 1.2057 Accuracy 0.5962\n",
            "Epoch 6 Batch 200 Loss 1.2034 Accuracy 0.5970\n",
            "Epoch 6 Batch 250 Loss 1.2016 Accuracy 0.5977\n",
            "Epoch 6 Batch 300 Loss 1.1984 Accuracy 0.5987\n",
            "Epoch 6 Batch 350 Loss 1.1940 Accuracy 0.6001\n",
            "Epoch 6 Batch 400 Loss 1.1902 Accuracy 0.6015\n",
            "Epoch 6 Loss 1.1875 Accuracy 0.6023\n",
            "Time taken for 1 epoch: 94.81 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1076 Accuracy 0.6221\n",
            "Epoch 7 Batch 50 Loss 1.1444 Accuracy 0.6164\n",
            "Epoch 7 Batch 100 Loss 1.1444 Accuracy 0.6161\n",
            "Epoch 7 Batch 150 Loss 1.1412 Accuracy 0.6169\n",
            "Epoch 7 Batch 200 Loss 1.1390 Accuracy 0.6177\n",
            "Epoch 7 Batch 250 Loss 1.1369 Accuracy 0.6183\n",
            "Epoch 7 Batch 300 Loss 1.1352 Accuracy 0.6189\n",
            "Epoch 7 Batch 350 Loss 1.1326 Accuracy 0.6197\n",
            "Epoch 7 Batch 400 Loss 1.1302 Accuracy 0.6205\n",
            "Epoch 7 Loss 1.1280 Accuracy 0.6212\n",
            "Time taken for 1 epoch: 94.88 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1124 Accuracy 0.6310\n",
            "Epoch 8 Batch 50 Loss 1.0897 Accuracy 0.6336\n",
            "Epoch 8 Batch 100 Loss 1.0887 Accuracy 0.6337\n",
            "Epoch 8 Batch 150 Loss 1.0876 Accuracy 0.6339\n",
            "Epoch 8 Batch 200 Loss 1.0856 Accuracy 0.6348\n",
            "Epoch 8 Batch 250 Loss 1.0841 Accuracy 0.6352\n",
            "Epoch 8 Batch 300 Loss 1.0827 Accuracy 0.6357\n",
            "Epoch 8 Batch 350 Loss 1.0807 Accuracy 0.6363\n",
            "Epoch 8 Batch 400 Loss 1.0791 Accuracy 0.6368\n",
            "Epoch 8 Loss 1.0777 Accuracy 0.6373\n",
            "Time taken for 1 epoch: 94.78 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0388 Accuracy 0.6509\n",
            "Epoch 9 Batch 50 Loss 1.0418 Accuracy 0.6480\n",
            "Epoch 9 Batch 100 Loss 1.0427 Accuracy 0.6479\n",
            "Epoch 9 Batch 150 Loss 1.0411 Accuracy 0.6485\n",
            "Epoch 9 Batch 200 Loss 1.0421 Accuracy 0.6481\n",
            "Epoch 9 Batch 250 Loss 1.0413 Accuracy 0.6484\n",
            "Epoch 9 Batch 300 Loss 1.0400 Accuracy 0.6488\n",
            "Epoch 9 Batch 350 Loss 1.0386 Accuracy 0.6490\n",
            "Epoch 9 Batch 400 Loss 1.0381 Accuracy 0.6492\n",
            "Epoch 9 Loss 1.0371 Accuracy 0.6496\n",
            "Time taken for 1 epoch: 94.82 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0128 Accuracy 0.6580\n",
            "Epoch 10 Batch 50 Loss 1.0075 Accuracy 0.6583\n",
            "Epoch 10 Batch 100 Loss 1.0079 Accuracy 0.6581\n",
            "Epoch 10 Batch 150 Loss 1.0075 Accuracy 0.6584\n",
            "Epoch 10 Batch 200 Loss 1.0041 Accuracy 0.6596\n",
            "Epoch 10 Batch 250 Loss 1.0031 Accuracy 0.6599\n",
            "Epoch 10 Batch 300 Loss 1.0016 Accuracy 0.6606\n",
            "Epoch 10 Batch 350 Loss 0.9998 Accuracy 0.6612\n",
            "Epoch 10 Batch 400 Loss 0.9984 Accuracy 0.6616\n",
            "Epoch 10 Loss 0.9975 Accuracy 0.6619\n",
            "Time taken for 1 epoch: 94.84 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.9584 Accuracy 0.6764\n",
            "Epoch 11 Batch 50 Loss 0.9662 Accuracy 0.6712\n",
            "Epoch 11 Batch 100 Loss 0.9640 Accuracy 0.6719\n",
            "Epoch 11 Batch 150 Loss 0.9626 Accuracy 0.6721\n",
            "Epoch 11 Batch 200 Loss 0.9609 Accuracy 0.6727\n",
            "Epoch 11 Batch 250 Loss 0.9601 Accuracy 0.6730\n",
            "Epoch 11 Batch 300 Loss 0.9593 Accuracy 0.6734\n",
            "Epoch 11 Batch 350 Loss 0.9582 Accuracy 0.6737\n",
            "Epoch 11 Batch 400 Loss 0.9567 Accuracy 0.6742\n",
            "Epoch 11 Loss 0.9559 Accuracy 0.6744\n",
            "Time taken for 1 epoch: 94.73 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.9162 Accuracy 0.6842\n",
            "Epoch 12 Batch 50 Loss 0.9260 Accuracy 0.6838\n",
            "Epoch 12 Batch 100 Loss 0.9248 Accuracy 0.6840\n",
            "Epoch 12 Batch 150 Loss 0.9252 Accuracy 0.6841\n",
            "Epoch 12 Batch 200 Loss 0.9242 Accuracy 0.6845\n",
            "Epoch 12 Batch 250 Loss 0.9233 Accuracy 0.6846\n",
            "Epoch 12 Batch 300 Loss 0.9225 Accuracy 0.6848\n",
            "Epoch 12 Batch 350 Loss 0.9216 Accuracy 0.6850\n",
            "Epoch 12 Batch 400 Loss 0.9204 Accuracy 0.6853\n",
            "Epoch 12 Loss 0.9196 Accuracy 0.6855\n",
            "Time taken for 1 epoch: 94.76 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.9168 Accuracy 0.6895\n",
            "Epoch 13 Batch 50 Loss 0.8923 Accuracy 0.6948\n",
            "Epoch 13 Batch 100 Loss 0.8914 Accuracy 0.6952\n",
            "Epoch 13 Batch 150 Loss 0.8912 Accuracy 0.6948\n",
            "Epoch 13 Batch 200 Loss 0.8905 Accuracy 0.6952\n",
            "Epoch 13 Batch 250 Loss 0.8904 Accuracy 0.6951\n",
            "Epoch 13 Batch 300 Loss 0.8895 Accuracy 0.6954\n",
            "Epoch 13 Batch 350 Loss 0.8887 Accuracy 0.6957\n",
            "Epoch 13 Batch 400 Loss 0.8885 Accuracy 0.6957\n",
            "Epoch 13 Loss 0.8874 Accuracy 0.6962\n",
            "Time taken for 1 epoch: 94.71 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.8495 Accuracy 0.7040\n",
            "Epoch 14 Batch 50 Loss 0.8548 Accuracy 0.7063\n",
            "Epoch 14 Batch 100 Loss 0.8597 Accuracy 0.7048\n",
            "Epoch 14 Batch 150 Loss 0.8600 Accuracy 0.7046\n",
            "Epoch 14 Batch 200 Loss 0.8610 Accuracy 0.7041\n",
            "Epoch 14 Batch 250 Loss 0.8599 Accuracy 0.7045\n",
            "Epoch 14 Batch 300 Loss 0.8589 Accuracy 0.7048\n",
            "Epoch 14 Batch 350 Loss 0.8584 Accuracy 0.7050\n",
            "Epoch 14 Batch 400 Loss 0.8579 Accuracy 0.7051\n",
            "Epoch 14 Loss 0.8575 Accuracy 0.7053\n",
            "Time taken for 1 epoch: 94.71 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.8390 Accuracy 0.7098\n",
            "Epoch 15 Batch 50 Loss 0.8320 Accuracy 0.7126\n",
            "Epoch 15 Batch 100 Loss 0.8317 Accuracy 0.7131\n",
            "Epoch 15 Batch 150 Loss 0.8324 Accuracy 0.7128\n",
            "Epoch 15 Batch 200 Loss 0.8336 Accuracy 0.7125\n",
            "Epoch 15 Batch 250 Loss 0.8315 Accuracy 0.7132\n",
            "Epoch 15 Batch 300 Loss 0.8313 Accuracy 0.7133\n",
            "Epoch 15 Batch 350 Loss 0.8316 Accuracy 0.7131\n",
            "Epoch 15 Batch 400 Loss 0.8310 Accuracy 0.7135\n",
            "Epoch 15 Loss 0.8310 Accuracy 0.7136\n",
            "Time taken for 1 epoch: 94.60 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.7880 Accuracy 0.7260\n",
            "Epoch 16 Batch 50 Loss 0.7988 Accuracy 0.7237\n",
            "Epoch 16 Batch 100 Loss 0.8038 Accuracy 0.7224\n",
            "Epoch 16 Batch 150 Loss 0.8060 Accuracy 0.7218\n",
            "Epoch 16 Batch 200 Loss 0.8066 Accuracy 0.7213\n",
            "Epoch 16 Batch 250 Loss 0.8069 Accuracy 0.7212\n",
            "Epoch 16 Batch 300 Loss 0.8074 Accuracy 0.7210\n",
            "Epoch 16 Batch 350 Loss 0.8076 Accuracy 0.7210\n",
            "Epoch 16 Batch 400 Loss 0.8073 Accuracy 0.7211\n",
            "Epoch 16 Loss 0.8071 Accuracy 0.7210\n",
            "Time taken for 1 epoch: 94.61 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.7952 Accuracy 0.7241\n",
            "Epoch 17 Batch 50 Loss 0.7808 Accuracy 0.7302\n",
            "Epoch 17 Batch 100 Loss 0.7818 Accuracy 0.7301\n",
            "Epoch 17 Batch 150 Loss 0.7832 Accuracy 0.7295\n",
            "Epoch 17 Batch 200 Loss 0.7841 Accuracy 0.7289\n",
            "Epoch 17 Batch 250 Loss 0.7848 Accuracy 0.7285\n",
            "Epoch 17 Batch 300 Loss 0.7853 Accuracy 0.7283\n",
            "Epoch 17 Batch 350 Loss 0.7853 Accuracy 0.7282\n",
            "Epoch 17 Batch 400 Loss 0.7849 Accuracy 0.7283\n",
            "Epoch 17 Loss 0.7850 Accuracy 0.7283\n",
            "Time taken for 1 epoch: 94.72 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.7749 Accuracy 0.7327\n",
            "Epoch 18 Batch 50 Loss 0.7584 Accuracy 0.7370\n",
            "Epoch 18 Batch 100 Loss 0.7616 Accuracy 0.7360\n",
            "Epoch 18 Batch 150 Loss 0.7618 Accuracy 0.7357\n",
            "Epoch 18 Batch 200 Loss 0.7630 Accuracy 0.7355\n",
            "Epoch 18 Batch 250 Loss 0.7635 Accuracy 0.7354\n",
            "Epoch 18 Batch 300 Loss 0.7638 Accuracy 0.7352\n",
            "Epoch 18 Batch 350 Loss 0.7643 Accuracy 0.7349\n",
            "Epoch 18 Batch 400 Loss 0.7645 Accuracy 0.7350\n",
            "Epoch 18 Loss 0.7644 Accuracy 0.7349\n",
            "Time taken for 1 epoch: 94.72 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.7527 Accuracy 0.7368\n",
            "Epoch 19 Batch 50 Loss 0.7378 Accuracy 0.7429\n",
            "Epoch 19 Batch 100 Loss 0.7397 Accuracy 0.7425\n",
            "Epoch 19 Batch 150 Loss 0.7412 Accuracy 0.7424\n",
            "Epoch 19 Batch 200 Loss 0.7427 Accuracy 0.7421\n",
            "Epoch 19 Batch 250 Loss 0.7430 Accuracy 0.7419\n",
            "Epoch 19 Batch 300 Loss 0.7441 Accuracy 0.7414\n",
            "Epoch 19 Batch 350 Loss 0.7443 Accuracy 0.7413\n",
            "Epoch 19 Batch 400 Loss 0.7443 Accuracy 0.7413\n",
            "Epoch 19 Loss 0.7443 Accuracy 0.7413\n",
            "Time taken for 1 epoch: 94.80 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.7283 Accuracy 0.7443\n",
            "Epoch 20 Batch 50 Loss 0.7172 Accuracy 0.7495\n",
            "Epoch 20 Batch 100 Loss 0.7202 Accuracy 0.7490\n",
            "Epoch 20 Batch 150 Loss 0.7227 Accuracy 0.7485\n",
            "Epoch 20 Batch 200 Loss 0.7240 Accuracy 0.7482\n",
            "Epoch 20 Batch 250 Loss 0.7245 Accuracy 0.7481\n",
            "Epoch 20 Batch 300 Loss 0.7256 Accuracy 0.7477\n",
            "Epoch 20 Batch 350 Loss 0.7258 Accuracy 0.7476\n",
            "Epoch 20 Batch 400 Loss 0.7256 Accuracy 0.7476\n",
            "Epoch 20 Loss 0.7261 Accuracy 0.7474\n",
            "Time taken for 1 epoch: 94.80 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.7107 Accuracy 0.7543\n",
            "Epoch 21 Batch 50 Loss 0.6972 Accuracy 0.7573\n",
            "Epoch 21 Batch 100 Loss 0.7013 Accuracy 0.7553\n",
            "Epoch 21 Batch 150 Loss 0.7036 Accuracy 0.7546\n",
            "Epoch 21 Batch 200 Loss 0.7049 Accuracy 0.7541\n",
            "Epoch 21 Batch 250 Loss 0.7064 Accuracy 0.7536\n",
            "Epoch 21 Batch 300 Loss 0.7070 Accuracy 0.7534\n",
            "Epoch 21 Batch 350 Loss 0.7076 Accuracy 0.7532\n",
            "Epoch 21 Batch 400 Loss 0.7081 Accuracy 0.7531\n",
            "Epoch 21 Loss 0.7080 Accuracy 0.7531\n",
            "Time taken for 1 epoch: 94.80 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.6692 Accuracy 0.7662\n",
            "Epoch 22 Batch 50 Loss 0.6813 Accuracy 0.7623\n",
            "Epoch 22 Batch 100 Loss 0.6845 Accuracy 0.7609\n",
            "Epoch 22 Batch 150 Loss 0.6861 Accuracy 0.7604\n",
            "Epoch 22 Batch 200 Loss 0.6878 Accuracy 0.7597\n",
            "Epoch 22 Batch 250 Loss 0.6890 Accuracy 0.7593\n",
            "Epoch 22 Batch 300 Loss 0.6901 Accuracy 0.7589\n",
            "Epoch 22 Batch 350 Loss 0.6909 Accuracy 0.7587\n",
            "Epoch 22 Batch 400 Loss 0.6913 Accuracy 0.7586\n",
            "Epoch 22 Loss 0.6917 Accuracy 0.7585\n",
            "Time taken for 1 epoch: 94.81 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.6570 Accuracy 0.7693\n",
            "Epoch 23 Batch 50 Loss 0.6650 Accuracy 0.7677\n",
            "Epoch 23 Batch 100 Loss 0.6681 Accuracy 0.7665\n",
            "Epoch 23 Batch 150 Loss 0.6703 Accuracy 0.7656\n",
            "Epoch 23 Batch 200 Loss 0.6715 Accuracy 0.7653\n",
            "Epoch 23 Batch 250 Loss 0.6721 Accuracy 0.7650\n",
            "Epoch 23 Batch 300 Loss 0.6728 Accuracy 0.7647\n",
            "Epoch 23 Batch 350 Loss 0.6735 Accuracy 0.7645\n",
            "Epoch 23 Batch 400 Loss 0.6747 Accuracy 0.7641\n",
            "Epoch 23 Loss 0.6753 Accuracy 0.7639\n",
            "Time taken for 1 epoch: 94.76 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.6398 Accuracy 0.7791\n",
            "Epoch 24 Batch 50 Loss 0.6508 Accuracy 0.7725\n",
            "Epoch 24 Batch 100 Loss 0.6510 Accuracy 0.7725\n",
            "Epoch 24 Batch 150 Loss 0.6550 Accuracy 0.7710\n",
            "Epoch 24 Batch 200 Loss 0.6571 Accuracy 0.7703\n",
            "Epoch 24 Batch 250 Loss 0.6586 Accuracy 0.7697\n",
            "Epoch 24 Batch 300 Loss 0.6593 Accuracy 0.7694\n",
            "Epoch 24 Batch 350 Loss 0.6596 Accuracy 0.7692\n",
            "Epoch 24 Batch 400 Loss 0.6600 Accuracy 0.7692\n",
            "Epoch 24 Loss 0.6605 Accuracy 0.7690\n",
            "Time taken for 1 epoch: 94.79 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.6105 Accuracy 0.7838\n",
            "Epoch 25 Batch 50 Loss 0.6356 Accuracy 0.7776\n",
            "Epoch 25 Batch 100 Loss 0.6387 Accuracy 0.7763\n",
            "Epoch 25 Batch 150 Loss 0.6400 Accuracy 0.7757\n",
            "Epoch 25 Batch 200 Loss 0.6416 Accuracy 0.7752\n",
            "Epoch 25 Batch 250 Loss 0.6424 Accuracy 0.7750\n",
            "Epoch 25 Batch 300 Loss 0.6434 Accuracy 0.7746\n",
            "Epoch 25 Batch 350 Loss 0.6451 Accuracy 0.7740\n",
            "Epoch 25 Batch 400 Loss 0.6455 Accuracy 0.7738\n",
            "Epoch 25 Loss 0.6458 Accuracy 0.7737\n",
            "Time taken for 1 epoch: 94.78 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.6284 Accuracy 0.7778\n",
            "Epoch 26 Batch 50 Loss 0.6219 Accuracy 0.7819\n",
            "Epoch 26 Batch 100 Loss 0.6233 Accuracy 0.7813\n",
            "Epoch 26 Batch 150 Loss 0.6249 Accuracy 0.7808\n",
            "Epoch 26 Batch 200 Loss 0.6275 Accuracy 0.7801\n",
            "Epoch 26 Batch 250 Loss 0.6292 Accuracy 0.7796\n",
            "Epoch 26 Batch 300 Loss 0.6297 Accuracy 0.7792\n",
            "Epoch 26 Batch 350 Loss 0.6301 Accuracy 0.7791\n",
            "Epoch 26 Batch 400 Loss 0.6313 Accuracy 0.7786\n",
            "Epoch 26 Loss 0.6321 Accuracy 0.7784\n",
            "Time taken for 1 epoch: 94.87 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.6165 Accuracy 0.7894\n",
            "Epoch 27 Batch 50 Loss 0.6070 Accuracy 0.7863\n",
            "Epoch 27 Batch 100 Loss 0.6071 Accuracy 0.7866\n",
            "Epoch 27 Batch 150 Loss 0.6100 Accuracy 0.7857\n",
            "Epoch 27 Batch 200 Loss 0.6115 Accuracy 0.7853\n",
            "Epoch 27 Batch 250 Loss 0.6137 Accuracy 0.7846\n",
            "Epoch 27 Batch 300 Loss 0.6151 Accuracy 0.7840\n",
            "Epoch 27 Batch 350 Loss 0.6160 Accuracy 0.7836\n",
            "Epoch 27 Batch 400 Loss 0.6175 Accuracy 0.7831\n",
            "Epoch 27 Loss 0.6180 Accuracy 0.7830\n",
            "Time taken for 1 epoch: 94.82 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.5835 Accuracy 0.7937\n",
            "Epoch 28 Batch 50 Loss 0.5945 Accuracy 0.7913\n",
            "Epoch 28 Batch 100 Loss 0.5977 Accuracy 0.7898\n",
            "Epoch 28 Batch 150 Loss 0.5994 Accuracy 0.7891\n",
            "Epoch 28 Batch 200 Loss 0.6010 Accuracy 0.7890\n",
            "Epoch 28 Batch 250 Loss 0.6022 Accuracy 0.7884\n",
            "Epoch 28 Batch 300 Loss 0.6028 Accuracy 0.7882\n",
            "Epoch 28 Batch 350 Loss 0.6036 Accuracy 0.7879\n",
            "Epoch 28 Batch 400 Loss 0.6041 Accuracy 0.7878\n",
            "Epoch 28 Loss 0.6047 Accuracy 0.7876\n",
            "Time taken for 1 epoch: 94.76 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.5910 Accuracy 0.7911\n",
            "Epoch 29 Batch 50 Loss 0.5811 Accuracy 0.7958\n",
            "Epoch 29 Batch 100 Loss 0.5838 Accuracy 0.7950\n",
            "Epoch 29 Batch 150 Loss 0.5866 Accuracy 0.7941\n",
            "Epoch 29 Batch 200 Loss 0.5890 Accuracy 0.7933\n",
            "Epoch 29 Batch 250 Loss 0.5899 Accuracy 0.7927\n",
            "Epoch 29 Batch 300 Loss 0.5909 Accuracy 0.7925\n",
            "Epoch 29 Batch 350 Loss 0.5915 Accuracy 0.7922\n",
            "Epoch 29 Batch 400 Loss 0.5921 Accuracy 0.7920\n",
            "Epoch 29 Loss 0.5929 Accuracy 0.7917\n",
            "Time taken for 1 epoch: 94.53 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.5921 Accuracy 0.7914\n",
            "Epoch 30 Batch 50 Loss 0.5652 Accuracy 0.8009\n",
            "Epoch 30 Batch 100 Loss 0.5700 Accuracy 0.7995\n",
            "Epoch 30 Batch 150 Loss 0.5725 Accuracy 0.7988\n",
            "Epoch 30 Batch 200 Loss 0.5744 Accuracy 0.7982\n",
            "Epoch 30 Batch 250 Loss 0.5761 Accuracy 0.7975\n",
            "Epoch 30 Batch 300 Loss 0.5774 Accuracy 0.7971\n",
            "Epoch 30 Batch 350 Loss 0.5782 Accuracy 0.7969\n",
            "Epoch 30 Batch 400 Loss 0.5792 Accuracy 0.7965\n",
            "Epoch 30 Loss 0.5799 Accuracy 0.7963\n",
            "Time taken for 1 epoch: 94.37 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWt3LC2Zl7UN"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE6QB21ul7UO"
      },
      "source": [
        "def evaluate(\n",
        "    transformer,\n",
        "    encoder_input,\n",
        "    decoder_input,\n",
        "    stop_symbol,\n",
        "    max_length=200,\n",
        "):\n",
        "    \"\"\"\n",
        "    Predicts the output of the model given the `input_sequence`.\n",
        "    The `input_sequence` is encoded by the Encoder, then its output is fed to the Decoder,\n",
        "    whose output is fed back into the Decoder until the `stop_symbol` token is produced.\n",
        "\n",
        "    This function works with a batch of inputs and stops when all outputs include a stop symbol.\n",
        "    \"\"\"\n",
        "\n",
        "    output = decoder_input\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output\n",
        "    )\n",
        "\n",
        "    enc_output = transformer.encoder(\n",
        "        encoder_input, False, enc_padding_mask\n",
        "    )  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, _ = transformer.decoder(\n",
        "            output, enc_output, False, combined_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = transformer.final_layer(dec_output)\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predicted_ids = tf.argmax(predictions[:, -1:, :], axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [\n",
        "                tf.cast(output, dtype=tf.int32),\n",
        "                tf.cast(predicted_ids, dtype=tf.int32),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "                \n",
        "        if sum(output.numpy()[0] == stop_symbol) == 4:\n",
        "            print('Stopped')\n",
        "            return output\n",
        "\n",
        "    return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4kkUFG5l7UO"
      },
      "source": [
        "def generate(transformer, input_sequence, target_sequence, input_tokenizer, target_tokenizer, steps, start_symbol, stop_symbol):\n",
        "\n",
        "    result = target_tokenizer.sequences_to_texts(target_sequence)[0]\n",
        "    \n",
        "    encoder_input = input_sequence\n",
        "    decoder_input = target_sequence\n",
        "\n",
        "    for _ in range(steps):\n",
        "\n",
        "        encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "        output = evaluate(transformer, encoder_input, decoder_input, stop_symbol)\n",
        "\n",
        "        generated_text = target_tokenizer.sequences_to_texts(output.numpy())[0]\n",
        "        \n",
        "        verses = [line.lstrip() + '<EOV> ' for line in generated_text.split('<EOV>') if line.strip() != '']\n",
        "        \n",
        "        result = ''.join([result, verses[-1]])\n",
        "                \n",
        "        verses = ''.join(verses[-3:])\n",
        "        \n",
        "        decoder_input = target_tokenizer.texts_to_sequences([verses])\n",
        "        \n",
        "        verses = remove_syll_token(verses)\n",
        "        verses = re.sub(r\"[ ]+\", \"\", verses)\n",
        "        verses = re.sub(\"<[^>]*>\", \" \\g<0> \", verses)\n",
        "        verses = re.sub(\"<EOV>  <GO>\", \"<EOV> <GO>\", verses)\n",
        "        verses = verses.strip()\n",
        "\n",
        "        encoder_input = input_tokenizer.texts_to_sequences([verses])\n",
        "        \n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5ZwOIEml7UP",
        "outputId": "be7913e0-f20b-438a-91bc-8cd11787bf2c"
      },
      "source": [
        "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
        "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]\n",
        "\n",
        "encoder_input = [input_text[0]]\n",
        "decoder_input = [target_text_tercet[0]]\n",
        "\n",
        "result = generate(transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksO2x7J9l7UP",
        "outputId": "d2a7af9e-b88e-4c01-da84-7a4c291a159a"
      },
      "source": [
        "print(strip_tokens(result))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|Nel |mez|zo |del |cam|min |di |no|stra |vi|ta\n",
            "|mi |ri|tro|vai |per |u|na |sel|va o|scu|ra,\n",
            "|ché |la |di|rit|ta |via |e|ra |smar|ri|ta.\n",
            "|E |quel|l’ om|bre, |che |più |non |si |di|scer|ne,\n",
            "|co|me |fu |l’ uom |che |non |suo |più |si |puo|te\n",
            "|da |tut|te |l’ al|tre |par|ti |già |di|vi|na.\n",
            "|La |tua |cit|tà |co|sì |com’ |io |ti |fio|ri\n",
            "|a |ri|guar|dar |de |le |sa|cre |che |be|ne,\n",
            "|quan|do |non |so |chi |son |si |fuor |di|ser|ra».\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U46tRTY3l7UP"
      },
      "source": [
        "## 5. Syllabification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ZfhBFul7UQ"
      },
      "source": [
        "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
        "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a927vivxl7UQ"
      },
      "source": [
        "encoder_input = tf.convert_to_tensor([input_text[0]])\n",
        "decoder_input = tf.convert_to_tensor([[start_symbol]])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibb9wgPFl7UQ",
        "outputId": "79f0f9f7-6740-4c28-e9cc-30115f4b6d91"
      },
      "source": [
        "syll_output = evaluate(transformer, encoder_input, decoder_input, stop_symbol, max_length=400)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfDuUYrrl7UR",
        "outputId": "1e6147a4-69af-46d6-ae9b-417b61c3eab7"
      },
      "source": [
        "print(target_tokenizer.sequences_to_texts(syll_output.numpy()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<GO> | p e r <SEP> | c h e <SEP> | l a <SEP> | v i | s t a <SEP> | m i a <SEP> | s o n <SEP> | l i <SEP> | d i | s e r | r a . <EOV> <GO> | Q u a n | d o <SEP> | n ’ <SEP> a | s c o n | d e | r e <SEP> i l <SEP> | m i o <SEP> | c a | p o <SEP> | f i | g l i o , <EOV> <GO> | u | d i | r e <SEP> e <SEP> | a l | t r o <SEP> a <SEP> | n o i , <SEP> | s e <SEP> | t u <SEP> | a | v e | r e <EOV> <GO> | c o | m e <SEP> | t u <SEP> | a <SEP> | t e <SEP> | c h e <SEP> | s i <SEP> | f a <SEP> | c o n | t e n | t a . <EOV>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Z5RipHl7UR"
      },
      "source": [
        "Potrebbe essere underfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sBN_jztl7US"
      },
      "source": [
        "## 6. Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvky6cw6l7US"
      },
      "source": [
        "transformer.save_weights('models/c2c-gen.h5')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1DpD4bil7US"
      },
      "source": [
        "new_transformer = Transformer(\n",
        "        num_layers=config[\"num_layers\"],\n",
        "        d_model=config[\"d_model\"],\n",
        "        num_heads=config[\"num_heads\"],\n",
        "        dff=config[\"dff\"],\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        pe_input=1000,\n",
        "        pe_target=1000,\n",
        "        rate=0.1,\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBadl6dCl7UT"
      },
      "source": [
        "# In order to load the new weights the model should be called once for the variables to be initialized\n",
        "\n",
        "# Any inp, tar is ok here\n",
        "inp = tf.convert_to_tensor([[start_symbol]])\n",
        "tar = tf.convert_to_tensor([[start_symbol]])\n",
        "\n",
        "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "\n",
        "new_transformer(inp, tar, False, enc_padding_mask, look_ahead_mask, dec_padding_mask);"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEZX2gSMl7UT"
      },
      "source": [
        "new_transformer.load_weights('models/c2c-gen.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eLZUmavl7UU",
        "outputId": "8ee73ec4-d5e7-47aa-f8a0-8853ba72a828"
      },
      "source": [
        "encoder_input = [input_text[0]]\n",
        "decoder_input = [target_text_tercet[0]]\n",
        "\n",
        "result = generate(new_transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n",
            "Stopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "T6bW9_1Ul7UV",
        "outputId": "f193be47-0c4d-44b6-9ce7-330f196046c5"
      },
      "source": [
        "result"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<GO> | N e l <SEP> | m e z | z o <SEP> | d e l <SEP> | c a m | m i n <SEP> | d i <SEP> | n o | s t r a <SEP> | v i | t a <EOV> <GO> | m i <SEP> | r i | t r o | v a i <SEP> | p e r <SEP> | u | n a <SEP> | s e l | v a <SEP> o | s c u | r a , <EOV> <GO> | c h é <SEP> | l a <SEP> | d i | r i t | t a <SEP> | v i a <SEP> | e | r a <SEP> | s m a r | r i | t a . <EOV><GO> | E <SEP> | q u e l | l ’ <SEP> o m | b r e , <SEP> | c h e <SEP> | p i ù <SEP> | n o n <SEP> | s i <SEP> | d i | s c e r | n e , <EOV> <GO> | c o | m e <SEP> | f u <SEP> | l ’ <SEP> u o m <SEP> | c h e <SEP> | n o n <SEP> | s u o <SEP> | p i ù <SEP> | s i <SEP> | p u o | t e <EOV> <GO> | d a <SEP> | t u t | t e <SEP> | l ’ <SEP> a l | t r e <SEP> | p a r | t i <SEP> | g i à <SEP> | d i | v i | n a . <EOV> <GO> | L a <SEP> | t u a <SEP> | c i t | t à <SEP> | c o | s ì <SEP> | c o m ’ <SEP> | i o <SEP> | t i <SEP> | f i o | r i <EOV> <GO> | a <SEP> | r i | g u a r | d a r <SEP> | d e <SEP> | l e <SEP> | s a | c r e <SEP> | c h e <SEP> | b e | n e , <EOV> <GO> | q u a n | d o <SEP> | n o n <SEP> | s o <SEP> | c h i <SEP> | s o n <SEP> | s i <SEP> | f u o r <SEP> | d i | s e r | r a » . <EOV> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wBbMbjNf0ol-",
        "outputId": "97c77b14-55c6-457b-fa08-0a69d13dc9ce"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    files.download('models/c2c-gen.h5')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b3dfb62a-1595-4f88-82d9-3bc06c97ff46\", \"c2c-gen.h5\", 32158680)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}