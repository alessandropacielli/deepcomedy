{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from deepcomedy.models.transformer import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT"
   },
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "input_file = \"data/divina_textonly.txt\"\n",
    "target_file = \"data/divina_syll_textonly.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "ACAEUyITY1dY",
    "outputId": "26daf544-3700-428a-c752-b10e0bbd6d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text: 558637 characters\n",
      "Length of target text: 873431 characters\n"
     ]
    }
   ],
   "source": [
    "input_text_raw = open(input_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "target_text_raw = open(target_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(\"Length of input text: {} characters\".format(len(input_text_raw)))\n",
    "print(\"Length of target text: {} characters\".format(len(target_text_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gradient": {},
    "id": "l1G45yR9Y1da"
   },
   "outputs": [],
   "source": [
    "input_vocab = sorted(set(input_text_raw))\n",
    "target_vocab = sorted(set(target_text_raw))\n",
    "input_vocab_size = len(input_vocab)\n",
    "target_vocab_size = len(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "p-w27LhpY1db",
    "outputId": "d4061a9c-09a0-46c1-b0a0-501d897ab0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 79\n",
      "Target vocab size: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
    "print(\"Target vocab size: {}\".format(target_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jmfE9GWu4KW"
   },
   "source": [
    "The *preprocess* function adds the start and end symbols to each line and eliminates the empty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gradient": {},
    "id": "SUsvn6SqY1dd"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
    "    \"\"\"\n",
    "    return [\"^\" + line.strip() + \"$\" for line in text.split(\"\\n\") if line.strip() != \"\"]\n",
    "\n",
    "\n",
    "input_text_prepr = preprocess(input_text_raw)\n",
    "target_text_prepr = preprocess(target_text_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMrXOt8uvC6j"
   },
   "source": [
    "The tokenizer encodes each line into a tensor of char-indexes and for simplicity fits only on the target's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {},
    "id": "ABb_2K6DY1de"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", char_level=True, lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_text_prepr)\n",
    "\n",
    "input_text_lines_enc = tokenizer.texts_to_sequences(input_text_prepr)\n",
    "target_text_lines_enc = tokenizer.texts_to_sequences(target_text_prepr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4bfzvcNvT-y"
   },
   "source": [
    "Padding is required in order to have a non-ragged tensor to feed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {},
    "id": "MBOh9LQeY1dg"
   },
   "outputs": [],
   "source": [
    "def pad(x):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {},
    "id": "9zV0xz48Y1dh"
   },
   "outputs": [],
   "source": [
    "input_text = pad(input_text_lines_enc)\n",
    "target_text = pad(target_text_lines_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_text, target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IN8x175vimK"
   },
   "source": [
    "The dataset is created by grouping the lines in batches and by shuffling them.\n",
    "\n",
    "Each input's line is in correspondence with its target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {},
    "id": "tZWLq7g3Y1dl"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "\n",
    "vocab_size = (\n",
    "    len(tokenizer.word_index) + 1\n",
    ")  # the +1 is added to take into account the id 0 of the padding\n",
    "\n",
    "max_length_targ, max_length_inp = target_text.shape[1], input_text.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
    "    BUFFER_SIZE\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {},
    "id": "LrdL396xqOL4"
   },
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    target_vocab_size=vocab_size,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_trainer = TransformerTrainer(transformer, checkpoint_save_path='./checkpoints/char-level-syll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9835 Accuracy 0.0016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-27da7ad339f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/deepcomedy/models/transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/.venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITVGgP8Su9MH"
   },
   "source": [
    "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz4YwsF04YEI"
   },
   "source": [
    "## 4. Syllabification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O44l1saVuebS"
   },
   "source": [
    "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
    "\n",
    "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
    "\n",
    "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
    "\n",
    "The translation stops when the end symbol is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gradient": {},
    "id": "BSNaKtSkvxcJ"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=200):\n",
    "\n",
    "    encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "\n",
    "    output = tf.convert_to_tensor([tokenizer.word_index[\"^\"]])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    result = \"\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
    "            axis=-1,\n",
    "        )\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
    "            break\n",
    "\n",
    "    # output.shape (1, tokens)\n",
    "\n",
    "    return result, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gradient": {},
    "id": "PkCDxGdeyF9f"
   },
   "outputs": [],
   "source": [
    "def print_translation(sentence, result, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {result}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5cULw_54F8w",
    "outputId": "c5591588-ffd5-43cb-9627-7398a9a6813c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : ^E come l’aere, quand’ è ben pïorno,$\n",
      "Prediction     : | E   | c o | m e   | l ’   a e | r e ,   | q u a n | d ’   è   | b e n   | p ï | o r | n o , $ \n",
      "Ground truth   : |E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\n"
     ]
    }
   ],
   "source": [
    "sentence = \"^E come l’aere, quand’ è ben pïorno,$\"\n",
    "ground_truth = \"|E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\"\n",
    "\n",
    "\n",
    "translated_text, attention_weights = evaluate(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Comedy transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
