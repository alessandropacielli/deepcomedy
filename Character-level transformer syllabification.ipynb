{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syllabification experiments using Transformers\n",
    "\n",
    "In this notebook we show our first experiments using the Transformer architecture to build a syllabifier. The training and the hyperparameters are not optimal (just 10 epochs, no hyperparameter sweeps performed), however we got pretty good results and this inspired us to keep working on this architecture. Other experiments can be found in the `Char2Char` and `Word2Char` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import wandb\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from deepcomedy.utils import *\n",
    "from deepcomedy.metrics import *\n",
    "\n",
    "import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT"
   },
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_tercet=\"\")\n",
    "text = preprocess_text(raw_text, end_of_tercet=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpbut2Vb_fU"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Avfk31uHblz8"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOV>\"\n",
    "input_verses = [x + sep for x in text.split(sep)][:-1]\n",
    "target_verses = [x + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVxwIU4gcGQe"
   },
   "source": [
    "Encode with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hUvF7DRscJTo"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_verses)\n",
    "enc_input_verses = tokenizer.texts_to_sequences(input_verses)\n",
    "enc_target_verses = tokenizer.texts_to_sequences(target_verses)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMv3NIsNcQ-d"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iAn-XqEFcT5h"
   },
   "outputs": [],
   "source": [
    "input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_input_verses, padding=\"post\"\n",
    ")\n",
    "target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_target_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1kX2bojP72Vq"
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_text, target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = make_dataset(input_train, target_train, batch_size=batch_size)\n",
    "validation_dataset = make_dataset(input_test, target_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaKKUqB726YI",
    "outputId": "4c447512-5078-4c7a-9021-c1f233fd8def"
   },
   "outputs": [],
   "source": [
    "best_config = {\"num_layers\": 4, \"d_model\": 256, \"num_heads\": 4, \"dff\": 1024}\n",
    "\n",
    "transformer, transformer_trainer = make_transformer_model(\n",
    "    best_config, vocab_size, vocab_size, checkpoint_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer_trainer.train(dataset, 10, validation_dataset=validation_dataset, validation_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_transformer_model(transformer, 'models/initial_syllabification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\" : 4,\n",
    "    \"d_model\" : 256,\n",
    "    \"num_heads\" : 4,\n",
    "    \"dff\" : 1024,\n",
    "}\n",
    "transformer = load_transformer_model(config, vocab_size, vocab_size, tokenizer, 'models/initial_syllabification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Syllabification\n",
    "\n",
    "### 3.1. Syllabification example \n",
    "Here we try to syllabify the first 100 verses of the test set. We use the evaluate function to pass the input to the model and autoregressively generate the output from the transformer in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = tokenizer.word_index['<GO>']\n",
    "stop_symbol = tokenizer.word_index['<EOV>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.convert_to_tensor(input_test[:100])\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|lu|cen|te |più |as|sai |di |quel |ch’ el|l’ e|ra.',\n",
       " '|che |si |sta|va|no a |l’ om|bra |die|tro al |sas|so',\n",
       " '|Poi, |ral|lar|ga|ti |per |la |stra|da |so|la,',\n",
       " '|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|to,',\n",
       " '|e |co|me |quel |ch’ è |pa|sto |la |ri|mi|ra;',\n",
       " '|con |le |quai |la |tua |E|ti|ca |per|trat|ta',\n",
       " '|ma |noi |siam |pe|re|grin |co|me |voi |sie|te.',\n",
       " '|La |lin|gua |ch’ io |par|lai |fu |tut|ta |spen|ta',\n",
       " '|che |guar|da ’l |pon|te, |che |Fio|ren|za |fes|se',\n",
       " '« |Io |sa|rò |pri|mo, e |tu |sa|rai |se|con|do».',\n",
       " '|por|re un |uom |per |lo |po|po|lo a’ |mar|tì|ri.',\n",
       " '|pri|ma |che |pos|sa |tut|ta in |sé |mu|tar|si;',\n",
       " '|con|tra ’l |di|sio, |fo |ben |ch’ io |non |dio|man|do”.',\n",
       " '|se |non |co|me |tri|sti|zia o |se|te o |fa|me:',\n",
       " '|vie |più |lu|cen|do, |co|min|cia|ron |can|ti',\n",
       " '|E |se |più |fu |lo |suo |par|lar |dif|fu|so,',\n",
       " '|a |Ce|pe|ran, |là |do|ve |fu |bu|giar|do',\n",
       " '|al |mio |di|sio |cer|ti|fi|ca|to |fer|mi.',\n",
       " '|non |fos|se |sta|ta a |Ce|sa|re |no|ver|ca,',\n",
       " '|pro|du|ce, e |cen|cri |con |an|fi|si|be|na,',\n",
       " '|che |mi |sco|lo|ra», |pre|ga|va,« |la |pel|le,',\n",
       " '|Noi |e|ra|vam |par|ti|ti |già |da |el|lo,',\n",
       " '|rac|co|man|dò |la |don|na |sua |più |ca|ra,',\n",
       " '|u|di|to |que|sto, |quan|do al|cu|na |pian|ta',\n",
       " '|e |giù |dal |col|lo |de |la |ri|pa |du|ra',\n",
       " '|Ahi |quan|to |mi |pa|rea |pien |di |di|sde|gno!',\n",
       " '|Ne |li oc|chi e|ra |cia|scu|na o|scu|ra e |ca|va,',\n",
       " '|que|sta |gran |tem|po |per |lo |mon|do |gio.',\n",
       " '|nel |mon|tar |sù, |co|sì |sa|rà |nel |ca|lo.',\n",
       " '|se|gue, |co|me ’l |ma|e|stro |fa ’l |di|scen|te;',\n",
       " '“ An|da|te, e |pre|di|ca|te al |mon|do |cian|ce”;',\n",
       " '|be|stia |mal|va|gia |che |co|là |si |cor|ca».',\n",
       " '|io |co|min|ciai:« El |par |che |tu |mi |nie|ghi,',\n",
       " '|o |de |la |pro|pria o |de |l’ al|trui |ver|go|gna',\n",
       " '|Que|sto |pas|sam|mo |co|me |ter|ra |du|ra;',\n",
       " '|di |qua, |di |là |soc|cor|rien |con |le |ma|ni',\n",
       " '|e |l’ i|dï|o|o |ch’ u|sai |e |che |fei.',\n",
       " '|gri|dò:« |Qual |io |fui |vi|vo, |tal |son |mor|to.',\n",
       " '|e |quel |di|la|ce|ra|ro a |bra|no a |bra|no;',\n",
       " '|Cre|d’ ï|oo |ch’ ei |cre|det|te |ch’ io |cre|des|se',\n",
       " '|Io |fui |di |Mon|tel|te|ro, io |son |Bon|con|te;',\n",
       " '|E |io |a |lui:« |Con |pian|ge|re e |con |lut|to,',\n",
       " '|ché |di |giu|sto |vo|ler |lo |suo |si |fa|ce:',\n",
       " '|Me|noc|ci o|ve |la |roc|cia e|ra |ta|glia|ta;',\n",
       " '|che |fu |som|mo |can|tor |del |som|mo |du|ce.',\n",
       " '|ne |la |mi|se|ria |do|ve |tu |mi |ve|di,',\n",
       " '|e |vi|de|mi e |co|nob|be|mi e |chia|ma|va,',\n",
       " '|e |driz|zò |li oc|chi al |ciel,« |che |ti |fia |chia|ro',\n",
       " '|de |l’ am|pio |lo|co o|ve |tor|nar |tu |ar|di”.',\n",
       " '|es|ser |po|rà |ch’ al |ve|der |non |vi |nòi».',\n",
       " '|che al |giu|di|cio |di|vin |pas|sion |com|por|ta?',\n",
       " '|che |di |sù|bi|to |chie|de o|ve |s’ ar|re|sta,',\n",
       " '|non |so|nò |sì |ter|ri|bil|men|te Or|lan|do.',\n",
       " '|poi, |di|ven|tan|do |l’ un |di |que|sti |se|gni,',\n",
       " '|E |non |re|stò |di |rui|na|re a |val|le',\n",
       " '|e al|tra è |quel|la |c’ ha |l’ a|ni|ma in|te|ra:',\n",
       " '|E |io |a |lui:« |Chi |son |li |due |ta|pi|ni',\n",
       " '|che, |per |ve|der, |non |in|du|gia ’l |par|ti|re:',\n",
       " '|per |frat|ta |nu|be, |già |pra|to |di |fio|ri',\n",
       " '|cin|que|cen|t’ an|ni e |più, |pur |mo |sen|tii',\n",
       " '« |Voi |vi|gi|la|te |ne |l’ et|ter|no |die,',\n",
       " '|de |l’ o|ro, |l’ ap|pe|to |do |dor|mor|ta|li?”,',\n",
       " '|e |cu’ |io |vi|di |su |in |ter|ra |la|ti|na,',\n",
       " '|ben |co|nob|bi il |ve|len |de |l’ ar|go|men|to.',\n",
       " '|a|ni|me |for|tu|na|te |tut|te |quan|te,',\n",
       " '|per |do|man|dar |la |mia |don|na |di |co|se',\n",
       " '|co|me ’l |se|gno |del |mon|do e |de’ |suoi |du|ci',\n",
       " '|e |per |co|lei |che ’l |lo|co |pri|ma |e|ses|se,',\n",
       " '|Ma |ta|le uc|cel |nel |bec|chet|to |s’ an|ni|da,',\n",
       " '|cer|to |non |ti |do|vrien |pun|ger |li |stra|li',\n",
       " '|già |di |be|re a |For|lì |con |men |sec|chez|za,',\n",
       " '|ché |poi |non |si |po|ria, |se ’l |dì |non |rie|de».',\n",
       " '|pur |di |non |per|der |tem|po, |sì |che ’n |quel|la',\n",
       " '|cia|scun |da |l’ al|tra |co|sta |li oc|chi |vol|se,',\n",
       " '|co|me |suol |se|gui|tar |per |al|cun |ca|so,',\n",
       " '|De |la |pro|fon|da |con|di|zion |di|vi|na',\n",
       " '|ri|spon|der |lei |con |vi|so |tem|pe|ra|to:',\n",
       " '|E |ag|gi a |men|te, |quan|do |tu |le |scri|vi,',\n",
       " '|con|ti|nü|ò |co|sì ’l |pro|ces|so |san|to:',\n",
       " '|a |quel |par|lar |che |mi |pa|rea |ne|mi|co.',\n",
       " '|che |pur |con |ci|bi |di |li|quor |d’ u|li|vi',\n",
       " '« |e |che |fai |d’ es|se |tal|vol|ta |ta|gla|glie,',\n",
       " '|ma |die|de |lor |ve|ra|ce |fon|da|men|to;',\n",
       " '|se|gui|tar |lei |per |tut|to |l’ in|no in|te|ro,',\n",
       " '|se |quel|la |con |ch’ io |par|lo |non |si |sec|ca».',\n",
       " '|Que|sta |na|tu|ra |sì |ol|tre |s’ in|gra|da',\n",
       " '|che |si |mu|rò |di |se|gni e |di |mar|tì|ri.',\n",
       " '|lo |ciel |ve|nir |più |e |più |ri|schia|ran|do;',\n",
       " '|sì |che |buon |frut|to |ra|do |se |ne |schian|ta.',\n",
       " '|Pri|ma |vuol |ben, |ma |non |la|scia il |ta|len|to',\n",
       " '|qua |giù |di|mo|ra e |qua |sù |non |a|scen|de,',\n",
       " '|se|der |là |so|lo, Ar|ri|go |d’ In|ghil|ter|ra:',\n",
       " '|ben |puoi |ve|der |per|ch’ io |co|sì |ra|gio|no.',\n",
       " '|O |som|ma |sa|pï|en|za, |quan|ta è |l’ ar|te',\n",
       " '|ch’ a|mor |con|sun|se |co|me |sol |va|po|ri,',\n",
       " '|se |la |co|sa |di|mes|sa in |la |sor|pre|sa',\n",
       " '|tal |che |mi |vin|se e |guar|dar |nol |po|tei.',\n",
       " '|Lo |du|ca |stet|te un |po|co a |te|sta |chi|na;',\n",
       " '|to|glie|va |li a|ni|mai |che |so|no in |ter|ra',\n",
       " '|Que|ste |pa|ro|le |fuor |del |du|ca |mio;']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_syll = target_test[:100]\n",
    "correct_syll = ' '.join(tokenizer.sequences_to_texts(correct_syll))\n",
    "correct_syll = strip_tokens(correct_syll)\n",
    "correct_syll = correct_syll.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches, similarities = zip(*validate_syllabification(stripped_output, correct_syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(exact_matches) / len(exact_matches)\n",
    "avg_similarities = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabification exact matches: 76.00%\n"
     ]
    }
   ],
   "source": [
    "print('Syllabification exact matches: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity: {:.2f}'.format(avg_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = np.array(stripped_output)\n",
    "correct_syll = np.array(correct_syll)\n",
    "error_mask = ~np.array(exact_matches)\n",
    "\n",
    "errors_output = stripped_output[error_mask]\n",
    "errors_correct = correct_syll[error_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|to,'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_correct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|sco,'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Syllabification of the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate function can handle many syllabification tasks in parallel, generating each output sentence simultaneously until all outputs contain at least one \\<EOV\\> token. This is faster than handling one sentence at a time, however we found that giving the whole test set in parallel results in GPU out-of-memory, so we came up with this solution that seems to be a good trade-off between parallelism and memory consumption.\n",
    "\n",
    "What we do is split the test set in batches of 100 verses, and call `evaluate` on one batch at a time passing the appropriate stopping condition.\n",
    "\n",
    "As an empirical proof, try using a `window_size` of 1: you will see that the ETA will grow to ~3 hours, while the whole process only took 20 minutes in our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [20:38<00:00, 34.41s/it]\n"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm.tqdm(range(math.ceil(len(input_test) / window_size))):\n",
    "    window = input_test[i*window_size:min((i + 1)*window_size, len(input_test))]\n",
    "    \n",
    "    encoder_input = tf.convert_to_tensor(window)\n",
    "    decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "    \n",
    "    output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n",
    "    \n",
    "    # Only take output before the first end of verse\n",
    "    stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "    stripped_output = list(map(strip_tokens, stripped_output))\n",
    "    \n",
    "    result += stripped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3559 [00:26<3:48:15,  3.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "window_size = 1\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm.tqdm(range(math.ceil(len(input_test) / window_size))):\n",
    "    window = input_test[i*window_size:min((i + 1)*window_size, len(input_test))]\n",
    "    \n",
    "    encoder_input = tf.convert_to_tensor(window)\n",
    "    decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "    \n",
    "    output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n",
    "    \n",
    "    # Only take output before the first end of verse\n",
    "    stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "    stripped_output = list(map(strip_tokens, stripped_output))\n",
    "    \n",
    "    result += stripped_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the syllabification we got from our model with the correct syllabification. The `validate_syllabification` function returns information about the verses that were correctly syllabified and the Levenshtein similarity (1 - edit distance) of each syllabified verse with the correct syllabification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_syll = target_test\n",
    "correct_syll = ' '.join(tokenizer.sequences_to_texts(correct_syll))\n",
    "correct_syll = strip_tokens(correct_syll)\n",
    "correct_syll = correct_syll.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches, similarities = zip(*validate_syllabification(result, correct_syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(exact_matches) / len(exact_matches)\n",
    "avg_similarities = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabification exact matches: 86.40%\n"
     ]
    }
   ],
   "source": [
    "print('Syllabification exact matches: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity: {:.2f}'.format(avg_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = np.array(result)\n",
    "correct_syll = np.array(correct_syll)\n",
    "error_mask = ~np.array(exact_matches)\n",
    "\n",
    "errors_output = stripped_output[error_mask]\n",
    "errors_correct = correct_syll[error_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Syllabification of other poetry\n",
    "\n",
    "We thought it would be a fun experiment to see if the model could syllabify other poetry, not just hendecasyllabic verses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<GO> S e m p r e <SEP> c a r o <SEP> m i <SEP> f u <SEP> q u e s t ’ e r m o <SEP> c o l l e , <EOV>',\n",
       " '<GO> e <SEP> q u e s t a <SEP> s i e p e , <SEP> c h e <SEP> d a <SEP> t a n t a <SEP> p a r t e <EOV>',\n",
       " '<GO> d e l l ’ u l t i m o <SEP> o r i z z o n t e <SEP> i l <SEP> g u a r d o <SEP> e s c l u d e . <EOV>',\n",
       " '<GO> M a <SEP> s e d e n d o <SEP> e <SEP> m i r a n d o , <SEP> i n t e r m i n a t i <EOV>',\n",
       " '<GO> s p a z i <SEP> d i <SEP> l à <SEP> d a <SEP> q u e l l a , <SEP> e <SEP> s o v r u m a n i <EOV>',\n",
       " '<GO> s i l e n z i , <SEP> e <SEP> p r o f o n d i s s i m a <SEP> q u ï e t e <EOV>',\n",
       " '<GO> i o <SEP> n e l <SEP> p e n s i e r <SEP> m i <SEP> f i n g o ; <SEP> o v e <SEP> p e r <SEP> p o c o <EOV>',\n",
       " '<GO> i l <SEP> c o r <SEP> n o n <SEP> s i <SEP> s p a u r a . <SEP> E <SEP> c o m e <SEP> i l <SEP> v e n t o <EOV>',\n",
       " '<GO> o d o <SEP> s t o r m i r <SEP> t r a <SEP> q u e s t e <SEP> p i a n t e , <SEP> i o <SEP> q u e l l o <EOV>',\n",
       " '<GO> i n f i n i t o <SEP> s i l e n z i o <SEP> a <SEP> q u e s t a <SEP> v o c e <EOV>',\n",
       " '<GO> v o <SEP> c o m p a r a n d o : <SEP> e <SEP> m i <SEP> s o v v i e n <SEP> l ’ e t e r n o , <EOV>',\n",
       " '<GO> e <SEP> l e <SEP> m o r t e <SEP> s t a g i o n i , <SEP> e <SEP> l a <SEP> p r e s e n t e <EOV>',\n",
       " '<GO> e <SEP> v i v a , <SEP> e <SEP> i l <SEP> s u o n <SEP> d i <SEP> l e i . <SEP> C o s ì <SEP> t r a <SEP> q u e s t a <EOV>',\n",
       " '<GO> i m m e n s i t à <SEP> s ’ a n n e g a <SEP> i l <SEP> p e n s i e r <SEP> m i o : <EOV>',\n",
       " '<GO> e <SEP> i l <SEP> n a u f r a g a r <SEP> m ’ è <SEP> d o l c e <SEP> i n <SEP> q u e s t o <SEP> m a r e . <EOV>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbitrary_verses = \"\"\"\n",
    "Sempre caro mi fu quest’ermo colle,\n",
    "e questa siepe, che da tanta parte\n",
    "dell’ultimo orizzonte il guardo esclude.\n",
    "Ma sedendo e mirando, interminati\n",
    "spazi di là da quella, e sovrumani\n",
    "silenzi, e profondissima quïete\n",
    "io nel pensier mi fingo; ove per poco\n",
    "il cor non si spaura. E come il vento\n",
    "odo stormir tra queste piante, io quello\n",
    "infinito silenzio a questa voce\n",
    "vo comparando: e mi sovvien l’eterno,\n",
    "e le morte stagioni, e la presente\n",
    "e viva, e il suon di lei. Così tra questa\n",
    "immensità s’annega il pensier mio:\n",
    "e il naufragar m’è dolce in questo mare.\n",
    "\"\"\"\n",
    "\n",
    "arbitrary_verses = preprocess_text(arbitrary_verses, end_of_tercet='')\n",
    "arbitrary_verses = [verse.strip() + ' <EOV>' for verse in arbitrary_verses.split('<EOV>')][:-1]\n",
    "arbitrary_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_verses = tokenizer.texts_to_sequences(arbitrary_verses)\n",
    "padded_verses = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_input = tf.convert_to_tensor(padded_verses)\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "\n",
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|Sem|pre |ca|ro |mi |fu |que|st’ er|mo |col|le,',\n",
       " '|e |que|sta |sie|pe, |che |da |tan|ta |par|te',\n",
       " '|del|l’ ul|ti|mo o|riz|zon|te il |guar|do e|sclu|de.',\n",
       " '|Ma |se|den|do e |mi|ran|do, in|ter|mi|na|ti',\n",
       " '|spa|zi |di |là |da |quel|la, e |so|vru|ma|ni',\n",
       " '|si|len|zi, e |pro|fon|dis|si|ma |quï|e|te',\n",
       " '|io |nel |pen|sier |mi |fin|go; o|ve |per |po|co',\n",
       " '|il |cor |non |si |spau|ra. E |co|me il |ven|to',\n",
       " '|o|do |stor|mir |tra |que|ste |pian|te, io |quel|lo',\n",
       " '|in|fi|ni|to |si|len|zio a |que|sta |vo|ce',\n",
       " '|vo |com|pa|ran|do: e |mi |sov|vien |l’ e|ter|no,',\n",
       " '|e |le |mor|te |sta|gio|ni, e |la |pre|sen|te',\n",
       " '|e |vi|va, e |il |suon |di |lei. |Co|sì |tra |que|sta',\n",
       " '|im|men|si|tà |s’ an|ne|ga il |pen|sier |mio:',\n",
       " '|e il |nau|fra|gar |m’ è |dol|ce in |que|sto |ma|re.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay true to our Roman roots we also picked a classic folk roman song, which incidentally contains quite a few synalephas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<GO> È <SEP> u n a <SEP> c a n z o n e <SEP> s e n z a <SEP> t i t o l o <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e ’ <SEP> f a <SEP> q u a r c h e <SEP> c o s a <EOV>',\n",
       " '<GO> N o n <SEP> è <SEP> g n e n t e <SEP> d e <SEP> s t r a o r d i n a r i o <EOV>',\n",
       " '<GO> È <SEP> r o b b a <SEP> d e r <SEP> p a e s e <SEP> n o s t r o <EOV>',\n",
       " '<GO> C h e <SEP> s e <SEP> p o ’ <SEP> c a n t à <SEP> p u r e <SEP> s e n z a <SEP> v o c e <EOV>',\n",
       " '<GO> B a s t a <SEP> ’ a <SEP> s a l u t e <EOV>',\n",
       " \"<GO> Q u a n n o <SEP> c ' è <SEP> ' a <SEP> s a l u t e <SEP> c ' è <SEP> t u t t o <EOV>\",\n",
       " '<GO> B a s t a <SEP> ’ a <SEP> s a l u t e <SEP> e <SEP> u n <SEP> p a r <SEP> d e <SEP> s c a r p e <SEP> n o v e <EOV>',\n",
       " '<GO> P o i <SEP> g i r à <SEP> t u t t o <SEP> e r <SEP> m o n n o <EOV>',\n",
       " '<GO> E <SEP> m ’ a <SEP> a c c o m p a g n o <SEP> d a <SEP> m e <EOV>',\n",
       " '<GO> P e ’ <SEP> f a <SEP> l a <SEP> v i t a <SEP> m e n o <SEP> a m a r a <EOV>',\n",
       " \"<GO> M e <SEP> s o ’ <SEP> c o m p r a t o <SEP> ' s t a <SEP> c h i t a r a <EOV>\",\n",
       " '<GO> E <SEP> q u a n n o <SEP> e r <SEP> s o l e <SEP> s c e n n e <SEP> e <SEP> m o r e <EOV>',\n",
       " '<GO> M e <SEP> s e n t o <SEP> ’ n <SEP> c o r e <SEP> c a n t a t o r e <EOV>',\n",
       " '<GO> L a <SEP> v o c e <SEP> e ’ <SEP> p o c a <SEP> m a <SEP> ’ n t o n a t a <EOV>',\n",
       " '<GO> N u n <SEP> s e r v e <SEP> a <SEP> f a <SEP> ’ n a <SEP> s e r e n a t a <EOV>',\n",
       " \"<GO> M a <SEP> s o l a m e n t e <SEP> a <SEP> f a <SEP> ' n <SEP> m a n i e r a <EOV>\",\n",
       " '<GO> D e <SEP> f a m m e <SEP> ’ n <SEP> s o g n o <SEP> a <SEP> p r i m a <SEP> s e r a <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e r c h é <SEP> m e <SEP> s e n t o <SEP> u n <SEP> f r i c c i c o <SEP> n e r <SEP> c o r e <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> s o g n à <EOV>',\n",
       " '<GO> P e r c h é <SEP> n e r <SEP> p e t t o <SEP> m e <SEP> c e <SEP> n a s c h i <SEP> ’ n <SEP> f i o r e <EOV>',\n",
       " '<GO> F i o r e <SEP> d e <SEP> l i l l à <EOV>',\n",
       " \"<GO> C h e <SEP> m ' a r i p o r t i <SEP> v e r s o <SEP> e r <SEP> p r i m o <SEP> a m o r e <EOV>\",\n",
       " '<GO> C h e <SEP> s o s p i r a v a <SEP> l e <SEP> c a n z o n i <SEP> m i e <EOV>',\n",
       " '<GO> E <SEP> m ’ a r i t o n t o n i v a <SEP> d e <SEP> b u c i e <EOV>',\n",
       " '<GO> C a n z o n i <SEP> b e l l e <SEP> e <SEP> a p p a s s i o n a t e <EOV>',\n",
       " '<GO> C h e <SEP> R o m a <SEP> m i a <SEP> m ’ a r i c o r d a t e <EOV>',\n",
       " '<GO> C a n t a t e <SEP> s o l o <SEP> p e ’ <SEP> d i s p e t t o <EOV>',\n",
       " '<GO> M a <SEP> c o ’ <SEP> ’ n a <SEP> s m a n i a <SEP> d e n t r o <SEP> a r <SEP> p e t t o <EOV>',\n",
       " '<GO> I o <SEP> n u n <SEP> v e <SEP> c a n t o <SEP> a <SEP> v o c e <SEP> p i e n a <EOV>',\n",
       " '<GO> M a <SEP> t u t t a <SEP> l ’ a n i m a <SEP> è <SEP> s e r e n a <EOV>',\n",
       " '<GO> E <SEP> q u a n n o <SEP> e r <SEP> c i e l o <SEP> s e <SEP> s c o l o r a <EOV>',\n",
       " '<GO> D e <SEP> m e <SEP> n e s s u n a <SEP> s e <SEP> ’ n n a m o r a <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e r c h é <SEP> m e <SEP> s e n t o <SEP> u n <SEP> f r i c c i c o <SEP> n e r <SEP> c o r e <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> s o g n à <EOV>',\n",
       " '<GO> P e r c h é <SEP> n e r <SEP> p e t t o <SEP> m e <SEP> c e <SEP> n a s c h i <SEP> u n <SEP> f i o r e <EOV>',\n",
       " '<GO> F i o r e <SEP> d e <SEP> l i l l à <EOV>',\n",
       " '<GO> C h e <SEP> m ’ a r i p o r t i <SEP> v e r s o <SEP> e r <SEP> p r i m o <SEP> a m o r e <EOV>',\n",
       " '<GO> C h e <SEP> s o s p i r a v a <SEP> l e <SEP> c a n z o n i <SEP> m i e <EOV>',\n",
       " '<GO> E <SEP> m ’ a r i t o n t o n i v a <SEP> d e <SEP> b u c i e <EOV>',\n",
       " '<EOT> <EOV>']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbitrary_verses = \"\"\"\n",
    "È una canzone senza titolo\n",
    "Tanto pe’ cantà\n",
    "Pe’ fa quarche cosa\n",
    "Non è gnente de straordinario\n",
    "È robba der paese nostro\n",
    "Che se po’ cantà pure senza voce\n",
    "Basta ’a salute\n",
    "Quanno c'è 'a salute c'è tutto\n",
    "Basta ’a salute e un par de scarpe nove\n",
    "Poi girà tutto er monno\n",
    "E m’a accompagno da me\n",
    "Pe’ fa la vita meno amara\n",
    "Me so’ comprato 'sta chitara\n",
    "E quanno er sole scenne e more\n",
    "Me sento ’n core cantatore\n",
    "La voce e’ poca ma ’ntonata\n",
    "Nun serve a fa ’na serenata\n",
    "Ma solamente a fa 'n maniera\n",
    "De famme ’n sogno a prima sera\n",
    "Tanto pe’ cantà\n",
    "Perché me sento un friccico ner core\n",
    "Tanto pe’ sognà\n",
    "Perché ner petto me ce naschi ’n fiore\n",
    "Fiore de lillà\n",
    "Che m'ariporti verso er primo amore\n",
    "Che sospirava le canzoni mie\n",
    "E m’aritontoniva de bucie\n",
    "Canzoni belle e appassionate\n",
    "Che Roma mia m’aricordate\n",
    "Cantate solo pe’ dispetto\n",
    "Ma co’ ’na smania dentro ar petto\n",
    "Io nun ve canto a voce piena\n",
    "Ma tutta l’anima è serena\n",
    "E quanno er cielo se scolora\n",
    "De me nessuna se ’nnamora\n",
    "Tanto pe’ cantà\n",
    "Perché me sento un friccico ner core\n",
    "Tanto pe’ sognà\n",
    "Perché ner petto me ce naschi un fiore\n",
    "Fiore de lillà\n",
    "Che m’ariporti verso er primo amore\n",
    "Che sospirava le canzoni mie\n",
    "E m’aritontoniva de bucie\n",
    "\"\"\"\n",
    "\n",
    "arbitrary_verses = preprocess_text(arbitrary_verses, end_of_tercet='')\n",
    "arbitrary_verses = [verse.strip() + ' <EOV>' for verse in arbitrary_verses.split('<EOV>')][:-1]\n",
    "arbitrary_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_verses = tokenizer.texts_to_sequences(arbitrary_verses)\n",
    "padded_verses = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_input = tf.convert_to_tensor(padded_verses)\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "\n",
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|T u|na |can|zo|ne |sen|za |ti|to|lo',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Pe’ |fa |quar|che |co|sa',\n",
       " '|Non |è |gnen|te |de |stra|or|di|na|rio',\n",
       " '|T |rob|ba |der |pa|e|se |no|stro',\n",
       " '|Che |se |po’ |can|tà |pu|re |sen|za |vo|ce',\n",
       " '|Ba|sta ’a |sa|lu|te',\n",
       " '|Quan|no |cè |a |sa|lu|te |cè |tut|to',\n",
       " '|Ba|sta ’a |sa|lu|te e |un |par |de |scar|pe |no|ve',\n",
       " '|Poi |gi|rà |tut|to er |mon|no',\n",
       " '|E |m’ ac|ca|com|pa|pno |da |me',\n",
       " '|Pe’ |fa |la |vi|ta |me|no a|ma|ra',\n",
       " '|Me |so’ |com|pra|to |sta |chi|ta|ra|ra',\n",
       " '|E |quan|no er |so|le |scen|ne e |e |mo|re',\n",
       " '|Me |sen|to ’n |co|re |can|tan|to|re',\n",
       " '|La |vo|ce e’’ |po|ca |ma ’n|to|na|ta|ta',\n",
       " '|Nun |ser|ve a |fa |fa|na |se|re|na|ta',\n",
       " '|Ma |so|la|men|te a |fa |n |ma|na|ra',\n",
       " '|De |fam|me ’n |so|gno a |pri|ma |se|ra',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Per|ché |me |sen|to un |fric|ci|co |ner |co|re',\n",
       " '|Tan|to |pe’ |so|gnà',\n",
       " '|Per|ché |ner |pet|to |me |ce |na|schi ’n |fio|re',\n",
       " '|Fio|re |de |lil|là',\n",
       " '|Che |ma|ri|por|ti |ver|so er |pri|mo a|mo|re',\n",
       " '|Che |so|spi|ra|va |le |can|zo|ni |mie',\n",
       " '|E |m’ a|ri|ton|to|ni|va |de |bu|cu|can|ton|to',\n",
       " '|Can|zo|ni |bel|le e |ap|pas|sio|na|te',\n",
       " '|Che |Ro|ma |mia |m’ a|ri|cor|da|te',\n",
       " '|Can|ta|te |so|lo |pe’ |di|spet|to',\n",
       " '|Ma |co’ |’na |sma|nia |den|tro ar |per |po',\n",
       " '|Io |nun |ve |can|to a |vo|ce |pie|na',\n",
       " '|Ma |tut|ta |l’ a|ni|ma è |se|re|na',\n",
       " '|E |quan|no er |cie|lo |se |seo|lo|ra',\n",
       " '|De |me |nes|su|na |se ’n|na|mo|ra',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Per|ché |me |sen|to un |fric|ci|co |ner |co|re',\n",
       " '|Tan|to |pe’ |so|gnà',\n",
       " '|Per|ché |ner |pet|to |me |ce |na|schi un |fio|re',\n",
       " '|Fio|re |de |lil|là',\n",
       " '|Che |m’ a|ri|por|ti |ver|so er |pri|mo a|mo|re',\n",
       " '|Che |so|spi|ra|va |le |can|zo|ni |mie',\n",
       " '|E |m’ a|ri|ton|to|ni|va |de |bu|cu|can|ton|to',\n",
       " '|S’ |S’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |so|s’ |s']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character_level_transformer_syllabification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
