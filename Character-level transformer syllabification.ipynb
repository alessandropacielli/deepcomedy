{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import wandb\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from deepcomedy.utils import *\n",
    "from deepcomedy.metrics import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT"
   },
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_tercet=\"\")\n",
    "text = preprocess_text(raw_text, end_of_tercet=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpbut2Vb_fU"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Avfk31uHblz8"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOV>\"\n",
    "input_verses = [x + sep for x in text.split(sep)][:-1]\n",
    "target_verses = [x + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVxwIU4gcGQe"
   },
   "source": [
    "Encode with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hUvF7DRscJTo"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_verses)\n",
    "enc_input_verses = tokenizer.texts_to_sequences(input_verses)\n",
    "enc_target_verses = tokenizer.texts_to_sequences(target_verses)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMv3NIsNcQ-d"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iAn-XqEFcT5h"
   },
   "outputs": [],
   "source": [
    "input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_input_verses, padding=\"post\"\n",
    ")\n",
    "target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_target_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1kX2bojP72Vq"
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_text, target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = make_dataset(input_train, target_train, batch_size=batch_size)\n",
    "validation_dataset = make_dataset(input_test, target_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZmAO5JHhP9G"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep-test-1\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"value\": 32},\n",
    "        \"epochs\": {\"values\": 10},\n",
    "        \"num_layers\": {\"values\": [4, 8, 12]},\n",
    "        \"num_heads\": {\"values\": [4, 8]},\n",
    "        \"d_model\": {\"values\": [128, 256]},\n",
    "        \"dff\": {\"value\": 1024},\n",
    "        # TODO include architecture + dataset\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"deepcomedy\", entity=\"deepcomedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        dataset = make_dataset(input_train, target_train, config[\"batch_size\"])\n",
    "        model, trainer = make_model(config)\n",
    "        trainer.train(dataset, config[\"epochs\"], log_wandb=True)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaKKUqB726YI",
    "outputId": "4c447512-5078-4c7a-9021-c1f233fd8def"
   },
   "outputs": [],
   "source": [
    "best_config = {\"num_layers\": 4, \"d_model\": 256, \"num_heads\": 4, \"dff\": 1024}\n",
    "\n",
    "transformer, transformer_trainer = make_transformer_model(\n",
    "    best_config, vocab_size, vocab_size, checkpoint_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7690 Accuracy 0.0115\n",
      "Epoch 1 Batch 50 Loss 3.8194 Accuracy 0.1316\n",
      "Epoch 1 Batch 100 Loss 3.4262 Accuracy 0.1715\n",
      "Epoch 1 Batch 150 Loss 3.2363 Accuracy 0.1933\n",
      "Epoch 1 Batch 200 Loss 3.0199 Accuracy 0.2273\n",
      "Epoch 1 Batch 250 Loss 2.8322 Accuracy 0.2575\n",
      "Epoch 1 Batch 300 Loss 2.6857 Accuracy 0.2810\n",
      "Epoch 1 Loss 2.6094 Accuracy 0.2937\n",
      "Time taken for 1 epoch: 29.22 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8332 Accuracy 0.4261\n",
      "Epoch 2 Batch 50 Loss 1.8122 Accuracy 0.4312\n",
      "Epoch 2 Batch 100 Loss 1.7746 Accuracy 0.4397\n",
      "Epoch 2 Batch 150 Loss 1.7385 Accuracy 0.4491\n",
      "Epoch 2 Batch 200 Loss 1.6993 Accuracy 0.4599\n",
      "Epoch 2 Batch 250 Loss 1.6459 Accuracy 0.4759\n",
      "Epoch 2 Batch 300 Loss 1.5578 Accuracy 0.5037\n",
      "Epoch 2 Loss 1.4831 Accuracy 0.5278\n",
      "Time taken for 1 epoch: 16.92 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6504 Accuracy 0.7937\n",
      "Epoch 3 Batch 50 Loss 0.5474 Accuracy 0.8276\n",
      "Epoch 3 Batch 100 Loss 0.4717 Accuracy 0.8513\n",
      "Epoch 3 Batch 150 Loss 0.4157 Accuracy 0.8681\n",
      "Epoch 3 Batch 200 Loss 0.3749 Accuracy 0.8806\n",
      "Epoch 3 Batch 250 Loss 0.3442 Accuracy 0.8901\n",
      "Epoch 3 Batch 300 Loss 0.3188 Accuracy 0.8981\n",
      "Epoch 3 Loss 0.3054 Accuracy 0.9022\n",
      "Time taken for 1 epoch: 16.90 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1542 Accuracy 0.9443\n",
      "Epoch 4 Batch 50 Loss 0.1582 Accuracy 0.9488\n",
      "Epoch 4 Batch 100 Loss 0.1528 Accuracy 0.9504\n",
      "Epoch 4 Batch 150 Loss 0.1474 Accuracy 0.9521\n",
      "Epoch 4 Batch 200 Loss 0.1408 Accuracy 0.9542\n",
      "Epoch 4 Batch 250 Loss 0.1377 Accuracy 0.9552\n",
      "Epoch 4 Batch 300 Loss 0.1331 Accuracy 0.9568\n",
      "Epoch 4 Loss 0.1296 Accuracy 0.9580\n",
      "Time taken for 1 epoch: 16.94 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0946 Accuracy 0.9680\n",
      "Epoch 5 Batch 50 Loss 0.1006 Accuracy 0.9674\n",
      "Epoch 5 Batch 100 Loss 0.0975 Accuracy 0.9687\n",
      "Epoch 5 Batch 150 Loss 0.0922 Accuracy 0.9706\n",
      "Epoch 5 Batch 200 Loss 0.0876 Accuracy 0.9723\n",
      "Epoch 5 Batch 250 Loss 0.0864 Accuracy 0.9727\n",
      "Epoch 5 Batch 300 Loss 0.0837 Accuracy 0.9737\n",
      "Epoch 5 Loss 0.0816 Accuracy 0.9743\n",
      "Time taken for 1 epoch: 16.89 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0606 Accuracy 0.9801\n",
      "Epoch 6 Batch 50 Loss 0.0623 Accuracy 0.9806\n",
      "Epoch 6 Batch 100 Loss 0.0607 Accuracy 0.9816\n",
      "Epoch 6 Batch 150 Loss 0.0595 Accuracy 0.9819\n",
      "Epoch 6 Batch 200 Loss 0.0557 Accuracy 0.9832\n",
      "Epoch 6 Batch 250 Loss 0.0535 Accuracy 0.9840\n",
      "Epoch 6 Batch 300 Loss 0.0541 Accuracy 0.9840\n",
      "Epoch 6 Loss 0.0531 Accuracy 0.9843\n",
      "Time taken for 1 epoch: 16.90 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0497 Accuracy 0.9860\n",
      "Epoch 7 Batch 50 Loss 0.0356 Accuracy 0.9897\n",
      "Epoch 7 Batch 100 Loss 0.0376 Accuracy 0.9892\n",
      "Epoch 7 Batch 150 Loss 0.0383 Accuracy 0.9892\n",
      "Epoch 7 Batch 200 Loss 0.0407 Accuracy 0.9886\n",
      "Epoch 7 Batch 250 Loss 0.0408 Accuracy 0.9886\n",
      "Epoch 7 Batch 300 Loss 0.0403 Accuracy 0.9888\n",
      "Epoch 7 Loss 0.0407 Accuracy 0.9888\n",
      "Time taken for 1 epoch: 16.90 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0494 Accuracy 0.9871\n",
      "Epoch 8 Batch 50 Loss 0.0354 Accuracy 0.9908\n",
      "Epoch 8 Batch 100 Loss 0.0403 Accuracy 0.9895\n",
      "Epoch 8 Batch 150 Loss 0.0396 Accuracy 0.9897\n",
      "Epoch 8 Batch 200 Loss 0.0409 Accuracy 0.9894\n",
      "Epoch 8 Batch 250 Loss 0.0422 Accuracy 0.9891\n",
      "Epoch 8 Batch 300 Loss 0.0420 Accuracy 0.9892\n",
      "Epoch 8 Loss 0.0416 Accuracy 0.9893\n",
      "Time taken for 1 epoch: 16.91 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0294 Accuracy 0.9914\n",
      "Epoch 9 Batch 50 Loss 0.0395 Accuracy 0.9895\n",
      "Epoch 9 Batch 100 Loss 0.0409 Accuracy 0.9890\n",
      "Epoch 9 Batch 150 Loss 0.0399 Accuracy 0.9894\n",
      "Epoch 9 Batch 200 Loss 0.0422 Accuracy 0.9890\n",
      "Epoch 9 Batch 250 Loss 0.0411 Accuracy 0.9892\n",
      "Epoch 9 Batch 300 Loss 0.0406 Accuracy 0.9894\n",
      "Epoch 9 Loss 0.0401 Accuracy 0.9896\n",
      "Time taken for 1 epoch: 16.86 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0499 Accuracy 0.9869\n",
      "Epoch 10 Batch 50 Loss 0.0434 Accuracy 0.9893\n",
      "Epoch 10 Batch 100 Loss 0.0465 Accuracy 0.9883\n",
      "Epoch 10 Batch 150 Loss 0.0455 Accuracy 0.9885\n",
      "Epoch 10 Batch 200 Loss 0.0433 Accuracy 0.9890\n",
      "Epoch 10 Batch 250 Loss 0.0431 Accuracy 0.9891\n",
      "Epoch 10 Batch 300 Loss 0.0428 Accuracy 0.9891\n",
      "Epoch 10 Loss 0.0427 Accuracy 0.9891\n",
      "Time taken for 1 epoch: 16.93 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = tokenizer.word_index['<GO>']\n",
    "stop_symbol = tokenizer.word_index['<EOV>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.convert_to_tensor(input_test[:100])\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|lu|cen|te |più |as|sai |di |quel |ch’ el|l’ |e|ra.',\n",
       " '|che |si |sta|va|no a |l’ om|bra |die|tro al |sas|so',\n",
       " '|Poi, |ral|lar|ga|ti |per |la |stra|da |so|la,',\n",
       " '|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|sco,',\n",
       " '|e |co|me |quel |ch’ è |pa|sto |la |ri|mi|ra;',\n",
       " '|con |le |quai |la |tua |E|ta|ca |per|trat|ta',\n",
       " '|ma |noi |siam |pe|re|grin |co|me |voi |sie|te.',\n",
       " '|La |lin|gua |ch’ io |par|lai |fu |tut|ta |spen|ta',\n",
       " '|che |guar|da ’l |pon|te, |che |Fio|ren|za |fes|se',\n",
       " '« |Io |sa|rò |pri|mo, e |tu |sa|rai |se|con|do».',\n",
       " '|por|re un |uom |per |lo |po|po|lo a’ |mar|tì|ri.',\n",
       " '|pri|ma |che |pos|sa |tut|ta in |sé |mu|tar|si;',\n",
       " '|con|tra ’l |di|sio, |fo |ben |ch’ io |non |di|man|do”.',\n",
       " '|se |non |co|me |tri|sti|zia o |se|te o |fa|me:',\n",
       " '|vie |più |lu|cen|do, |co|mi|cia|ron |can|ti',\n",
       " '|E |se |più |fu |lo |suo |par|lar |dif|fu|so,',\n",
       " '|a |Ce|pe|ran, |là |do|ve |fu |bu|giar|do',\n",
       " '|al |mio |di|sio |cer|ti|fi|ca|to |fer|mi.',\n",
       " '|non |fos|se |sta|ta a |Ce|sa|re |no|ver|ca,',\n",
       " '|pro|du|ce, e |cen|cri |con |an|fi|si|be|na,',\n",
       " '|che |mi |sco|lo|ra», |pre|ga|va,«« |la |pel|le,',\n",
       " '|Noi |e|ra|vam |par|ti|ti |già |da |el|lo,',\n",
       " '|rac|co|man|dò |la |don|na |sua |più |ca|ra,',\n",
       " '|u|di|to |que|sto, |quan|do al|cu|na |pian|ta',\n",
       " '|e |giù |dal |col|lo |de |la |ri|pa |du|ra',\n",
       " '|Ahi |quan|to |mi |pa|rea |pien |di |di|sddde|gno!',\n",
       " '|Ne |li oc|chi e|ra |cia|scu|na o|scu|ra e |ca|va,',\n",
       " '|que|sta |gran |tem|po |per |lo |mon|do |gio.',\n",
       " '|nel |mon|tar |sù, |co|sì |sa|rà |nel |ca|lo.',\n",
       " '|se|gue, |co|me ’l |mae|stro |fa ’l |di|scen|sce;',\n",
       " '“ |An|da|te, e |pre|di|ca|te al |mon|do |cian|ce”;',\n",
       " '|be|stia |mal|va|gia |che |co|là |si |cor|ca».',\n",
       " '|io |co|min|ciai:« |El |par |che |tu |mi |nie|ghi,',\n",
       " '|o |de |la |pro|pria o |de |l’ al|trui |ver|go|gna',\n",
       " '|Que|sto |pas|sam|mo |co|me |ter|ra |du|ra;',\n",
       " '|di |qua, |di |là |soc|cor|rien |con |le |ma|ni',\n",
       " '|e |l’ i|dï|o|ma |ch’ u|sai |che |fei.',\n",
       " '|gri|dò:« |Qual |io |fui |vi|vo, |tal |son |mor|to.',\n",
       " '|e |quel |di|la|ce|ro a |a |bra|no a |bra|no;',\n",
       " '|Cre|d’ ï|o |ch’ ei |cre|det|te |ch’ io |cre|des|se',\n",
       " '|Io |fui |di |Mon|te|fel|tro, io |son |Bon|con|te;',\n",
       " '|E |io |a |lui:« |Con |pian|ge|re e |con |lut|to,',\n",
       " '|ché |di |giu|sto |vo|ler |lo |suo |si |fa|ce:',\n",
       " '|Me|noc|ci o|ve |la |roc|cia e|ra |ta|glia|ta;',\n",
       " '|che |fu |som|mo |can|tor |del |som|mo |du|ce.',\n",
       " '|ne |la |mi|se|ria |do|ve |tu |mi |ve|di,',\n",
       " '|e |vi|de|mi e |co|nob|be|mi e |chia|ma|va,',\n",
       " '|e |driz|zò |li oc|chi al |ciel,« |che |ti |fia |chia|ro',\n",
       " '|de |l’ am|pio |lo|co o|ve |to|nar |tu |ar|di”.',\n",
       " '|es|ser |po|rà |ch’ al |ve|der |non |vi |nòi”.',\n",
       " '|che al |giu|di|cio |di|vin |pas|sion |com|por|ta?',\n",
       " '|che |di |sù|bi|to |chie|de o|ve |s’ ar|re|sta,',\n",
       " '|non |so|nò |sì |ter|ri|bil|ten|te Or|lan|do.',\n",
       " '|poi, |di|ven|tan|do |l’ un |di |que|sti |se|gni,',\n",
       " '|E |non |re|stò |di |rui|na|re a |val|le',\n",
       " '|e |al|tra è |quel|la |c’ ha |l’ a|ni|man|te|ra:',\n",
       " '|E |io |a |lui:« |Chi |son |li |due |ta|pi|ni',\n",
       " '|che, |per |ve|der, |non |in|du|gia ’l |par|ti|re:',\n",
       " '|per |frat|ta |nu|be, |già |pra|to |di |fio|ri',\n",
       " '|cin|que|cen|t’ an|ni e |più, |pur |mo |sen|ti',\n",
       " '« |Voi |vi|gi|la|te |ne |l’ et|ter|no |die,',\n",
       " '|de |l’ o|ro, |l’ ap|pe|to |de’ |mor|ta|li?”,',\n",
       " '|e |cu’ |io |vi|di |su |in |te|ra |la|ti|na,',\n",
       " '|ben |co|nob|bi il |ve|len |de |l’ ar|go|men|to.',\n",
       " '|a|ni|me |for|tu|na|te |tut|tu |quan|te,',\n",
       " '|per |do|man|dar |la |mia |do|na |di |co|se',\n",
       " '|co|me ’l |se|gno |del |mon|do e |de’ |suoi |du|ci',\n",
       " '|e |per |co|lei |che ’l |lo|co |pri|ma e|les|se,',\n",
       " '|Ma |ta|le uc|cel |nel |bec|chet|to |s’ an|ni|da,',\n",
       " '|cer|to |non |ti |do|vrien |pun|ger |li |stra|li',\n",
       " '|già |di |be|re a |For|lì |con |men |sec|chez|za,',\n",
       " '|ché |poi |non |si |po|ria, |se ’l |dì |non |rie|de».',\n",
       " '|pur |di |non |per|der |tem|po, |sì |che ’n |qul|la',\n",
       " '|cia|scun |da |l’ al|tra |co|sta |li oc|chi |vol|se,',\n",
       " '|co|me |suol |se|gu|tar |per |al|cun |ca|so,',\n",
       " '|De |la |pro|fon|da |con|di|zion |di|vi|na',\n",
       " '|ri|spon|der |lei |con |vi|so |tem|pe|ra|to:',\n",
       " '|E |ag|gi a |men|te, |quan|do |tu |le |scri|vi,',\n",
       " '|con|ti|nü|ò |co|sì ’l |pro|ces|so |san|to:',\n",
       " '|a |quel |par|lar |che |mi |pa|rea |ne|mi|co.',\n",
       " '|che |pur |con |ci|bi |di |li|quor |d’ u|li|vi',\n",
       " '« |e |che |fai |d’ es|se |tal|tol|ta |ta|na|glie,',\n",
       " '|ma |die|de |lor |ve|ra|ce |fon|da|men|to;',\n",
       " '|se|gui|tar |lei |per |tut|to |l’ in|no in|te|ro,',\n",
       " '|se |quel|la |con |ch’ io |pa|lo |non |si |sec|ca».',\n",
       " '|Que|sta |na|tu|ra |sì |ol|tre |s’ in|gra|da',\n",
       " '|che |si |mu|rò |di |se|gni e |di |mar|tì|ri.',\n",
       " '|lo |ciel |ve|nir |più |e |più |ri|schia|ran|do;',\n",
       " '|sì |che |buon |frut|to |ra|do |se |ne |schian|ta.',\n",
       " '|Pri|ma |vuol |ben, |ma |non |la|scia |il |ta|len|to',\n",
       " '|qua |giù |di|mo|ra e |qua |sù |non |a|scen|de,',\n",
       " '|se|der |là |so|lo, Ar|ri|go |d’ In|ghil|ter|ra:',\n",
       " '|ben |puoi |ve|der |per|ch’ io |co|sì |ra|gio|no.',\n",
       " '|O |som|ma |sa|pï|en|za, |quan|ta è |l’ ar|te',\n",
       " '|ch’ a|mor |con|sun|se |co|me |sol |va|po|ri,',\n",
       " '|se |la |co|sa |di|mes|sa in |la |sor|pre|sa',\n",
       " '|tal |che |mi |vin|se e |guar|dar |nol |po|tei.',\n",
       " '|Lo |du|ca |stet|te un |po|co a |te|sta |chi|na;',\n",
       " '|to|glie|va |li a|ni|mai |che |so|no in |ter|ra',\n",
       " '|Que|ste |pa|ro|le |fuor |del |du|ca |mio;']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_syll = target_test[:100]\n",
    "correct_syll = ' '.join(tokenizer.sequences_to_texts(correct_syll))\n",
    "correct_syll = strip_tokens(correct_syll)\n",
    "correct_syll = correct_syll.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches, similarities = zip(*validate_syllabification(stripped_output, correct_syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(exact_matches) / len(exact_matches)\n",
    "avg_similarities = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabification exact matches: 76.00%\n"
     ]
    }
   ],
   "source": [
    "print('Syllabification exact matches: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity: {:.2f}'.format(avg_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = np.array(stripped_output)\n",
    "correct_syll = np.array(correct_syll)\n",
    "error_mask = ~np.array(exact_matches)\n",
    "\n",
    "errors_output = stripped_output[error_mask]\n",
    "errors_correct = correct_syll[error_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|to,'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_correct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|sco,'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_output[1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character_level_transformer_syllabification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
