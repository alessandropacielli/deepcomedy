{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import load_verses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT"
   },
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "input_file = \"data/divina_textonly.txt\"\n",
    "target_file = \"data/divina_syll_textonly.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0b642052c982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_input_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_verses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mraw_target_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_verses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL/deepcomedy/deepcomedy/preprocessing.py\u001b[0m in \u001b[0;36mload_verses\u001b[0;34m(path, char_level, pad)\u001b[0m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded_text' is not defined"
     ]
    }
   ],
   "source": [
    "raw_input_text, input_text, input_tokenizer = load_verses(input_file, char_level=True, pad=True)\n",
    "raw_target_text, target_text, target_tokenizer = load_verses(target_file, char_level=True, pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "ACAEUyITY1dY",
    "outputId": "26daf544-3700-428a-c752-b10e0bbd6d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text: 558637 characters\n",
      "Length of target text: 873431 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input text: {} characters\".format(len(raw_input_text)))\n",
    "print(\"Length of target text: {} characters\".format(len(raw_target_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "gradient": {},
    "id": "l1G45yR9Y1da"
   },
   "outputs": [],
   "source": [
    "input_vocab = sorted(set(input_tokenizer.word_index.keys()))\n",
    "target_vocab = sorted(set(target_tokenizer.word_index.keys()))\n",
    "input_vocab_size = len(input_vocab)\n",
    "target_vocab_size = len(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "p-w27LhpY1db",
    "outputId": "d4061a9c-09a0-46c1-b0a0-501d897ab0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 79\n",
      "Target vocab size: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
    "print(\"Target vocab size: {}\".format(target_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IN8x175vimK"
   },
   "source": [
    "The dataset is created by grouping the lines in batches and by shuffling them.\n",
    "\n",
    "Each input's line is in correspondence with its target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {},
    "id": "tZWLq7g3Y1dl"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "num_layers = 6\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "vocab_size = (\n",
    "    len(tokenizer.word_index) + 1\n",
    ")  # the +1 is added to take into account the padding token\n",
    "\n",
    "max_length_targ, max_length_inp = target_text.shape[1], input_text.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
    "    BUFFER_SIZE\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    target_vocab_size=vocab_size,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_trainer = TransformerTrainer(transformer, checkpoint_save_path='./checkpoints/char-level-syll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_trainer.train(dataset, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITVGgP8Su9MH"
   },
   "source": [
    "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz4YwsF04YEI"
   },
   "source": [
    "## 4. Syllabification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O44l1saVuebS"
   },
   "source": [
    "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
    "\n",
    "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
    "\n",
    "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
    "\n",
    "The translation stops when the end symbol is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=200):\n",
    "\n",
    "    encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "\n",
    "    output = tf.convert_to_tensor([tokenizer.word_index[\"^\"]])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    result = \"\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
    "            axis=-1,\n",
    "        )\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
    "            break\n",
    "\n",
    "    # output.shape (1, tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gradient": {},
    "id": "PkCDxGdeyF9f"
   },
   "outputs": [],
   "source": [
    "def print_translation(sentence, result, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {result}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5cULw_54F8w",
    "outputId": "c5591588-ffd5-43cb-9627-7398a9a6813c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : ^E come l’aere, quand’ è ben pïorno,$\n",
      "Prediction     : | E   | c o | m e   | l ’   a e | r e ,   | q u a n | d ’   è   | b e n   | p ï | o r | n o , $ \n",
      "Ground truth   : |E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\n"
     ]
    }
   ],
   "source": [
    "sentence = \"^E come l’aere, quand’ è ben pïorno,$\"\n",
    "ground_truth = \"|E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\"\n",
    "\n",
    "\n",
    "translated_text, attention_weights = evaluate(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Comedy transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
