{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syllabification experiments using Transformers\n",
    "\n",
    "In this notebook we show our first experiments using the Transformer architecture to build a syllabifier. The training and the hyperparameters are not optimal (just 10 epochs, no hyperparameter sweeps performed), however we got pretty good results and this inspired us to keep working on this architecture. Other experiments can be found in the `Char2Char` and `Word2Char` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import wandb\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from deepcomedy.utils import *\n",
    "from deepcomedy.metrics import *\n",
    "\n",
    "import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT"
   },
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_tercet=\"\")\n",
    "text = preprocess_text(raw_text, end_of_tercet=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGpbut2Vb_fU"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Avfk31uHblz8"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOV>\"\n",
    "input_verses = [x + sep for x in text.split(sep)][:-1]\n",
    "target_verses = [x + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVxwIU4gcGQe"
   },
   "source": [
    "Encode with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hUvF7DRscJTo"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_verses)\n",
    "enc_input_verses = tokenizer.texts_to_sequences(input_verses)\n",
    "enc_target_verses = tokenizer.texts_to_sequences(target_verses)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMv3NIsNcQ-d"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iAn-XqEFcT5h"
   },
   "outputs": [],
   "source": [
    "input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_input_verses, padding=\"post\"\n",
    ")\n",
    "target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_target_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1kX2bojP72Vq"
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_text, target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = make_dataset(input_train, target_train, batch_size=batch_size)\n",
    "validation_dataset = make_dataset(input_test, target_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaKKUqB726YI",
    "outputId": "4c447512-5078-4c7a-9021-c1f233fd8def"
   },
   "outputs": [],
   "source": [
    "best_config = {\"num_layers\": 4, \"d_model\": 256, \"num_heads\": 4, \"dff\": 1024}\n",
    "\n",
    "transformer, transformer_trainer = make_transformer_model(\n",
    "    best_config, vocab_size, vocab_size, checkpoint_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.1930 Accuracy 0.0071\n",
      "Epoch 1 Batch 50 Loss 4.0188 Accuracy 0.1295\n",
      "Epoch 1 Batch 100 Loss 3.5211 Accuracy 0.1728\n",
      "Epoch 1 Batch 150 Loss 3.2827 Accuracy 0.1966\n",
      "Epoch 1 Batch 200 Loss 3.0484 Accuracy 0.2309\n",
      "Epoch 1 Batch 250 Loss 2.8551 Accuracy 0.2611\n",
      "Epoch 1 Batch 300 Loss 2.7064 Accuracy 0.2849\n",
      "Epoch 1 Batch 0 Validation Loss 1.7240 Validation Accuracy 0.4466\n",
      "Epoch 1 Batch 50 Validation Loss 1.7252 Validation Accuracy 0.4591\n",
      "Epoch 1 Batch 100 Validation Loss 1.7278 Validation Accuracy 0.4567\n",
      "Epoch 1 Loss 2.6256 Accuracy 0.2976\n",
      "Time taken for 1 epoch: 42.77 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8372 Accuracy 0.4202\n",
      "Epoch 2 Batch 50 Loss 1.8110 Accuracy 0.4314\n",
      "Epoch 2 Batch 100 Loss 1.7727 Accuracy 0.4409\n",
      "Epoch 2 Batch 150 Loss 1.7371 Accuracy 0.4488\n",
      "Epoch 2 Batch 200 Loss 1.7048 Accuracy 0.4569\n",
      "Epoch 2 Batch 250 Loss 1.6710 Accuracy 0.4657\n",
      "Epoch 2 Batch 300 Loss 1.6272 Accuracy 0.4786\n",
      "Epoch 2 Batch 0 Validation Loss 0.8765 Validation Accuracy 0.7110\n",
      "Epoch 2 Batch 50 Validation Loss 0.8467 Validation Accuracy 0.7237\n",
      "Epoch 2 Batch 100 Validation Loss 0.8476 Validation Accuracy 0.7241\n",
      "Epoch 2 Loss 1.5866 Accuracy 0.4915\n",
      "Time taken for 1 epoch: 31.61 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0370 Accuracy 0.6609\n",
      "Epoch 3 Batch 50 Loss 0.8616 Accuracy 0.7253\n",
      "Epoch 3 Batch 100 Loss 0.7047 Accuracy 0.7752\n",
      "Epoch 3 Batch 150 Loss 0.5979 Accuracy 0.8092\n",
      "Epoch 3 Batch 200 Loss 0.5223 Accuracy 0.8330\n",
      "Epoch 3 Batch 250 Loss 0.4669 Accuracy 0.8506\n",
      "Epoch 3 Batch 300 Loss 0.4231 Accuracy 0.8645\n",
      "Epoch 3 Batch 0 Validation Loss 0.0965 Validation Accuracy 0.9670\n",
      "Epoch 3 Batch 50 Validation Loss 0.0773 Validation Accuracy 0.9734\n",
      "Epoch 3 Batch 100 Validation Loss 0.0776 Validation Accuracy 0.9733\n",
      "Epoch 3 Loss 0.3984 Accuracy 0.8722\n",
      "Time taken for 1 epoch: 31.64 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1788 Accuracy 0.9477\n",
      "Epoch 4 Batch 50 Loss 0.1670 Accuracy 0.9454\n",
      "Epoch 4 Batch 100 Loss 0.1542 Accuracy 0.9495\n",
      "Epoch 4 Batch 150 Loss 0.1476 Accuracy 0.9520\n",
      "Epoch 4 Batch 200 Loss 0.1403 Accuracy 0.9544\n",
      "Epoch 4 Batch 250 Loss 0.1395 Accuracy 0.9549\n",
      "Epoch 4 Batch 300 Loss 0.1320 Accuracy 0.9575\n",
      "Epoch 4 Batch 0 Validation Loss 0.0333 Validation Accuracy 0.9857\n",
      "Epoch 4 Batch 50 Validation Loss 0.0447 Validation Accuracy 0.9858\n",
      "Epoch 4 Batch 100 Validation Loss 0.0446 Validation Accuracy 0.9859\n",
      "Epoch 4 Loss 0.1281 Accuracy 0.9588\n",
      "Time taken for 1 epoch: 31.76 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1127 Accuracy 0.9660\n",
      "Epoch 5 Batch 50 Loss 0.0925 Accuracy 0.9716\n",
      "Epoch 5 Batch 100 Loss 0.0895 Accuracy 0.9721\n",
      "Epoch 5 Batch 150 Loss 0.0908 Accuracy 0.9717\n",
      "Epoch 5 Batch 200 Loss 0.0859 Accuracy 0.9732\n",
      "Epoch 5 Batch 250 Loss 0.0825 Accuracy 0.9744\n",
      "Epoch 5 Batch 300 Loss 0.0786 Accuracy 0.9756\n",
      "Epoch 5 Batch 0 Validation Loss 0.0353 Validation Accuracy 0.9907\n",
      "Epoch 5 Batch 50 Validation Loss 0.0281 Validation Accuracy 0.9914\n",
      "Epoch 5 Batch 100 Validation Loss 0.0291 Validation Accuracy 0.9913\n",
      "Epoch 5 Loss 0.0772 Accuracy 0.9761\n",
      "Time taken for 1 epoch: 31.21 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0334 Accuracy 0.9903\n",
      "Epoch 6 Batch 50 Loss 0.0591 Accuracy 0.9826\n",
      "Epoch 6 Batch 100 Loss 0.0546 Accuracy 0.9840\n",
      "Epoch 6 Batch 150 Loss 0.0542 Accuracy 0.9843\n",
      "Epoch 6 Batch 200 Loss 0.0521 Accuracy 0.9849\n",
      "Epoch 6 Batch 250 Loss 0.0507 Accuracy 0.9854\n",
      "Epoch 6 Batch 300 Loss 0.0494 Accuracy 0.9860\n",
      "Epoch 6 Batch 0 Validation Loss 0.0256 Validation Accuracy 0.9914\n",
      "Epoch 6 Batch 50 Validation Loss 0.0279 Validation Accuracy 0.9934\n",
      "Epoch 6 Batch 100 Validation Loss 0.0258 Validation Accuracy 0.9939\n",
      "Epoch 6 Loss 0.0500 Accuracy 0.9859\n",
      "Time taken for 1 epoch: 31.61 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0473 Accuracy 0.9873\n",
      "Epoch 7 Batch 50 Loss 0.0486 Accuracy 0.9872\n",
      "Epoch 7 Batch 100 Loss 0.0461 Accuracy 0.9878\n",
      "Epoch 7 Batch 150 Loss 0.0457 Accuracy 0.9880\n",
      "Epoch 7 Batch 200 Loss 0.0445 Accuracy 0.9883\n",
      "Epoch 7 Batch 250 Loss 0.0442 Accuracy 0.9884\n",
      "Epoch 7 Batch 300 Loss 0.0444 Accuracy 0.9883\n",
      "Epoch 7 Batch 0 Validation Loss 0.0166 Validation Accuracy 0.9949\n",
      "Epoch 7 Batch 50 Validation Loss 0.0176 Validation Accuracy 0.9959\n",
      "Epoch 7 Batch 100 Validation Loss 0.0191 Validation Accuracy 0.9956\n",
      "Epoch 7 Loss 0.0448 Accuracy 0.9883\n",
      "Time taken for 1 epoch: 31.38 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0340 Accuracy 0.9858\n",
      "Epoch 8 Batch 50 Loss 0.0404 Accuracy 0.9893\n",
      "Epoch 8 Batch 100 Loss 0.0416 Accuracy 0.9894\n",
      "Epoch 8 Batch 150 Loss 0.0416 Accuracy 0.9892\n",
      "Epoch 8 Batch 200 Loss 0.0408 Accuracy 0.9894\n",
      "Epoch 8 Batch 250 Loss 0.0438 Accuracy 0.9888\n",
      "Epoch 8 Batch 300 Loss 0.0431 Accuracy 0.9890\n",
      "Epoch 8 Batch 0 Validation Loss 0.0203 Validation Accuracy 0.9929\n",
      "Epoch 8 Batch 50 Validation Loss 0.0332 Validation Accuracy 0.9921\n",
      "Epoch 8 Batch 100 Validation Loss 0.0317 Validation Accuracy 0.9922\n",
      "Epoch 8 Loss 0.0427 Accuracy 0.9891\n",
      "Time taken for 1 epoch: 31.41 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0530 Accuracy 0.9864\n",
      "Epoch 9 Batch 50 Loss 0.0383 Accuracy 0.9903\n",
      "Epoch 9 Batch 100 Loss 0.0399 Accuracy 0.9899\n",
      "Epoch 9 Batch 150 Loss 0.0416 Accuracy 0.9895\n",
      "Epoch 9 Batch 200 Loss 0.0422 Accuracy 0.9894\n",
      "Epoch 9 Batch 250 Loss 0.0417 Accuracy 0.9896\n",
      "Epoch 9 Batch 300 Loss 0.0429 Accuracy 0.9893\n",
      "Epoch 9 Batch 0 Validation Loss 0.0194 Validation Accuracy 0.9935\n",
      "Epoch 9 Batch 50 Validation Loss 0.0262 Validation Accuracy 0.9939\n",
      "Epoch 9 Batch 100 Validation Loss 0.0276 Validation Accuracy 0.9938\n",
      "Epoch 9 Loss 0.0426 Accuracy 0.9894\n",
      "Time taken for 1 epoch: 31.35 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0293 Accuracy 0.9916\n",
      "Epoch 10 Batch 50 Loss 0.0436 Accuracy 0.9891\n",
      "Epoch 10 Batch 100 Loss 0.0455 Accuracy 0.9885\n",
      "Epoch 10 Batch 150 Loss 0.0439 Accuracy 0.9890\n",
      "Epoch 10 Batch 200 Loss 0.0438 Accuracy 0.9890\n",
      "Epoch 10 Batch 250 Loss 0.0452 Accuracy 0.9887\n",
      "Epoch 10 Batch 300 Loss 0.0458 Accuracy 0.9887\n",
      "Epoch 10 Batch 0 Validation Loss 0.0065 Validation Accuracy 0.9981\n",
      "Epoch 10 Batch 50 Validation Loss 0.0183 Validation Accuracy 0.9962\n",
      "Epoch 10 Batch 100 Validation Loss 0.0176 Validation Accuracy 0.9962\n",
      "Epoch 10 Loss 0.0446 Accuracy 0.9889\n",
      "Time taken for 1 epoch: 31.72 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 10, validation_dataset=validation_dataset, validation_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Syllabification\n",
    "\n",
    "### 3.1. Syllabification example \n",
    "Here we try to syllabify the first 100 verses of the test set. We use the evaluate function to pass the input to the model and autoregressively generate the output from the transformer in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = tokenizer.word_index['<GO>']\n",
    "stop_symbol = tokenizer.word_index['<EOV>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.convert_to_tensor(input_test[:100])\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|lu|cen|te |più |as|sai |di |quel |ch’ el|l’ e|ra.',\n",
       " '|che |si |sta|va|no a |l’ om|bra |die|tro al |sas|so',\n",
       " '|Poi, |ral|lar|ga|ti |per |la |stra|da |so|la,',\n",
       " '|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|to,',\n",
       " '|e |co|me |quel |ch’ è |pa|sto |la |ri|mi|ra;',\n",
       " '|con |le |quai |la |tua |E|ti|ca |per|trat|ta',\n",
       " '|ma |noi |siam |pe|re|grin |co|me |voi |sie|te.',\n",
       " '|La |lin|gua |ch’ io |par|lai |fu |tut|ta |spen|ta',\n",
       " '|che |guar|da ’l |pon|te, |che |Fio|ren|za |fes|se',\n",
       " '« |Io |sa|rò |pri|mo, e |tu |sa|rai |se|con|do».',\n",
       " '|por|re un |uom |per |lo |po|po|lo a’ |mar|tì|ri.',\n",
       " '|pri|ma |che |pos|sa |tut|ta in |sé |mu|tar|si;',\n",
       " '|con|tra ’l |di|sio, |fo |ben |ch’ io |non |dio|man|do”.',\n",
       " '|se |non |co|me |tri|sti|zia o |se|te o |fa|me:',\n",
       " '|vie |più |lu|cen|do, |co|min|cia|ron |can|ti',\n",
       " '|E |se |più |fu |lo |suo |par|lar |dif|fu|so,',\n",
       " '|a |Ce|pe|ran, |là |do|ve |fu |bu|giar|do',\n",
       " '|al |mio |di|sio |cer|ti|fi|ca|to |fer|mi.',\n",
       " '|non |fos|se |sta|ta a |Ce|sa|re |no|ver|ca,',\n",
       " '|pro|du|ce, e |cen|cri |con |an|fi|si|be|na,',\n",
       " '|che |mi |sco|lo|ra», |pre|ga|va,« |la |pel|le,',\n",
       " '|Noi |e|ra|vam |par|ti|ti |già |da |el|lo,',\n",
       " '|rac|co|man|dò |la |don|na |sua |più |ca|ra,',\n",
       " '|u|di|to |que|sto, |quan|do al|cu|na |pian|ta',\n",
       " '|e |giù |dal |col|lo |de |la |ri|pa |du|ra',\n",
       " '|Ahi |quan|to |mi |pa|rea |pien |di |di|sde|gno!',\n",
       " '|Ne |li oc|chi e|ra |cia|scu|na o|scu|ra e |ca|va,',\n",
       " '|que|sta |gran |tem|po |per |lo |mon|do |gio.',\n",
       " '|nel |mon|tar |sù, |co|sì |sa|rà |nel |ca|lo.',\n",
       " '|se|gue, |co|me ’l |ma|e|stro |fa ’l |di|scen|te;',\n",
       " '“ An|da|te, e |pre|di|ca|te al |mon|do |cian|ce”;',\n",
       " '|be|stia |mal|va|gia |che |co|là |si |cor|ca».',\n",
       " '|io |co|min|ciai:« El |par |che |tu |mi |nie|ghi,',\n",
       " '|o |de |la |pro|pria o |de |l’ al|trui |ver|go|gna',\n",
       " '|Que|sto |pas|sam|mo |co|me |ter|ra |du|ra;',\n",
       " '|di |qua, |di |là |soc|cor|rien |con |le |ma|ni',\n",
       " '|e |l’ i|dï|o|o |ch’ u|sai |e |che |fei.',\n",
       " '|gri|dò:« |Qual |io |fui |vi|vo, |tal |son |mor|to.',\n",
       " '|e |quel |di|la|ce|ra|ro a |bra|no a |bra|no;',\n",
       " '|Cre|d’ ï|oo |ch’ ei |cre|det|te |ch’ io |cre|des|se',\n",
       " '|Io |fui |di |Mon|tel|te|ro, io |son |Bon|con|te;',\n",
       " '|E |io |a |lui:« |Con |pian|ge|re e |con |lut|to,',\n",
       " '|ché |di |giu|sto |vo|ler |lo |suo |si |fa|ce:',\n",
       " '|Me|noc|ci o|ve |la |roc|cia e|ra |ta|glia|ta;',\n",
       " '|che |fu |som|mo |can|tor |del |som|mo |du|ce.',\n",
       " '|ne |la |mi|se|ria |do|ve |tu |mi |ve|di,',\n",
       " '|e |vi|de|mi e |co|nob|be|mi e |chia|ma|va,',\n",
       " '|e |driz|zò |li oc|chi al |ciel,« |che |ti |fia |chia|ro',\n",
       " '|de |l’ am|pio |lo|co o|ve |tor|nar |tu |ar|di”.',\n",
       " '|es|ser |po|rà |ch’ al |ve|der |non |vi |nòi».',\n",
       " '|che al |giu|di|cio |di|vin |pas|sion |com|por|ta?',\n",
       " '|che |di |sù|bi|to |chie|de o|ve |s’ ar|re|sta,',\n",
       " '|non |so|nò |sì |ter|ri|bil|men|te Or|lan|do.',\n",
       " '|poi, |di|ven|tan|do |l’ un |di |que|sti |se|gni,',\n",
       " '|E |non |re|stò |di |rui|na|re a |val|le',\n",
       " '|e al|tra è |quel|la |c’ ha |l’ a|ni|ma in|te|ra:',\n",
       " '|E |io |a |lui:« |Chi |son |li |due |ta|pi|ni',\n",
       " '|che, |per |ve|der, |non |in|du|gia ’l |par|ti|re:',\n",
       " '|per |frat|ta |nu|be, |già |pra|to |di |fio|ri',\n",
       " '|cin|que|cen|t’ an|ni e |più, |pur |mo |sen|tii',\n",
       " '« |Voi |vi|gi|la|te |ne |l’ et|ter|no |die,',\n",
       " '|de |l’ o|ro, |l’ ap|pe|to |do |dor|mor|ta|li?”,',\n",
       " '|e |cu’ |io |vi|di |su |in |ter|ra |la|ti|na,',\n",
       " '|ben |co|nob|bi il |ve|len |de |l’ ar|go|men|to.',\n",
       " '|a|ni|me |for|tu|na|te |tut|te |quan|te,',\n",
       " '|per |do|man|dar |la |mia |don|na |di |co|se',\n",
       " '|co|me ’l |se|gno |del |mon|do e |de’ |suoi |du|ci',\n",
       " '|e |per |co|lei |che ’l |lo|co |pri|ma |e|ses|se,',\n",
       " '|Ma |ta|le uc|cel |nel |bec|chet|to |s’ an|ni|da,',\n",
       " '|cer|to |non |ti |do|vrien |pun|ger |li |stra|li',\n",
       " '|già |di |be|re a |For|lì |con |men |sec|chez|za,',\n",
       " '|ché |poi |non |si |po|ria, |se ’l |dì |non |rie|de».',\n",
       " '|pur |di |non |per|der |tem|po, |sì |che ’n |quel|la',\n",
       " '|cia|scun |da |l’ al|tra |co|sta |li oc|chi |vol|se,',\n",
       " '|co|me |suol |se|gui|tar |per |al|cun |ca|so,',\n",
       " '|De |la |pro|fon|da |con|di|zion |di|vi|na',\n",
       " '|ri|spon|der |lei |con |vi|so |tem|pe|ra|to:',\n",
       " '|E |ag|gi a |men|te, |quan|do |tu |le |scri|vi,',\n",
       " '|con|ti|nü|ò |co|sì ’l |pro|ces|so |san|to:',\n",
       " '|a |quel |par|lar |che |mi |pa|rea |ne|mi|co.',\n",
       " '|che |pur |con |ci|bi |di |li|quor |d’ u|li|vi',\n",
       " '« |e |che |fai |d’ es|se |tal|vol|ta |ta|gla|glie,',\n",
       " '|ma |die|de |lor |ve|ra|ce |fon|da|men|to;',\n",
       " '|se|gui|tar |lei |per |tut|to |l’ in|no in|te|ro,',\n",
       " '|se |quel|la |con |ch’ io |par|lo |non |si |sec|ca».',\n",
       " '|Que|sta |na|tu|ra |sì |ol|tre |s’ in|gra|da',\n",
       " '|che |si |mu|rò |di |se|gni e |di |mar|tì|ri.',\n",
       " '|lo |ciel |ve|nir |più |e |più |ri|schia|ran|do;',\n",
       " '|sì |che |buon |frut|to |ra|do |se |ne |schian|ta.',\n",
       " '|Pri|ma |vuol |ben, |ma |non |la|scia il |ta|len|to',\n",
       " '|qua |giù |di|mo|ra e |qua |sù |non |a|scen|de,',\n",
       " '|se|der |là |so|lo, Ar|ri|go |d’ In|ghil|ter|ra:',\n",
       " '|ben |puoi |ve|der |per|ch’ io |co|sì |ra|gio|no.',\n",
       " '|O |som|ma |sa|pï|en|za, |quan|ta è |l’ ar|te',\n",
       " '|ch’ a|mor |con|sun|se |co|me |sol |va|po|ri,',\n",
       " '|se |la |co|sa |di|mes|sa in |la |sor|pre|sa',\n",
       " '|tal |che |mi |vin|se e |guar|dar |nol |po|tei.',\n",
       " '|Lo |du|ca |stet|te un |po|co a |te|sta |chi|na;',\n",
       " '|to|glie|va |li a|ni|mai |che |so|no in |ter|ra',\n",
       " '|Que|ste |pa|ro|le |fuor |del |du|ca |mio;']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_syll = target_test[:100]\n",
    "correct_syll = ' '.join(tokenizer.sequences_to_texts(correct_syll))\n",
    "correct_syll = strip_tokens(correct_syll)\n",
    "correct_syll = correct_syll.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches, similarities = zip(*validate_syllabification(stripped_output, correct_syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(exact_matches) / len(exact_matches)\n",
    "avg_similarities = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabification exact matches: 76.00%\n"
     ]
    }
   ],
   "source": [
    "print('Syllabification exact matches: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity: {:.2f}'.format(avg_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = np.array(stripped_output)\n",
    "correct_syll = np.array(correct_syll)\n",
    "error_mask = ~np.array(exact_matches)\n",
    "\n",
    "errors_output = stripped_output[error_mask]\n",
    "errors_correct = correct_syll[error_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|to,'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_correct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Po|scia |ch’ io |v’ eb|bi al|cun |ri|co|no|sciu|sco,'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Syllabification of the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate function can handle many syllabification tasks in parallel, generating each output sentence simultaneously until all outputs contain at least one \\<EOV\\> token. This is faster than handling one sentence at a time, however we found that giving the whole test set in parallel results in GPU out-of-memory, so we came up with this solution that seems to be a good trade-off between parallelism and memory consumption.\n",
    "\n",
    "What we do is split the test set in batches of 100 verses, and call `evaluate` on one batch at a time passing the appropriate stopping condition.\n",
    "\n",
    "As an empirical proof, try using a `window_size` of 1: you will see that the ETA will grow to ~3 hours, while the whole process only took 20 minutes in our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [20:38<00:00, 34.41s/it]\n"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm.tqdm(range(math.ceil(len(input_test) / window_size))):\n",
    "    window = input_test[i*window_size:min((i + 1)*window_size, len(input_test))]\n",
    "    \n",
    "    encoder_input = tf.convert_to_tensor(window)\n",
    "    decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "    \n",
    "    output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n",
    "    \n",
    "    # Only take output before the first end of verse\n",
    "    stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "    stripped_output = list(map(strip_tokens, stripped_output))\n",
    "    \n",
    "    result += stripped_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3559 [00:26<3:48:15,  3.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
     ]
    }
   ],
   "source": [
    "window_size = 1\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm.tqdm(range(math.ceil(len(input_test) / window_size))):\n",
    "    window = input_test[i*window_size:min((i + 1)*window_size, len(input_test))]\n",
    "    \n",
    "    encoder_input = tf.convert_to_tensor(window)\n",
    "    decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "    \n",
    "    output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n",
    "    \n",
    "    # Only take output before the first end of verse\n",
    "    stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "    stripped_output = list(map(strip_tokens, stripped_output))\n",
    "    \n",
    "    result += stripped_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the syllabification we got from our model with the correct syllabification. The `validate_syllabification` function returns information about the verses that were correctly syllabified and the Levenshtein similarity (1 - edit distance) of each syllabified verse with the correct syllabification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_syll = target_test\n",
    "correct_syll = ' '.join(tokenizer.sequences_to_texts(correct_syll))\n",
    "correct_syll = strip_tokens(correct_syll)\n",
    "correct_syll = correct_syll.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches, similarities = zip(*validate_syllabification(result, correct_syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(exact_matches) / len(exact_matches)\n",
    "avg_similarities = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabification exact matches: 86.40%\n"
     ]
    }
   ],
   "source": [
    "print('Syllabification exact matches: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average similarity: {:.2f}'.format(avg_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_output = np.array(result)\n",
    "correct_syll = np.array(correct_syll)\n",
    "error_mask = ~np.array(exact_matches)\n",
    "\n",
    "errors_output = stripped_output[error_mask]\n",
    "errors_correct = correct_syll[error_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Syllabification of other \"poetry\"\n",
    "\n",
    "We thought it would be a fun experiment to see if the model could syllabify other poetry, not just hendecasyllabic verses. To stay true to our Roman roots we chose a classic folk roman song, which incidentally contains quite a few synalephas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<GO> È <SEP> u n a <SEP> c a n z o n e <SEP> s e n z a <SEP> t i t o l o <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e ’ <SEP> f a <SEP> q u a r c h e <SEP> c o s a <EOV>',\n",
       " '<GO> N o n <SEP> è <SEP> g n e n t e <SEP> d e <SEP> s t r a o r d i n a r i o <EOV>',\n",
       " '<GO> È <SEP> r o b b a <SEP> d e r <SEP> p a e s e <SEP> n o s t r o <EOV>',\n",
       " '<GO> C h e <SEP> s e <SEP> p o ’ <SEP> c a n t à <SEP> p u r e <SEP> s e n z a <SEP> v o c e <EOV>',\n",
       " '<GO> B a s t a <SEP> ’ a <SEP> s a l u t e <EOV>',\n",
       " \"<GO> Q u a n n o <SEP> c ' è <SEP> ' a <SEP> s a l u t e <SEP> c ' è <SEP> t u t t o <EOV>\",\n",
       " '<GO> B a s t a <SEP> ’ a <SEP> s a l u t e <SEP> e <SEP> u n <SEP> p a r <SEP> d e <SEP> s c a r p e <SEP> n o v e <EOV>',\n",
       " '<GO> P o i <SEP> g i r à <SEP> t u t t o <SEP> e r <SEP> m o n n o <EOV>',\n",
       " '<GO> E <SEP> m ’ a <SEP> a c c o m p a g n o <SEP> d a <SEP> m e <EOV>',\n",
       " '<GO> P e ’ <SEP> f a <SEP> l a <SEP> v i t a <SEP> m e n o <SEP> a m a r a <EOV>',\n",
       " \"<GO> M e <SEP> s o ’ <SEP> c o m p r a t o <SEP> ' s t a <SEP> c h i t a r a <EOV>\",\n",
       " '<GO> E <SEP> q u a n n o <SEP> e r <SEP> s o l e <SEP> s c e n n e <SEP> e <SEP> m o r e <EOV>',\n",
       " '<GO> M e <SEP> s e n t o <SEP> ’ n <SEP> c o r e <SEP> c a n t a t o r e <EOV>',\n",
       " '<GO> L a <SEP> v o c e <SEP> e ’ <SEP> p o c a <SEP> m a <SEP> ’ n t o n a t a <EOV>',\n",
       " '<GO> N u n <SEP> s e r v e <SEP> a <SEP> f a <SEP> ’ n a <SEP> s e r e n a t a <EOV>',\n",
       " \"<GO> M a <SEP> s o l a m e n t e <SEP> a <SEP> f a <SEP> ' n <SEP> m a n i e r a <EOV>\",\n",
       " '<GO> D e <SEP> f a m m e <SEP> ’ n <SEP> s o g n o <SEP> a <SEP> p r i m a <SEP> s e r a <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e r c h é <SEP> m e <SEP> s e n t o <SEP> u n <SEP> f r i c c i c o <SEP> n e r <SEP> c o r e <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> s o g n à <EOV>',\n",
       " '<GO> P e r c h é <SEP> n e r <SEP> p e t t o <SEP> m e <SEP> c e <SEP> n a s c h i <SEP> ’ n <SEP> f i o r e <EOV>',\n",
       " '<GO> F i o r e <SEP> d e <SEP> l i l l à <EOV>',\n",
       " \"<GO> C h e <SEP> m ' a r i p o r t i <SEP> v e r s o <SEP> e r <SEP> p r i m o <SEP> a m o r e <EOV>\",\n",
       " '<GO> C h e <SEP> s o s p i r a v a <SEP> l e <SEP> c a n z o n i <SEP> m i e <EOV>',\n",
       " '<GO> E <SEP> m ’ a r i t o n t o n i v a <SEP> d e <SEP> b u c i e <EOV>',\n",
       " '<GO> C a n z o n i <SEP> b e l l e <SEP> e <SEP> a p p a s s i o n a t e <EOV>',\n",
       " '<GO> C h e <SEP> R o m a <SEP> m i a <SEP> m ’ a r i c o r d a t e <EOV>',\n",
       " '<GO> C a n t a t e <SEP> s o l o <SEP> p e ’ <SEP> d i s p e t t o <EOV>',\n",
       " '<GO> M a <SEP> c o ’ <SEP> ’ n a <SEP> s m a n i a <SEP> d e n t r o <SEP> a r <SEP> p e t t o <EOV>',\n",
       " '<GO> I o <SEP> n u n <SEP> v e <SEP> c a n t o <SEP> a <SEP> v o c e <SEP> p i e n a <EOV>',\n",
       " '<GO> M a <SEP> t u t t a <SEP> l ’ a n i m a <SEP> è <SEP> s e r e n a <EOV>',\n",
       " '<GO> E <SEP> q u a n n o <SEP> e r <SEP> c i e l o <SEP> s e <SEP> s c o l o r a <EOV>',\n",
       " '<GO> D e <SEP> m e <SEP> n e s s u n a <SEP> s e <SEP> ’ n n a m o r a <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> c a n t à <EOV>',\n",
       " '<GO> P e r c h é <SEP> m e <SEP> s e n t o <SEP> u n <SEP> f r i c c i c o <SEP> n e r <SEP> c o r e <EOV>',\n",
       " '<GO> T a n t o <SEP> p e ’ <SEP> s o g n à <EOV>',\n",
       " '<GO> P e r c h é <SEP> n e r <SEP> p e t t o <SEP> m e <SEP> c e <SEP> n a s c h i <SEP> u n <SEP> f i o r e <EOV>',\n",
       " '<GO> F i o r e <SEP> d e <SEP> l i l l à <EOV>',\n",
       " '<GO> C h e <SEP> m ’ a r i p o r t i <SEP> v e r s o <SEP> e r <SEP> p r i m o <SEP> a m o r e <EOV>',\n",
       " '<GO> C h e <SEP> s o s p i r a v a <SEP> l e <SEP> c a n z o n i <SEP> m i e <EOV>',\n",
       " '<GO> E <SEP> m ’ a r i t o n t o n i v a <SEP> d e <SEP> b u c i e <EOV>',\n",
       " '<EOT> <EOV>']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbitrary_verses = \"\"\"\n",
    "È una canzone senza titolo\n",
    "Tanto pe’ cantà\n",
    "Pe’ fa quarche cosa\n",
    "Non è gnente de straordinario\n",
    "È robba der paese nostro\n",
    "Che se po’ cantà pure senza voce\n",
    "Basta ’a salute\n",
    "Quanno c'è 'a salute c'è tutto\n",
    "Basta ’a salute e un par de scarpe nove\n",
    "Poi girà tutto er monno\n",
    "E m’a accompagno da me\n",
    "Pe’ fa la vita meno amara\n",
    "Me so’ comprato 'sta chitara\n",
    "E quanno er sole scenne e more\n",
    "Me sento ’n core cantatore\n",
    "La voce e’ poca ma ’ntonata\n",
    "Nun serve a fa ’na serenata\n",
    "Ma solamente a fa 'n maniera\n",
    "De famme ’n sogno a prima sera\n",
    "Tanto pe’ cantà\n",
    "Perché me sento un friccico ner core\n",
    "Tanto pe’ sognà\n",
    "Perché ner petto me ce naschi ’n fiore\n",
    "Fiore de lillà\n",
    "Che m'ariporti verso er primo amore\n",
    "Che sospirava le canzoni mie\n",
    "E m’aritontoniva de bucie\n",
    "Canzoni belle e appassionate\n",
    "Che Roma mia m’aricordate\n",
    "Cantate solo pe’ dispetto\n",
    "Ma co’ ’na smania dentro ar petto\n",
    "Io nun ve canto a voce piena\n",
    "Ma tutta l’anima è serena\n",
    "E quanno er cielo se scolora\n",
    "De me nessuna se ’nnamora\n",
    "Tanto pe’ cantà\n",
    "Perché me sento un friccico ner core\n",
    "Tanto pe’ sognà\n",
    "Perché ner petto me ce naschi un fiore\n",
    "Fiore de lillà\n",
    "Che m’ariporti verso er primo amore\n",
    "Che sospirava le canzoni mie\n",
    "E m’aritontoniva de bucie\n",
    "\"\"\"\n",
    "\n",
    "arbitrary_verses = preprocess_text(arbitrary_verses)\n",
    "arbitrary_verses = [verse.strip() + ' <EOV>' for verse in arbitrary_verses.split('<EOV>')]\n",
    "arbitrary_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_verses = tokenizer.texts_to_sequences(arbitrary_verses)\n",
    "padded_verses = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    encoded_verses, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_input = tf.convert_to_tensor(padded_verses)\n",
    "decoder_input = tf.repeat([[start_symbol]], repeats=encoder_input.shape[0], axis=0)\n",
    "\n",
    "output = evaluate(transformer, encoder_input, decoder_input,  stop_symbol, stopping_condition=stop_after_stop_symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take output before the first end of verse\n",
    "stripped_output = list(map(lambda x: x.split('<EOV>')[0], tokenizer.sequences_to_texts(output.numpy())))\n",
    "stripped_output = list(map(strip_tokens, stripped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|T u|na |can|zo|ne |sen|za |ti|to|lo',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Pe’ |fa |quar|che |co|sa',\n",
       " '|Non |è |gnen|te |de |stra|or|di|na|rio',\n",
       " '|T |rob|ba |der |pa|e|se |no|stro',\n",
       " '|Che |se |po’ |can|tà |pu|re |sen|za |vo|ce',\n",
       " '|Ba|sta ’a |sa|lu|te',\n",
       " '|Quan|no |cè |a |sa|lu|te |cè |tut|to',\n",
       " '|Ba|sta ’a |sa|lu|te e |un |par |de |scar|pe |no|ve',\n",
       " '|Poi |gi|rà |tut|to er |mon|no',\n",
       " '|E |m’ ac|ca|com|pa|pno |da |me',\n",
       " '|Pe’ |fa |la |vi|ta |me|no a|ma|ra',\n",
       " '|Me |so’ |com|pra|to |sta |chi|ta|ra|ra',\n",
       " '|E |quan|no er |so|le |scen|ne e |e |mo|re',\n",
       " '|Me |sen|to ’n |co|re |can|tan|to|re',\n",
       " '|La |vo|ce e’’ |po|ca |ma ’n|to|na|ta|ta',\n",
       " '|Nun |ser|ve a |fa |fa|na |se|re|na|ta',\n",
       " '|Ma |so|la|men|te a |fa |n |ma|na|ra',\n",
       " '|De |fam|me ’n |so|gno a |pri|ma |se|ra',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Per|ché |me |sen|to un |fric|ci|co |ner |co|re',\n",
       " '|Tan|to |pe’ |so|gnà',\n",
       " '|Per|ché |ner |pet|to |me |ce |na|schi ’n |fio|re',\n",
       " '|Fio|re |de |lil|là',\n",
       " '|Che |ma|ri|por|ti |ver|so er |pri|mo a|mo|re',\n",
       " '|Che |so|spi|ra|va |le |can|zo|ni |mie',\n",
       " '|E |m’ a|ri|ton|to|ni|va |de |bu|cu|can|ton|to',\n",
       " '|Can|zo|ni |bel|le e |ap|pas|sio|na|te',\n",
       " '|Che |Ro|ma |mia |m’ a|ri|cor|da|te',\n",
       " '|Can|ta|te |so|lo |pe’ |di|spet|to',\n",
       " '|Ma |co’ |’na |sma|nia |den|tro ar |per |po',\n",
       " '|Io |nun |ve |can|to a |vo|ce |pie|na',\n",
       " '|Ma |tut|ta |l’ a|ni|ma è |se|re|na',\n",
       " '|E |quan|no er |cie|lo |se |seo|lo|ra',\n",
       " '|De |me |nes|su|na |se ’n|na|mo|ra',\n",
       " '|Tan|to |pe’ |can|tà',\n",
       " '|Per|ché |me |sen|to un |fric|ci|co |ner |co|re',\n",
       " '|Tan|to |pe’ |so|gnà',\n",
       " '|Per|ché |ner |pet|to |me |ce |na|schi un |fio|re',\n",
       " '|Fio|re |de |lil|là',\n",
       " '|Che |m’ a|ri|por|ti |ver|so er |pri|mo a|mo|re',\n",
       " '|Che |so|spi|ra|va |le |can|zo|ni |mie',\n",
       " '|E |m’ a|ri|ton|to|ni|va |de |bu|cu|can|ton|to',\n",
       " '|S’ |S’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |s’ |so|s’ |s']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character_level_transformer_syllabification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
