{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-input syll-output transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqjF58zbxeBa",
        "outputId": "52310b00-da78-4681-8c08-ddcb97b644a5"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfQD7yAgxeBn",
        "outputId": "430e000f-bc55-4f4d-e2c1-58f77286f2e7"
      },
      "source": [
        "# TODO\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NNZisZExeBp"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81DRc6NMxeBq"
      },
      "source": [
        "raw_input_text, _, _= load_verses(\n",
        "    input_file, char_level=False, pad=False\n",
        ")\n",
        "raw_target_text, _, _ = load_verses(\n",
        "    target_file, char_level=False, pad=False\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gbbqK58xeBq"
      },
      "source": [
        "def preprocess_target(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace(' ', ' <SEP> ')\n",
        "    x = x.replace('|', ' <SYL> ')\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x = re.sub(r'([,’.;«»:?!“”—‘\\-\"()])', r\" \\1 \", x)\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    x = '<GO> ' + x\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCENLJWxeBr"
      },
      "source": [
        "input_text = [verse.strip() for verse in raw_input_text.split('\\n') if verse.strip() != '']\n",
        "input_text = list(map(preprocess_input, input_text))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvY5HrwqxeBr"
      },
      "source": [
        "target_text = [verse.strip() for verse in raw_target_text.split('\\n') if verse.strip() != '']\n",
        "target_text = list(map(preprocess_target, target_text))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPkwKFkcxeBt"
      },
      "source": [
        "input_tercets = []\n",
        "target_tercets = []\n",
        "\n",
        "for line in range(len(input_text) - 6):\n",
        "    input_tercets.append(' '.join(input_text[line:line+3]) + ' <EOT>')\n",
        "    target_tercets.append(' '.join(target_text[line+3:line+6]) + ' <EOT>')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N37AvekuxeBv"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "input_tokenizer.fit_on_texts(input_tercets)\n",
        "input_text = input_tokenizer.texts_to_sequences(input_tercets)\n",
        "\n",
        "input_vocab = set(input_tokenizer.word_index.keys())\n",
        "input_vocab_size = len(input_vocab) + 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDeugXXJxeBw"
      },
      "source": [
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters=\"\", char_level=False, lower=False\n",
        ")\n",
        "target_tokenizer.fit_on_texts(target_tercets)\n",
        "target_text = target_tokenizer.texts_to_sequences(target_tercets)\n",
        "\n",
        "target_vocab = set(target_tokenizer.word_index.keys())\n",
        "target_vocab_size = len(target_vocab) + 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEE2bQTVxeBw"
      },
      "source": [
        "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_text, padding=\"post\"\n",
        ")\n",
        "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_text, padding=\"post\"\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujuvEQLrxeBw"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input, padded_target\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AGR1mb_xeBy"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/word-syll-gen\"\n",
        "\n",
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=checkpoint_path\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQxxCMlxeBz",
        "outputId": "9ef4f584-b3dc-4925-d217-bb72e66777c5"
      },
      "source": [
        "transformer_trainer.train(dataset, EPOCHS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.9046 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.9916 Accuracy 0.1424\n",
            "Epoch 1 Batch 100 Loss 6.0812 Accuracy 0.2255\n",
            "Epoch 1 Batch 150 Loss 5.5818 Accuracy 0.2563\n",
            "Epoch 1 Loss 5.4629 Accuracy 0.2630\n",
            "Time taken for 1 epoch: 91.69 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.1807 Accuracy 0.3384\n",
            "Epoch 2 Batch 50 Loss 3.9072 Accuracy 0.4051\n",
            "Epoch 2 Batch 100 Loss 3.5955 Accuracy 0.4377\n",
            "Epoch 2 Batch 150 Loss 3.3668 Accuracy 0.4530\n",
            "Epoch 2 Loss 3.3121 Accuracy 0.4563\n",
            "Time taken for 1 epoch: 79.03 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.7288 Accuracy 0.4849\n",
            "Epoch 3 Batch 50 Loss 2.6522 Accuracy 0.4931\n",
            "Epoch 3 Batch 100 Loss 2.5942 Accuracy 0.4984\n",
            "Epoch 3 Batch 150 Loss 2.5416 Accuracy 0.5073\n",
            "Epoch 3 Loss 2.5269 Accuracy 0.5098\n",
            "Time taken for 1 epoch: 78.64 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.3562 Accuracy 0.5393\n",
            "Epoch 4 Batch 50 Loss 2.3385 Accuracy 0.5428\n",
            "Epoch 4 Batch 100 Loss 2.3177 Accuracy 0.5472\n",
            "Epoch 4 Batch 150 Loss 2.2993 Accuracy 0.5504\n",
            "Epoch 4 Loss 2.2936 Accuracy 0.5513\n",
            "Time taken for 1 epoch: 78.99 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2279 Accuracy 0.5602\n",
            "Epoch 5 Batch 50 Loss 2.2126 Accuracy 0.5653\n",
            "Epoch 5 Batch 100 Loss 2.1930 Accuracy 0.5674\n",
            "Epoch 5 Batch 150 Loss 2.1730 Accuracy 0.5699\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-syll-gen/ckpt-1\n",
            "Epoch 5 Loss 2.1674 Accuracy 0.5707\n",
            "Time taken for 1 epoch: 79.16 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.1246 Accuracy 0.5731\n",
            "Epoch 6 Batch 50 Loss 2.0849 Accuracy 0.5801\n",
            "Epoch 6 Batch 100 Loss 2.0684 Accuracy 0.5830\n",
            "Epoch 6 Batch 150 Loss 2.0555 Accuracy 0.5848\n",
            "Epoch 6 Loss 2.0511 Accuracy 0.5853\n",
            "Time taken for 1 epoch: 78.78 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.0310 Accuracy 0.5922\n",
            "Epoch 7 Batch 50 Loss 1.9843 Accuracy 0.5949\n",
            "Epoch 7 Batch 100 Loss 1.9721 Accuracy 0.5963\n",
            "Epoch 7 Batch 150 Loss 1.9600 Accuracy 0.5978\n",
            "Epoch 7 Loss 1.9563 Accuracy 0.5984\n",
            "Time taken for 1 epoch: 78.40 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.9044 Accuracy 0.6105\n",
            "Epoch 8 Batch 50 Loss 1.8922 Accuracy 0.6075\n",
            "Epoch 8 Batch 100 Loss 1.8796 Accuracy 0.6096\n",
            "Epoch 8 Batch 150 Loss 1.8659 Accuracy 0.6118\n",
            "Epoch 8 Loss 1.8618 Accuracy 0.6124\n",
            "Time taken for 1 epoch: 78.88 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.8053 Accuracy 0.6197\n",
            "Epoch 9 Batch 50 Loss 1.7885 Accuracy 0.6243\n",
            "Epoch 9 Batch 100 Loss 1.7765 Accuracy 0.6262\n",
            "Epoch 9 Batch 150 Loss 1.7623 Accuracy 0.6283\n",
            "Epoch 9 Loss 1.7582 Accuracy 0.6290\n",
            "Time taken for 1 epoch: 78.22 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.6953 Accuracy 0.6353\n",
            "Epoch 10 Batch 50 Loss 1.6800 Accuracy 0.6402\n",
            "Epoch 10 Batch 100 Loss 1.6728 Accuracy 0.6415\n",
            "Epoch 10 Batch 150 Loss 1.6653 Accuracy 0.6428\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-syll-gen/ckpt-2\n",
            "Epoch 10 Loss 1.6631 Accuracy 0.6433\n",
            "Time taken for 1 epoch: 79.09 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.5937 Accuracy 0.6552\n",
            "Epoch 11 Batch 50 Loss 1.5984 Accuracy 0.6533\n",
            "Epoch 11 Batch 100 Loss 1.5919 Accuracy 0.6541\n",
            "Epoch 11 Batch 150 Loss 1.5827 Accuracy 0.6556\n",
            "Epoch 11 Loss 1.5805 Accuracy 0.6559\n",
            "Time taken for 1 epoch: 78.21 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.5218 Accuracy 0.6704\n",
            "Epoch 12 Batch 50 Loss 1.5211 Accuracy 0.6650\n",
            "Epoch 12 Batch 100 Loss 1.5188 Accuracy 0.6652\n",
            "Epoch 12 Batch 150 Loss 1.5128 Accuracy 0.6661\n",
            "Epoch 12 Loss 1.5112 Accuracy 0.6662\n",
            "Time taken for 1 epoch: 78.45 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.4597 Accuracy 0.6733\n",
            "Epoch 13 Batch 50 Loss 1.4522 Accuracy 0.6755\n",
            "Epoch 13 Batch 100 Loss 1.4526 Accuracy 0.6758\n",
            "Epoch 13 Batch 150 Loss 1.4493 Accuracy 0.6762\n",
            "Epoch 13 Loss 1.4482 Accuracy 0.6763\n",
            "Time taken for 1 epoch: 78.24 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.3857 Accuracy 0.6849\n",
            "Epoch 14 Batch 50 Loss 1.3983 Accuracy 0.6843\n",
            "Epoch 14 Batch 100 Loss 1.3966 Accuracy 0.6844\n",
            "Epoch 14 Batch 150 Loss 1.3918 Accuracy 0.6850\n",
            "Epoch 14 Loss 1.3912 Accuracy 0.6851\n",
            "Time taken for 1 epoch: 78.58 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.3476 Accuracy 0.6921\n",
            "Epoch 15 Batch 50 Loss 1.3407 Accuracy 0.6929\n",
            "Epoch 15 Batch 100 Loss 1.3395 Accuracy 0.6933\n",
            "Epoch 15 Batch 150 Loss 1.3379 Accuracy 0.6934\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/word-syll-gen/ckpt-3\n",
            "Epoch 15 Loss 1.3374 Accuracy 0.6935\n",
            "Time taken for 1 epoch: 78.79 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.3009 Accuracy 0.7004\n",
            "Epoch 16 Batch 50 Loss 1.2832 Accuracy 0.7022\n",
            "Epoch 16 Batch 100 Loss 1.2848 Accuracy 0.7019\n",
            "Epoch 16 Batch 150 Loss 1.2856 Accuracy 0.7016\n",
            "Epoch 16 Loss 1.2849 Accuracy 0.7018\n",
            "Time taken for 1 epoch: 78.69 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.2410 Accuracy 0.7041\n",
            "Epoch 17 Batch 50 Loss 1.2276 Accuracy 0.7122\n",
            "Epoch 17 Batch 100 Loss 1.2326 Accuracy 0.7111\n",
            "Epoch 17 Batch 150 Loss 1.2359 Accuracy 0.7101\n",
            "Epoch 17 Loss 1.2356 Accuracy 0.7100\n",
            "Time taken for 1 epoch: 78.15 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.1649 Accuracy 0.7219\n",
            "Epoch 18 Batch 50 Loss 1.1794 Accuracy 0.7195\n",
            "Epoch 18 Batch 100 Loss 1.1851 Accuracy 0.7183\n",
            "Epoch 18 Batch 150 Loss 1.1875 Accuracy 0.7178\n",
            "Epoch 18 Loss 1.1875 Accuracy 0.7179\n",
            "Time taken for 1 epoch: 78.48 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.1455 Accuracy 0.7249\n",
            "Epoch 19 Batch 50 Loss 1.1311 Accuracy 0.7271\n",
            "Epoch 19 Batch 100 Loss 1.1356 Accuracy 0.7263\n",
            "Epoch 19 Batch 150 Loss 1.1396 Accuracy 0.7257\n",
            "Epoch 19 Loss 1.1403 Accuracy 0.7255\n",
            "Time taken for 1 epoch: 78.32 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0609 Accuracy 0.7379\n",
            "Epoch 20 Batch 50 Loss 1.0879 Accuracy 0.7355\n",
            "Epoch 20 Batch 100 Loss 1.0935 Accuracy 0.7340\n",
            "Epoch 20 Batch 150 Loss 1.0964 Accuracy 0.7333\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/word-syll-gen/ckpt-4\n",
            "Epoch 20 Loss 1.0967 Accuracy 0.7332\n",
            "Time taken for 1 epoch: 79.24 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.0364 Accuracy 0.7466\n",
            "Epoch 21 Batch 50 Loss 1.0413 Accuracy 0.7425\n",
            "Epoch 21 Batch 100 Loss 1.0486 Accuracy 0.7418\n",
            "Epoch 21 Batch 150 Loss 1.0529 Accuracy 0.7407\n",
            "Epoch 21 Loss 1.0536 Accuracy 0.7406\n",
            "Time taken for 1 epoch: 78.29 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.9790 Accuracy 0.7603\n",
            "Epoch 22 Batch 50 Loss 0.9979 Accuracy 0.7508\n",
            "Epoch 22 Batch 100 Loss 1.0038 Accuracy 0.7492\n",
            "Epoch 22 Batch 150 Loss 1.0090 Accuracy 0.7482\n",
            "Epoch 22 Loss 1.0101 Accuracy 0.7480\n",
            "Time taken for 1 epoch: 78.52 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9462 Accuracy 0.7616\n",
            "Epoch 23 Batch 50 Loss 0.9541 Accuracy 0.7591\n",
            "Epoch 23 Batch 100 Loss 0.9630 Accuracy 0.7570\n",
            "Epoch 23 Batch 150 Loss 0.9719 Accuracy 0.7550\n",
            "Epoch 23 Loss 0.9722 Accuracy 0.7551\n",
            "Time taken for 1 epoch: 78.16 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.8770 Accuracy 0.7801\n",
            "Epoch 24 Batch 50 Loss 0.9115 Accuracy 0.7670\n",
            "Epoch 24 Batch 100 Loss 0.9240 Accuracy 0.7643\n",
            "Epoch 24 Batch 150 Loss 0.9302 Accuracy 0.7626\n",
            "Epoch 24 Loss 0.9318 Accuracy 0.7621\n",
            "Time taken for 1 epoch: 78.65 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.8589 Accuracy 0.7727\n",
            "Epoch 25 Batch 50 Loss 0.8766 Accuracy 0.7732\n",
            "Epoch 25 Batch 100 Loss 0.8852 Accuracy 0.7713\n",
            "Epoch 25 Batch 150 Loss 0.8919 Accuracy 0.7698\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/word-syll-gen/ckpt-5\n",
            "Epoch 25 Loss 0.8930 Accuracy 0.7695\n",
            "Time taken for 1 epoch: 78.80 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.8155 Accuracy 0.7874\n",
            "Epoch 26 Batch 50 Loss 0.8325 Accuracy 0.7809\n",
            "Epoch 26 Batch 100 Loss 0.8403 Accuracy 0.7796\n",
            "Epoch 26 Batch 150 Loss 0.8460 Accuracy 0.7785\n",
            "Epoch 26 Loss 0.8463 Accuracy 0.7785\n",
            "Time taken for 1 epoch: 78.83 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.7781 Accuracy 0.7935\n",
            "Epoch 27 Batch 50 Loss 0.7823 Accuracy 0.7915\n",
            "Epoch 27 Batch 100 Loss 0.7923 Accuracy 0.7893\n",
            "Epoch 27 Batch 150 Loss 0.7988 Accuracy 0.7877\n",
            "Epoch 27 Loss 0.8000 Accuracy 0.7875\n",
            "Time taken for 1 epoch: 78.38 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.7172 Accuracy 0.8102\n",
            "Epoch 28 Batch 50 Loss 0.7383 Accuracy 0.8012\n",
            "Epoch 28 Batch 100 Loss 0.7499 Accuracy 0.7987\n",
            "Epoch 28 Batch 150 Loss 0.7559 Accuracy 0.7972\n",
            "Epoch 28 Loss 0.7577 Accuracy 0.7968\n",
            "Time taken for 1 epoch: 78.94 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.6919 Accuracy 0.8121\n",
            "Epoch 29 Batch 50 Loss 0.6967 Accuracy 0.8102\n",
            "Epoch 29 Batch 100 Loss 0.7061 Accuracy 0.8080\n",
            "Epoch 29 Batch 150 Loss 0.7135 Accuracy 0.8062\n",
            "Epoch 29 Loss 0.7154 Accuracy 0.8057\n",
            "Time taken for 1 epoch: 78.73 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.6192 Accuracy 0.8291\n",
            "Epoch 30 Batch 50 Loss 0.6608 Accuracy 0.8184\n",
            "Epoch 30 Batch 100 Loss 0.6714 Accuracy 0.8155\n",
            "Epoch 30 Batch 150 Loss 0.6758 Accuracy 0.8141\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/word-syll-gen/ckpt-6\n",
            "Epoch 30 Loss 0.6768 Accuracy 0.8138\n",
            "Time taken for 1 epoch: 79.75 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.5822 Accuracy 0.8360\n",
            "Epoch 31 Batch 50 Loss 0.6198 Accuracy 0.8269\n",
            "Epoch 31 Batch 100 Loss 0.6309 Accuracy 0.8240\n",
            "Epoch 31 Batch 150 Loss 0.6391 Accuracy 0.8219\n",
            "Epoch 31 Loss 0.6415 Accuracy 0.8213\n",
            "Time taken for 1 epoch: 78.92 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.6012 Accuracy 0.8281\n",
            "Epoch 32 Batch 50 Loss 0.5869 Accuracy 0.8337\n",
            "Epoch 32 Batch 100 Loss 0.5971 Accuracy 0.8316\n",
            "Epoch 32 Batch 150 Loss 0.6055 Accuracy 0.8296\n",
            "Epoch 32 Loss 0.6077 Accuracy 0.8290\n",
            "Time taken for 1 epoch: 79.20 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.5539 Accuracy 0.8399\n",
            "Epoch 33 Batch 50 Loss 0.5570 Accuracy 0.8412\n",
            "Epoch 33 Batch 100 Loss 0.5679 Accuracy 0.8384\n",
            "Epoch 33 Batch 150 Loss 0.5736 Accuracy 0.8374\n",
            "Epoch 33 Loss 0.5755 Accuracy 0.8369\n",
            "Time taken for 1 epoch: 78.74 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.5190 Accuracy 0.8530\n",
            "Epoch 34 Batch 50 Loss 0.5250 Accuracy 0.8492\n",
            "Epoch 34 Batch 100 Loss 0.5368 Accuracy 0.8460\n",
            "Epoch 34 Batch 150 Loss 0.5441 Accuracy 0.8439\n",
            "Epoch 34 Loss 0.5466 Accuracy 0.8434\n",
            "Time taken for 1 epoch: 79.09 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.4994 Accuracy 0.8538\n",
            "Epoch 35 Batch 50 Loss 0.4988 Accuracy 0.8548\n",
            "Epoch 35 Batch 100 Loss 0.5104 Accuracy 0.8522\n",
            "Epoch 35 Batch 150 Loss 0.5176 Accuracy 0.8504\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/word-syll-gen/ckpt-7\n",
            "Epoch 35 Loss 0.5196 Accuracy 0.8501\n",
            "Time taken for 1 epoch: 79.33 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.4785 Accuracy 0.8577\n",
            "Epoch 36 Batch 50 Loss 0.4702 Accuracy 0.8619\n",
            "Epoch 36 Batch 100 Loss 0.4836 Accuracy 0.8583\n",
            "Epoch 36 Batch 150 Loss 0.4908 Accuracy 0.8565\n",
            "Epoch 36 Loss 0.4928 Accuracy 0.8560\n",
            "Time taken for 1 epoch: 79.06 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.4384 Accuracy 0.8727\n",
            "Epoch 37 Batch 50 Loss 0.4497 Accuracy 0.8677\n",
            "Epoch 37 Batch 100 Loss 0.4567 Accuracy 0.8655\n",
            "Epoch 37 Batch 150 Loss 0.4648 Accuracy 0.8632\n",
            "Epoch 37 Loss 0.4664 Accuracy 0.8629\n",
            "Time taken for 1 epoch: 78.81 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.4319 Accuracy 0.8723\n",
            "Epoch 38 Batch 50 Loss 0.4228 Accuracy 0.8741\n",
            "Epoch 38 Batch 100 Loss 0.4343 Accuracy 0.8708\n",
            "Epoch 38 Batch 150 Loss 0.4442 Accuracy 0.8681\n",
            "Epoch 38 Loss 0.4459 Accuracy 0.8677\n",
            "Time taken for 1 epoch: 79.08 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.3922 Accuracy 0.8801\n",
            "Epoch 39 Batch 50 Loss 0.4038 Accuracy 0.8791\n",
            "Epoch 39 Batch 100 Loss 0.4136 Accuracy 0.8763\n",
            "Epoch 39 Batch 150 Loss 0.4229 Accuracy 0.8738\n",
            "Epoch 39 Loss 0.4251 Accuracy 0.8732\n",
            "Time taken for 1 epoch: 78.54 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.3748 Accuracy 0.8864\n",
            "Epoch 40 Batch 50 Loss 0.3841 Accuracy 0.8843\n",
            "Epoch 40 Batch 100 Loss 0.3946 Accuracy 0.8812\n",
            "Epoch 40 Batch 150 Loss 0.4024 Accuracy 0.8791\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/word-syll-gen/ckpt-8\n",
            "Epoch 40 Loss 0.4040 Accuracy 0.8787\n",
            "Time taken for 1 epoch: 79.58 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.3520 Accuracy 0.8887\n",
            "Epoch 41 Batch 50 Loss 0.3677 Accuracy 0.8882\n",
            "Epoch 41 Batch 100 Loss 0.3783 Accuracy 0.8854\n",
            "Epoch 41 Batch 150 Loss 0.3853 Accuracy 0.8835\n",
            "Epoch 41 Loss 0.3872 Accuracy 0.8829\n",
            "Time taken for 1 epoch: 78.85 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.3644 Accuracy 0.8894\n",
            "Epoch 42 Batch 50 Loss 0.3493 Accuracy 0.8934\n",
            "Epoch 42 Batch 100 Loss 0.3575 Accuracy 0.8910\n",
            "Epoch 42 Batch 150 Loss 0.3673 Accuracy 0.8883\n",
            "Epoch 42 Loss 0.3693 Accuracy 0.8878\n",
            "Time taken for 1 epoch: 79.35 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.3188 Accuracy 0.8994\n",
            "Epoch 43 Batch 50 Loss 0.3389 Accuracy 0.8960\n",
            "Epoch 43 Batch 100 Loss 0.3444 Accuracy 0.8945\n",
            "Epoch 43 Batch 150 Loss 0.3517 Accuracy 0.8924\n",
            "Epoch 43 Loss 0.3537 Accuracy 0.8918\n",
            "Time taken for 1 epoch: 78.81 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.3256 Accuracy 0.8987\n",
            "Epoch 44 Batch 50 Loss 0.3214 Accuracy 0.9009\n",
            "Epoch 44 Batch 100 Loss 0.3313 Accuracy 0.8979\n",
            "Epoch 44 Batch 150 Loss 0.3393 Accuracy 0.8956\n",
            "Epoch 44 Loss 0.3410 Accuracy 0.8952\n",
            "Time taken for 1 epoch: 79.23 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.2959 Accuracy 0.9094\n",
            "Epoch 45 Batch 50 Loss 0.3087 Accuracy 0.9049\n",
            "Epoch 45 Batch 100 Loss 0.3181 Accuracy 0.9021\n",
            "Epoch 45 Batch 150 Loss 0.3271 Accuracy 0.8995\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/word-syll-gen/ckpt-9\n",
            "Epoch 45 Loss 0.3290 Accuracy 0.8989\n",
            "Time taken for 1 epoch: 79.67 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3107 Accuracy 0.9007\n",
            "Epoch 46 Batch 50 Loss 0.2998 Accuracy 0.9069\n",
            "Epoch 46 Batch 100 Loss 0.3071 Accuracy 0.9048\n",
            "Epoch 46 Batch 150 Loss 0.3131 Accuracy 0.9031\n",
            "Epoch 46 Loss 0.3152 Accuracy 0.9025\n",
            "Time taken for 1 epoch: 79.41 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.2774 Accuracy 0.9092\n",
            "Epoch 47 Batch 50 Loss 0.2823 Accuracy 0.9118\n",
            "Epoch 47 Batch 100 Loss 0.2926 Accuracy 0.9087\n",
            "Epoch 47 Batch 150 Loss 0.2999 Accuracy 0.9067\n",
            "Epoch 47 Loss 0.3020 Accuracy 0.9062\n",
            "Time taken for 1 epoch: 79.04 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.2588 Accuracy 0.9191\n",
            "Epoch 48 Batch 50 Loss 0.2714 Accuracy 0.9151\n",
            "Epoch 48 Batch 100 Loss 0.2810 Accuracy 0.9124\n",
            "Epoch 48 Batch 150 Loss 0.2896 Accuracy 0.9099\n",
            "Epoch 48 Loss 0.2921 Accuracy 0.9091\n",
            "Time taken for 1 epoch: 79.41 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.2717 Accuracy 0.9172\n",
            "Epoch 49 Batch 50 Loss 0.2623 Accuracy 0.9174\n",
            "Epoch 49 Batch 100 Loss 0.2717 Accuracy 0.9146\n",
            "Epoch 49 Batch 150 Loss 0.2790 Accuracy 0.9124\n",
            "Epoch 49 Loss 0.2809 Accuracy 0.9119\n",
            "Time taken for 1 epoch: 79.08 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.2525 Accuracy 0.9189\n",
            "Epoch 50 Batch 50 Loss 0.2515 Accuracy 0.9205\n",
            "Epoch 50 Batch 100 Loss 0.2616 Accuracy 0.9178\n",
            "Epoch 50 Batch 150 Loss 0.2688 Accuracy 0.9156\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/word-syll-gen/ckpt-10\n",
            "Epoch 50 Loss 0.2708 Accuracy 0.9150\n",
            "Time taken for 1 epoch: 79.97 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "TODO change this :)\n",
        "\n",
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVhkqi-_D1nQ"
      },
      "source": [
        "def postprocess(x):\n",
        "    x = x.replace('<SEP>', ' ')\n",
        "    x = x.replace('<SYL>', '|')\n",
        "    x = x.replace(' <GO> ', '\\n')\n",
        "    return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "ed0db7b8-f7be-4e01-f303-d0b44d68b809"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> nel <SEP> <SYL> suo <SEP> <SYL> lu <SYL> me <SEP> <SYL> die <SYL> tro <SEP> a <SEP> <SYL> le <SEP> <SYL> ser <SYL> pi <SYL> ne <SEP> , <SEP> <GO> <SYL> di <SEP> <SYL> quel <SYL> la <SEP> <SYL> fie <SYL> ra <SEP> <SYL> che <SEP> <SYL> to <SYL> sto <SEP> <SYL> s <SEP> ’ <SEP> <SEP> a <SYL> spet <SYL> ta <SEP> ; <SEP> <GO> <SYL> e <SEP> <SYL> io <SEP> , <SEP> <SEP> <SYL> se <SEP> <SYL> non <SEP> <SYL> fos <SYL> si <SEP> <SYL> pur <SEP> <SYL> a <SEP> <SYL> sé <SEP> <SYL> no <SYL> ta <SEP> . <SEP> <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usetPrX-EvPl",
        "outputId": "205b8675-4906-4914-f4b8-5e30dcd964bc"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| e   | nel   | suo   | lu | me   | die | tro   a   | le   | ser | pi | ne   ,  \n",
            "| di   | quel | la   | fie | ra   | che   | to | sto   | s   ’     a | spet | ta   ;  \n",
            "| e   | io   ,     | se   | non   | fos | si   | pur   | a   | sé   | no | ta   .   <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() #qui potrebbe anche essere [0,0]\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "edc4c179-7776-415b-ebce-dd8929618738"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SYL> e <SEP> <SYL> poi <SEP> <SYL> che <SEP> <SYL> di <SEP> <SYL> là <SEP> <SYL> mi <SEP> <SYL> ven <SYL> det <SYL> ta <SEP> <SYL> pun <SYL> to <SEP> , <SEP> <GO> <SYL> con <SYL> vien <SEP> <SYL> ch <SEP> ’ <SEP> <SEP> i <SEP> ’ <SEP> <SEP> <SYL> cre <SYL> do <SEP> <SYL> che <SEP> <SYL> l <SEP> ’ <SEP> <SEP> al <SYL> to <SEP> <SYL> mon <SYL> do <SEP> , <SEP> <GO> <SYL> con <SEP> <SYL> tut <SYL> te <SEP> <SYL> le <SEP> <SYL> co <SYL> se <SEP> <SYL> che <SEP> <SYL> qui <SEP> <SYL> si <SEP> <SYL> mon <SYL> da <SEP> . <SEP> <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBrcICi_F1P9",
        "outputId": "bce4e550-ca2c-4caa-e1d6-1d4a177c63ba"
      },
      "source": [
        "print(postprocess(generated_text))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| e   | poi   | che   | di   | là   | mi   | ven | det | ta   | pun | to   ,  \n",
            "| con | vien   | ch   ’     i   ’     | cre | do   | che   | l   ’     al | to   | mon | do   ,  \n",
            "| con   | tut | te   | le   | co | se   | che   | qui   | si   | mon | da   .   <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}