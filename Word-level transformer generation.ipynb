{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-level transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcDazsVbL_tS",
        "outputId": "eeb8d750-043d-4e12-eb95-d4bfb6d7d167"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTxmMPGM04O",
        "outputId": "d3567f6c-bc2b-4ab6-af77-df386f705ad7"
      },
      "source": [
        "!tar zxvf deepcomedy.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deepcomedy/\n",
            "deepcomedy/util/\n",
            "deepcomedy/util/predicate.py\n",
            "deepcomedy/util/__init__.py\n",
            "deepcomedy/util/.ipynb_checkpoints/\n",
            "deepcomedy/util/.ipynb_checkpoints/predicate-checkpoint.py\n",
            "deepcomedy/models/\n",
            "deepcomedy/models/layers.py\n",
            "deepcomedy/models/transformer.py\n",
            "deepcomedy/models/__pycache__/\n",
            "deepcomedy/models/__pycache__/layers.cpython-37.pyc\n",
            "deepcomedy/models/__pycache__/__init__.cpython-37.pyc\n",
            "deepcomedy/models/__pycache__/transformer.cpython-37.pyc\n",
            "deepcomedy/models/__init__.py\n",
            "deepcomedy/models/.ipynb_checkpoints/\n",
            "deepcomedy/models/.ipynb_checkpoints/transformer-checkpoint.py\n",
            "deepcomedy/preprocessing.py\n",
            "deepcomedy/__pycache__/\n",
            "deepcomedy/__pycache__/__init__.cpython-37.pyc\n",
            "deepcomedy/__pycache__/preprocessing.cpython-37.pyc\n",
            "deepcomedy/__init__.py\n",
            "deepcomedy/.ipynb_checkpoints/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQw0NL5L_tl"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUtq8DQkL_tm"
      },
      "source": [
        "raw_input_text, input_text, input_tokenizer = load_verses(\n",
        "    input_file, char_level=False, pad=False\n",
        ")\n",
        "raw_target_text, target_text, target_tokenizer = load_verses(\n",
        "    target_file, char_level=True, pad=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "a4480a72-7635-4c7b-a833-a88f29c21a60"
      },
      "source": [
        "print(\"Length of input text: {} characters\".format(len(raw_input_text)))\n",
        "print(\"Length of target text: {} characters\".format(len(raw_target_text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input text: 558637 characters\n",
            "Length of target text: 873431 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "l1G45yR9Y1da"
      },
      "source": [
        "input_vocab = sorted(set(input_tokenizer.word_index.keys()))\n",
        "target_vocab = sorted(set(target_tokenizer.word_index.keys()))\n",
        "\n",
        "# + 1 to account for padding token \"0\"\n",
        "input_vocab_size = len(input_vocab) + 1\n",
        "target_vocab_size = len(target_vocab) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "2abd931b-d7f3-4cc7-8cf3-ca3464489684"
      },
      "source": [
        "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
        "print(\"Target vocab size: {}\".format(target_vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 20750\n",
            "Target vocab size: 82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKv92yAL_t8"
      },
      "source": [
        "input_tercets = []\n",
        "target_tercets = []\n",
        "\n",
        "for line in range(len(input_text) - 6):\n",
        "    input_tercets.append(list(chain(*input_text[line : line + 3])))\n",
        "    target_tercets.append(list(chain(*target_text[line + 3 : line + 6])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbLJmP4L_t-"
      },
      "source": [
        "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_tercets, padding=\"post\"\n",
        ")\n",
        "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_tercets, padding=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjAxjw_8L_uB"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input, padded_target\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yMOh-7-L_uD"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/word-level-gen\"\n",
        "\n",
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=checkpoint_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaR03YUNL_uF",
        "outputId": "91d27376-1758-40fc-e0ae-391d3b79869c"
      },
      "source": [
        "transformer_trainer.train(dataset, EPOCHS)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.9069 Accuracy 0.6899\n",
            "Epoch 1 Batch 50 Loss 0.9018 Accuracy 0.6934\n",
            "Epoch 1 Batch 100 Loss 0.9042 Accuracy 0.6928\n",
            "Epoch 1 Batch 150 Loss 0.9058 Accuracy 0.6924\n",
            "Epoch 1 Loss 0.9064 Accuracy 0.6922\n",
            "Time taken for 1 epoch: 79.02 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8778 Accuracy 0.7035\n",
            "Epoch 2 Batch 50 Loss 0.8791 Accuracy 0.7011\n",
            "Epoch 2 Batch 100 Loss 0.8841 Accuracy 0.6996\n",
            "Epoch 2 Batch 150 Loss 0.8861 Accuracy 0.6989\n",
            "Epoch 2 Loss 0.8864 Accuracy 0.6988\n",
            "Time taken for 1 epoch: 78.58 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8495 Accuracy 0.7125\n",
            "Epoch 3 Batch 50 Loss 0.8615 Accuracy 0.7067\n",
            "Epoch 3 Batch 100 Loss 0.8645 Accuracy 0.7057\n",
            "Epoch 3 Batch 150 Loss 0.8666 Accuracy 0.7050\n",
            "Epoch 3 Loss 0.8674 Accuracy 0.7049\n",
            "Time taken for 1 epoch: 78.41 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8545 Accuracy 0.7082\n",
            "Epoch 4 Batch 50 Loss 0.8369 Accuracy 0.7148\n",
            "Epoch 4 Batch 100 Loss 0.8439 Accuracy 0.7125\n",
            "Epoch 4 Batch 150 Loss 0.8478 Accuracy 0.7111\n",
            "Epoch 4 Loss 0.8484 Accuracy 0.7110\n",
            "Time taken for 1 epoch: 78.42 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8088 Accuracy 0.7203\n",
            "Epoch 5 Batch 50 Loss 0.8247 Accuracy 0.7193\n",
            "Epoch 5 Batch 100 Loss 0.8267 Accuracy 0.7185\n",
            "Epoch 5 Batch 150 Loss 0.8290 Accuracy 0.7176\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-level-gen/ckpt-5\n",
            "Epoch 5 Loss 0.8298 Accuracy 0.7174\n",
            "Time taken for 1 epoch: 78.93 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8047 Accuracy 0.7234\n",
            "Epoch 6 Batch 50 Loss 0.7991 Accuracy 0.7267\n",
            "Epoch 6 Batch 100 Loss 0.8030 Accuracy 0.7260\n",
            "Epoch 6 Batch 150 Loss 0.8040 Accuracy 0.7254\n",
            "Epoch 6 Loss 0.8043 Accuracy 0.7254\n",
            "Time taken for 1 epoch: 78.57 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7788 Accuracy 0.7315\n",
            "Epoch 7 Batch 50 Loss 0.7710 Accuracy 0.7355\n",
            "Epoch 7 Batch 100 Loss 0.7780 Accuracy 0.7339\n",
            "Epoch 7 Batch 150 Loss 0.7804 Accuracy 0.7331\n",
            "Epoch 7 Loss 0.7808 Accuracy 0.7330\n",
            "Time taken for 1 epoch: 78.52 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7370 Accuracy 0.7441\n",
            "Epoch 8 Batch 50 Loss 0.7493 Accuracy 0.7429\n",
            "Epoch 8 Batch 100 Loss 0.7552 Accuracy 0.7412\n",
            "Epoch 8 Batch 150 Loss 0.7576 Accuracy 0.7406\n",
            "Epoch 8 Loss 0.7580 Accuracy 0.7406\n",
            "Time taken for 1 epoch: 78.46 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.7051 Accuracy 0.7569\n",
            "Epoch 9 Batch 50 Loss 0.7270 Accuracy 0.7510\n",
            "Epoch 9 Batch 100 Loss 0.7315 Accuracy 0.7493\n",
            "Epoch 9 Batch 150 Loss 0.7348 Accuracy 0.7480\n",
            "Epoch 9 Loss 0.7348 Accuracy 0.7479\n",
            "Time taken for 1 epoch: 78.47 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6910 Accuracy 0.7590\n",
            "Epoch 10 Batch 50 Loss 0.7035 Accuracy 0.7583\n",
            "Epoch 10 Batch 100 Loss 0.7089 Accuracy 0.7562\n",
            "Epoch 10 Batch 150 Loss 0.7130 Accuracy 0.7549\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-level-gen/ckpt-6\n",
            "Epoch 10 Loss 0.7139 Accuracy 0.7547\n",
            "Time taken for 1 epoch: 79.07 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "TODO change this :)\n",
        "\n",
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"\"\n",
        "    tokenized_result = []\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
        "\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"$\"]:\n",
        "            result += \"\\n\"\n",
        "        if result.count(\"$\") == 3:\n",
        "            \n",
        "            return result, tokenized_result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyjldo4Tl1MW"
      },
      "source": [
        "def clean_detokenized(x):\n",
        "  x = ''.join(x.split('\\n'))\n",
        "  x = re.sub(r'\\b \\b', '', x)\n",
        "  x = re.sub(r'| \\b', '', x)\n",
        "  x = re.sub(r'\\b |', '', x)\n",
        "  x = re.sub(r'\\|', '', x)\n",
        "  x = re.sub(r'\\$', ' $', x)\n",
        "  x = re.sub(r'[ ]+', ' ', x)\n",
        "  return x"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNRJrlybrWor"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DrSWbh0GnY3K",
        "outputId": "2098aaf8-f05c-4da3-c04d-6edb7edabe2e"
      },
      "source": [
        "raw_target_text.split('\\n')[6]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' |Ahi |quan|to a |dir |qual |e|ra è |co|sa |du|ra           '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GyFRuM4am86W",
        "outputId": "0448c028-06ad-4298-903f-2a6ddc002e56"
      },
      "source": [
        "clean_detokenized(target_tokenizer.sequences_to_texts([target_tercets[0]])[0])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'^ Ahi quanto a dir qual era è cosa dura $ ^ esta selva selvaggia e aspra e forte $ ^ che nel pensier rinova la paura! $'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtSP1D-nroO2"
      },
      "source": [
        "## Feeding the encoder the last output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "44fb4da9-ccdf-40d3-979e-78cb66523076"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"^\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"^\"]]\n",
        "\n",
        "generated_text, _ = generate_greedy(encoder_input, decoder_input)\n",
        "print(clean_detokenized(generated_text))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "che mi fu per esser tutto quanto fori, $ ^ perché non so li occhi miei si disira, $ ^ e potreva che sola terra fami. $ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj6V5d0Fdn16",
        "outputId": "f3358c58-f090-4c14-f05a-f70490c7b053"
      },
      "source": [
        "\n",
        "tokenized_output = input_tokenizer.texts_to_sequences(clean_detokenized(generated_text))\n",
        "generated_text, _ = generate_greedy(tokenized_generated, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| c h e   | m i   | p a r | l a | r e   e   | s o | n o   i n | t e n | d e r   | f u i , $ \n",
            "^ | t a l   | m i   | f e | c e   | s ì ,   | c h e   | p e r   | l o   | s t r a | d i | t o $ \n",
            "^ | t u t | t o   | m i   | p a r | l a r   | s o | l a   | t u a   | v i | s t a , $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uj0IR3Yrrsp"
      },
      "source": [
        "## Feeding the decoder the last output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S3tJwuws8_D"
      },
      "source": [
        "def clean_detokenized(x):\n",
        "  x = ''.join(x.split('\\n'))\n",
        "  x = re.sub(r'\\b \\b', '', x)\n",
        "  x = re.sub(r'| \\b', '', x)\n",
        "  x = re.sub(r'\\b |', '', x)\n",
        "  x = re.sub(r'\\$', '$', x)\n",
        "  x = re.sub(r'\\^ ', '^', x)\n",
        "  x = re.sub(r'[ ]+', ' ', x)\n",
        "  return x"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWaTHqckrtwA",
        "outputId": "19e36bea-c212-453d-8080-cd6dbdd3e1f0"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"^\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"^\"]]\n",
        "\n",
        "generated_text, tokenized_generated = generate_greedy(encoder_input, decoder_input)\n",
        "print(clean_detokenized(generated_text))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|che |mi |fu |per |es|ser |tut|to |quan|to |fo|ri, $ ^ |per|ché |non |so |li oc|chi |miei |si |di|si|ra, $ ^ |e |po|tre|va |che |so|la |ter|ra |fa|mi. $ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur2IdjJ1tINC"
      },
      "source": [
        "x = clean_detokenized(generated_text).split('$')[1:]\n",
        "x = list(map(lambda x: x.strip(), x))\n",
        "x = list(filter(lambda x: x != '', x))\n",
        "x = list(map(lambda x: x + '$' , x))\n",
        "x = ''.join(x)\n",
        "x = target_tokenizer.texts_to_sequences([x])[0]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEogvHoDrwK8",
        "outputId": "20e8c923-9829-47c5-d881-53bd2a764887"
      },
      "source": [
        "generated_text, tokenized_generated = generate_greedy(encoder_input, x)\n",
        "print(generated_text)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^ | E   | q u e | s t a   | v i r | t ù   | c h i a | m a   i n   | m e z | z a   | f r e | s t a , $ \n",
            "^ | c o | m e   | f o | r a $ \n",
            "^ | t e   | s t a | t a | t o r o ,   | t a   | t a | t o r a   | t o r a   | t o , $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5LRTshv0uG"
      },
      "source": [
        "Abbiamo provato due modi per generare:\n",
        "1. Dare all'encoder in input una terzina e ottenere la terzina successiva (come abbiamo allenato la rete a fare fondamentalmente), poi passare la terzina generata sempre all'encoder per ottenere la successiva e così via.\n",
        "1. Dare all'encoder in input uno start symbol e al decoder gli ultimi due versi della terzina generata. Il risultato dovrebbe tenere in considerazione esclusivamente il verso che ne esce fuori (TODO modificare generate greedy in modo tale che restituisca esclusivamente il next verse).\n",
        "1. TODO next provare a dare qualcosa all'encoder e al decoder contemporaneamente (es. contesto di generazione per il decoder generato dall'encoder?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    output_ = []\n",
        "\n",
        "    terces = 0\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        pred = tf.random.categorical(predictions, num_samples=1)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "            result += \"\\n\"\n",
        "\n",
        "        if result.count(\"$\") == 3:\n",
        "            terces += 1\n",
        "\n",
        "        if terces == 3:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "117c4c41-1b95-4506-f0d0-e8a0d75bef80"
      },
      "source": [
        "encoder_input = [tokenizer.word_index[\"^\"]]\n",
        "decoder_input = [tokenizer.word_index[\"^\"]]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| s o | v r a   | l e   | s t e l | l e   | s t e l | l e   | d e l   | s u o   | v e r | b o , $ \n",
            "^ | p e r   | c h e   | l a   | v i | v a   | l u | c e   | c o n | v i e n   | c a | r e . $ \n",
            "^ | E   | q u e | s t o   | s i   | f e | c e   | c o n | v i e n   | c h e   | v a n | n o $ \n",
            "^ ^ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNngr1tM2PgW",
        "outputId": "52e96f58-3659-4635-af78-b096a50d0561"
      },
      "source": [
        "sentence = \"ciao\"\n",
        "encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "decoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| v e   | g i à   | m a i   | n o n   | f u   | m a i   | n é   | r i | s t r e t | t a . $ \n",
            "^ | O r   | s a i   | t u   | d i e | t r o ,   e   | n o n   | t i   | p a r | l a | v a | r o : $ \n",
            "^ | p e r   | c h e   | l e   | s t e l | l e   | c h e   ’ n   | s u   | l a   | p r o | p r i a $ \n",
            "^ | \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}