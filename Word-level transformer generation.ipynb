{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import load_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT",
    "tags": []
   },
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "input_file = \"data/divina_textonly.txt\"\n",
    "target_file = \"data/divina_syll_textonly.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input_text, input_text, input_tokenizer = load_verses(\n",
    "    input_file, char_level=False, pad=False\n",
    ")\n",
    "raw_target_text, target_text, target_tokenizer = load_verses(\n",
    "    target_file, char_level=True, pad=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "ACAEUyITY1dY",
    "outputId": "26daf544-3700-428a-c752-b10e0bbd6d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text: 558637 characters\n",
      "Length of target text: 873431 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input text: {} characters\".format(len(raw_input_text)))\n",
    "print(\"Length of target text: {} characters\".format(len(raw_target_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gradient": {},
    "id": "l1G45yR9Y1da"
   },
   "outputs": [],
   "source": [
    "input_vocab = sorted(set(input_tokenizer.word_index.keys()))\n",
    "target_vocab = sorted(set(target_tokenizer.word_index.keys()))\n",
    "\n",
    "# + 1 to account for padding token \"0\"\n",
    "input_vocab_size = len(input_vocab) + 1\n",
    "target_vocab_size = len(target_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "p-w27LhpY1db",
    "outputId": "d4061a9c-09a0-46c1-b0a0-501d897ab0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 20749\n",
      "Target vocab size: 81\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
    "print(\"Target vocab size: {}\".format(target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tercets = []\n",
    "target_tercets = []\n",
    "\n",
    "for line in range(len(input_text) - 6):\n",
    "    input_tercets.append(list(chain(*input_text[line : line + 3])))\n",
    "    target_tercets.append(list(chain(*target_text[line + 3 : line + 6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_tercets, padding=\"post\"\n",
    ")\n",
    "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_tercets, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    padded_input, padded_target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {},
    "id": "tZWLq7g3Y1dl"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
    "    BUFFER_SIZE\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {},
    "id": "nSE2Rh-_qzo7"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/word-level-gen\"\n",
    "\n",
    "transformer_trainer = TransformerTrainer(\n",
    "    transformer, checkpoint_save_path=checkpoint_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_trainer.train(dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz4YwsF04YEI"
   },
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O44l1saVuebS"
   },
   "source": [
    "TODO change this :)\n",
    "\n",
    "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
    "\n",
    "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
    "\n",
    "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
    "\n",
    "The translation stops when the end symbol is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "gradient": {},
    "id": "BSNaKtSkvxcJ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, max_length=200):\n",
    "\n",
    "    encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "\n",
    "    output = tf.convert_to_tensor([tokenizer.word_index[\"^\"]])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    result = \"\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
    "            axis=-1,\n",
    "        )\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
    "\n",
    "            break\n",
    "\n",
    "    # output.shape (1, tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "1-UvZOCbkK2d"
   },
   "outputs": [],
   "source": [
    "def print_translation(sentence, result, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {result}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R26smp7BkSZu",
    "outputId": "f28066fd-dc7b-4b35-885f-0fad4ec483d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : ^E come l’aere, quand’ è ben pïorno,$\n",
      "Prediction     : | c h e   | l a   | m i a   | v i | s t a   | m i   | f é   | p a | r e a   | m a | l e $ \n",
      "Ground truth   : |E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\n"
     ]
    }
   ],
   "source": [
    "sentence = \"^E come l’aere, quand’ è ben pïorno,$\"\n",
    "ground_truth = \"|E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\"\n",
    "\n",
    "\n",
    "translated_text = translate(sentence)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "akQ7PnRmidiS"
   },
   "outputs": [],
   "source": [
    "def generate_greedy(encoder_input, decoder_input):\n",
    "\n",
    "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    result = \"\"\n",
    "\n",
    "    for i in range(200):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
    "            axis=-1,\n",
    "        )\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
    "            result += \"\\n\"\n",
    "        if result.count(\"$\") == 3:\n",
    "            return result\n",
    "\n",
    "    # output.shape (1, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8Qv4LFhkdJK",
    "outputId": "3285a1fb-689a-437e-fd57-2e8e38e43411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| c h e   | l a   | m i a   | v i | s t a   | m i   | f é   | p a | r e a   | m a | l e $ \n",
      "^ | c h e   | l ’   a | b i | t o   | d e   | l ’   a r | g o | m e n | t o   | s e | g n o . $ \n",
      "^ | E   | q u e l | l a   | c h e   | p r o | p r i e | n e   i l   | c a | l o r   | m i o $ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input = [tokenizer.word_index[\"^\"]]\n",
    "decoder_input = [tokenizer.word_index[\"^\"]]\n",
    "\n",
    "generated_text = generate_greedy(encoder_input, decoder_input)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "ij2lr385ystg"
   },
   "outputs": [],
   "source": [
    "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
    "\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    output_ = []\n",
    "\n",
    "    terces = 0\n",
    "\n",
    "    for i in range(200):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
    "\n",
    "        predictions /= temperature\n",
    "        predictions = np.squeeze(predictions, axis=0)\n",
    "        indices = np.squeeze(indices, axis=0)\n",
    "        indices = np.squeeze(indices, axis=0)\n",
    "        pred = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        predicted_id = indices[predicted_id]\n",
    "\n",
    "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
    "            result += \"\\n\"\n",
    "\n",
    "        if result.count(\"$\") == 3:\n",
    "            terces += 1\n",
    "\n",
    "        if terces == 3:\n",
    "            return result\n",
    "\n",
    "    # output.shape (1, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_jjxe1az9K6",
    "outputId": "117c4c41-1b95-4506-f0d0-e8a0d75bef80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| s o | v r a   | l e   | s t e l | l e   | s t e l | l e   | d e l   | s u o   | v e r | b o , $ \n",
      "^ | p e r   | c h e   | l a   | v i | v a   | l u | c e   | c o n | v i e n   | c a | r e . $ \n",
      "^ | E   | q u e | s t o   | s i   | f e | c e   | c o n | v i e n   | c h e   | v a n | n o $ \n",
      "^ ^ \n"
     ]
    }
   ],
   "source": [
    "encoder_input = [tokenizer.word_index[\"^\"]]\n",
    "decoder_input = [tokenizer.word_index[\"^\"]]\n",
    "\n",
    "generated_text = generate_topk(encoder_input, decoder_input)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNngr1tM2PgW",
    "outputId": "52e96f58-3659-4635-af78-b096a50d0561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| v e   | g i à   | m a i   | n o n   | f u   | m a i   | n é   | r i | s t r e t | t a . $ \n",
      "^ | O r   | s a i   | t u   | d i e | t r o ,   e   | n o n   | t i   | p a r | l a | v a | r o : $ \n",
      "^ | p e r   | c h e   | l e   | s t e l | l e   | c h e   ’ n   | s u   | l a   | p r o | p r i a $ \n",
      "^ | \n"
     ]
    }
   ],
   "source": [
    "sentence = \"ciao\"\n",
    "encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "decoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "\n",
    "generated_text = generate_topk(encoder_input, decoder_input)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Comedy transformers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
