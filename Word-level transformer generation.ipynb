{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-level transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcDazsVbL_tS",
        "outputId": "eeb8d750-043d-4e12-eb95-d4bfb6d7d167"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTxmMPGM04O",
        "outputId": "d3567f6c-bc2b-4ab6-af77-df386f705ad7"
      },
      "source": [
        "!tar zxvf deepcomedy.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deepcomedy/\n",
            "deepcomedy/util/\n",
            "deepcomedy/util/predicate.py\n",
            "deepcomedy/util/__init__.py\n",
            "deepcomedy/util/.ipynb_checkpoints/\n",
            "deepcomedy/util/.ipynb_checkpoints/predicate-checkpoint.py\n",
            "deepcomedy/models/\n",
            "deepcomedy/models/layers.py\n",
            "deepcomedy/models/transformer.py\n",
            "deepcomedy/models/__pycache__/\n",
            "deepcomedy/models/__pycache__/layers.cpython-37.pyc\n",
            "deepcomedy/models/__pycache__/__init__.cpython-37.pyc\n",
            "deepcomedy/models/__pycache__/transformer.cpython-37.pyc\n",
            "deepcomedy/models/__init__.py\n",
            "deepcomedy/models/.ipynb_checkpoints/\n",
            "deepcomedy/models/.ipynb_checkpoints/transformer-checkpoint.py\n",
            "deepcomedy/preprocessing.py\n",
            "deepcomedy/__pycache__/\n",
            "deepcomedy/__pycache__/__init__.cpython-37.pyc\n",
            "deepcomedy/__pycache__/preprocessing.cpython-37.pyc\n",
            "deepcomedy/__init__.py\n",
            "deepcomedy/.ipynb_checkpoints/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQw0NL5L_tl"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUtq8DQkL_tm"
      },
      "source": [
        "raw_input_text, input_text, input_tokenizer = load_verses(\n",
        "    input_file, char_level=False, pad=False\n",
        ")\n",
        "raw_target_text, target_text, target_tokenizer = load_verses(\n",
        "    target_file, char_level=True, pad=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "a4480a72-7635-4c7b-a833-a88f29c21a60"
      },
      "source": [
        "print(\"Length of input text: {} characters\".format(len(raw_input_text)))\n",
        "print(\"Length of target text: {} characters\".format(len(raw_target_text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input text: 558637 characters\n",
            "Length of target text: 873431 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "l1G45yR9Y1da"
      },
      "source": [
        "input_vocab = sorted(set(input_tokenizer.word_index.keys()))\n",
        "target_vocab = sorted(set(target_tokenizer.word_index.keys()))\n",
        "\n",
        "# + 1 to account for padding token \"0\"\n",
        "input_vocab_size = len(input_vocab) + 1\n",
        "target_vocab_size = len(target_vocab) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "2abd931b-d7f3-4cc7-8cf3-ca3464489684"
      },
      "source": [
        "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
        "print(\"Target vocab size: {}\".format(target_vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 20750\n",
            "Target vocab size: 82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKv92yAL_t8"
      },
      "source": [
        "input_tercets = []\n",
        "target_tercets = []\n",
        "\n",
        "for line in range(len(input_text) - 6):\n",
        "    input_tercets.append(list(chain(*input_text[line : line + 3])))\n",
        "    target_tercets.append(list(chain(*target_text[line + 3 : line + 6])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEbLJmP4L_t-"
      },
      "source": [
        "padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_tercets, padding=\"post\"\n",
        ")\n",
        "padded_target = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_tercets, padding=\"post\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjAxjw_8L_uB"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input, padded_target\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_length_targ, max_length_inp = target_train.shape[1], input_train.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yMOh-7-L_uD"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/word-level-gen\"\n",
        "\n",
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=checkpoint_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaR03YUNL_uF",
        "outputId": "7eb354d9-09bb-422c-8a3d-41d1fbcf8c5d"
      },
      "source": [
        "transformer_trainer.train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.2230 Accuracy 0.5936\n",
            "Epoch 1 Batch 50 Loss 1.2353 Accuracy 0.5884\n",
            "Epoch 1 Batch 100 Loss 1.2293 Accuracy 0.5897\n",
            "Epoch 1 Batch 150 Loss 1.2243 Accuracy 0.5912\n",
            "Epoch 1 Loss 1.2232 Accuracy 0.5918\n",
            "Time taken for 1 epoch: 78.66 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1958 Accuracy 0.6066\n",
            "Epoch 2 Batch 50 Loss 1.1797 Accuracy 0.6057\n",
            "Epoch 2 Batch 100 Loss 1.1809 Accuracy 0.6055\n",
            "Epoch 2 Batch 150 Loss 1.1785 Accuracy 0.6061\n",
            "Epoch 2 Loss 1.1772 Accuracy 0.6065\n",
            "Time taken for 1 epoch: 78.29 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1368 Accuracy 0.6160\n",
            "Epoch 3 Batch 50 Loss 1.1392 Accuracy 0.6181\n",
            "Epoch 3 Batch 100 Loss 1.1391 Accuracy 0.6186\n",
            "Epoch 3 Batch 150 Loss 1.1370 Accuracy 0.6193\n",
            "Epoch 3 Loss 1.1357 Accuracy 0.6198\n",
            "Time taken for 1 epoch: 78.32 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.1276 Accuracy 0.6225\n",
            "Epoch 4 Batch 50 Loss 1.1006 Accuracy 0.6318\n",
            "Epoch 4 Batch 100 Loss 1.1012 Accuracy 0.6308\n",
            "Epoch 4 Batch 150 Loss 1.0991 Accuracy 0.6314\n",
            "Epoch 4 Loss 1.0987 Accuracy 0.6314\n",
            "Time taken for 1 epoch: 78.29 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.0506 Accuracy 0.6504\n",
            "Epoch 5 Batch 50 Loss 1.0663 Accuracy 0.6420\n",
            "Epoch 5 Batch 100 Loss 1.0648 Accuracy 0.6420\n",
            "Epoch 5 Batch 150 Loss 1.0651 Accuracy 0.6420\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-level-gen/ckpt-3\n",
            "Epoch 5 Loss 1.0654 Accuracy 0.6419\n",
            "Time taken for 1 epoch: 78.81 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0208 Accuracy 0.6580\n",
            "Epoch 6 Batch 50 Loss 1.0365 Accuracy 0.6519\n",
            "Epoch 6 Batch 100 Loss 1.0348 Accuracy 0.6517\n",
            "Epoch 6 Batch 150 Loss 1.0337 Accuracy 0.6521\n",
            "Epoch 6 Loss 1.0334 Accuracy 0.6522\n",
            "Time taken for 1 epoch: 78.50 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0126 Accuracy 0.6569\n",
            "Epoch 7 Batch 50 Loss 1.0025 Accuracy 0.6620\n",
            "Epoch 7 Batch 100 Loss 1.0045 Accuracy 0.6613\n",
            "Epoch 7 Batch 150 Loss 1.0038 Accuracy 0.6616\n",
            "Epoch 7 Loss 1.0040 Accuracy 0.6617\n",
            "Time taken for 1 epoch: 78.35 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.9853 Accuracy 0.6690\n",
            "Epoch 8 Batch 50 Loss 0.9756 Accuracy 0.6701\n",
            "Epoch 8 Batch 100 Loss 0.9772 Accuracy 0.6698\n",
            "Epoch 8 Batch 150 Loss 0.9775 Accuracy 0.6695\n",
            "Epoch 8 Loss 0.9776 Accuracy 0.6695\n",
            "Time taken for 1 epoch: 78.39 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9575 Accuracy 0.6764\n",
            "Epoch 9 Batch 50 Loss 0.9480 Accuracy 0.6794\n",
            "Epoch 9 Batch 100 Loss 0.9531 Accuracy 0.6774\n",
            "Epoch 9 Batch 150 Loss 0.9534 Accuracy 0.6775\n",
            "Epoch 9 Loss 0.9531 Accuracy 0.6774\n",
            "Time taken for 1 epoch: 78.30 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.9204 Accuracy 0.6862\n",
            "Epoch 10 Batch 50 Loss 0.9262 Accuracy 0.6862\n",
            "Epoch 10 Batch 100 Loss 0.9285 Accuracy 0.6858\n",
            "Epoch 10 Batch 150 Loss 0.9288 Accuracy 0.6857\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-level-gen/ckpt-4\n",
            "Epoch 10 Loss 0.9288 Accuracy 0.6857\n",
            "Time taken for 1 epoch: 78.81 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "TODO change this :)\n",
        "\n",
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"\"\n",
        "    tokenized_result = []\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
        "\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == target_tokenizer.word_index[\"$\"]:\n",
        "            result += \"\\n\"\n",
        "        if result.count(\"$\") == 3:\n",
        "            return result, tokenized_result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "01f04c24-6069-4f2c-f63f-95990031c288"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"^\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"^\"]]\n",
        "\n",
        "generated_text, tokenized_generated = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| c h e   | p e r   | l o   | s o m | m o   | p r i | m o   | c h e   | s ’   a c | c e n | d e , $ \n",
            "^ | p e r   | c h e   | s i   | r i | t r a | v a   | c o n | t e n | t a   | r o | t a . $ \n",
            "^ | E   | q u e | s t a   | r i | v o l | t a   | c h e   | p e r   | l e   | s c o | g l i e $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj6V5d0Fdn16",
        "outputId": "477174ad-932d-4b9a-bb09-e6410048493b"
      },
      "source": [
        "generated_text, tokenized_generated = generate_greedy(tokenized_generated, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| p e r   | l ’   a l | t a   | p r i | m a   | c h e   | t u t | t e   | l e   | s t e l | l e . $ \n",
            "^ | D i   | q u e | s t a   | p r i | m a   | c h e   | t u t | t e   | l ’   a | n i | m a $ \n",
            "^ | p e r   | l o   | s p i | r i | t o   | d e   | l ’   a l | t r o   | s i   | r e | t r o , $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqGdoIcxcZZK"
      },
      "source": [
        "generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6TR6pVpJbQln",
        "outputId": "9195bbcd-ad3a-4a50-fb7b-abd67c07fb35"
      },
      "source": [
        "re.sub('', '', generated_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'| c h e   | p e r   | l o   | s o m | m o   | p r i | m o   | c h e   | s ’   a c | c e n | d e , $ \\n^ | p e r   | c h e   | s i   | r i | t r a | v a   | c o n | t e n | t a   | r o | t a . $ \\n^ | E   | q u e | s t a   | r i | v o l | t a   | c h e   | p e r   | l e   | s c o | g l i e $ \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    output_ = []\n",
        "\n",
        "    terces = 0\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        pred = tf.random.categorical(predictions, num_samples=1)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "            result += \"\\n\"\n",
        "\n",
        "        if result.count(\"$\") == 3:\n",
        "            terces += 1\n",
        "\n",
        "        if terces == 3:\n",
        "            return result\n",
        "\n",
        "    # output.shape (1, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "117c4c41-1b95-4506-f0d0-e8a0d75bef80"
      },
      "source": [
        "encoder_input = [tokenizer.word_index[\"^\"]]\n",
        "decoder_input = [tokenizer.word_index[\"^\"]]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| s o | v r a   | l e   | s t e l | l e   | s t e l | l e   | d e l   | s u o   | v e r | b o , $ \n",
            "^ | p e r   | c h e   | l a   | v i | v a   | l u | c e   | c o n | v i e n   | c a | r e . $ \n",
            "^ | E   | q u e | s t o   | s i   | f e | c e   | c o n | v i e n   | c h e   | v a n | n o $ \n",
            "^ ^ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNngr1tM2PgW",
        "outputId": "52e96f58-3659-4635-af78-b096a50d0561"
      },
      "source": [
        "sentence = \"ciao\"\n",
        "encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "decoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| v e   | g i à   | m a i   | n o n   | f u   | m a i   | n é   | r i | s t r e t | t a . $ \n",
            "^ | O r   | s a i   | t u   | d i e | t r o ,   e   | n o n   | t i   | p a r | l a | v a | r o : $ \n",
            "^ | p e r   | c h e   | l e   | s t e l | l e   | c h e   ’ n   | s u   | l a   | p r o | p r i a $ \n",
            "^ | \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}