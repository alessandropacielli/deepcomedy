{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-level transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRKGBdAtVIfZ"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/wordlevel\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "8RuMqNB4ujuT"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "ec599638-b051-4a9e-d281-2270840e11f2"
      },
      "source": [
        "input_text_raw = open(input_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(\"Length of text: {} characters\".format(len(input_text_raw)))\n",
        "target_text_raw = open(target_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(\"Length of text: {} characters\".format(len(target_text_raw)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 578077 characters\n",
            "Length of text: 892871 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "RDRNHkB-VIfc"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
        "    \"\"\"\n",
        "    return [\"^ \" + line.strip() + \" $\" for line in text.split(\"\\n\") if line.strip() != \"\"]\n",
        "\n",
        "input_text_prepr = preprocess(input_text_raw)\n",
        "target_text_prepr = preprocess(target_text_raw)\n",
        "target_text_prepr = list(map(lambda x: re.sub('\\|', ' ', x), target_text_prepr))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "vgFtCA9wVIfe"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False, oov_token='<UNK>')\n",
        "input_tokenizer.fit_on_texts(input_text_prepr)\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", split=' ', lower=False, oov_token='<UNK>')\n",
        "target_tokenizer.fit_on_texts(target_text_prepr)\n",
        "\n",
        "input_text_lines_enc = input_tokenizer.texts_to_sequences(input_text_prepr)\n",
        "target_text_lines_enc = target_tokenizer.texts_to_sequences(target_text_prepr)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "qu_z0ozzVIff"
      },
      "source": [
        "input_vocab_size = len(input_tokenizer.word_index)\n",
        "target_vocab_size = len(target_tokenizer.word_index)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "e7207250-15fd-4f4e-f9df-6909d58d87a3"
      },
      "source": [
        "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
        "print(\"Target vocab size target: {}\".format(target_vocab_size))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 20750\n",
            "Target vocab size target: 4191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bfzvcNvT-y"
      },
      "source": [
        "Padding is required in order to have a non-ragged tensor to feed to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "MBOh9LQeY1dg"
      },
      "source": [
        "def pad(x):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding=\"post\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "9zV0xz48Y1dh"
      },
      "source": [
        "input_text = pad(input_text_lines_enc)\n",
        "target_text = pad(target_text_lines_enc)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "FE_LiRvcVIfi"
      },
      "source": [
        "input_text_ = []\n",
        "target_text_ = []\n",
        "\n",
        "for line_number in range(0, len(text) - 4):\n",
        "    \n",
        "    input_verses = []\n",
        "    target_verses = []\n",
        "    \n",
        "    for i in range(4):\n",
        "        input_verses += list(input_text[line_number + i])\n",
        "        target_verses += list(target_text[line_number + i])\n",
        "    \n",
        "    input_text_.append(input_verses)\n",
        "    target_text_.append(target_verses)\n",
        "    \n",
        "input_text_ = np.array(input_text_)\n",
        "target_text_ = np.array(target_text_)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7xGxZmlPY1dk"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    input_text_, target_text_\n",
        "    )"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IN8x175vimK"
      },
      "source": [
        "The dataset is created by grouping the lines in batches and by shuffling them.\n",
        "\n",
        "Each input's line is in correspondence with its target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "input_vocab_size = (\n",
        "    len(input_tokenizer.word_index) + 1\n",
        ")  # the +1 is added to take into account the id 0 of the padding\n",
        "\n",
        "target_vocab_size = (\n",
        "    len(target_tokenizer.word_index) + 1\n",
        ")\n",
        "\n",
        "input_max_length = input_text.shape[1]\n",
        "target_max_length = target_text.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHNAazT5Rs_"
      },
      "source": [
        "We define the positional encoding to add to the embedding.\n",
        "\n",
        "This allows to take into account the order of the characters in the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "f200V0QnkBBS"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OvnGjGhvkD9R"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500eU4tu6n-g"
      },
      "source": [
        "We define two masks: \n",
        "\n",
        "one is used to mask the padding added to the sequences in the preprocessing step; \n",
        "\n",
        "the other one is used to mask the positions following the current one and not predicted yet;\n",
        "\n",
        "The first mask is used from both the encoder and the decoder, while the last mask is used only in the self-attention of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OVwx6Y4Tku1V"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "3p1-yIYimnvB"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "R-Q4J7EzfuLH"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxzWROrTM9ib"
      },
      "source": [
        "The *scaled_dot_product_attention* gets the attention weights by applying the softmax to the rescaled dot product between the query matrix and the key matrix, while the output is obtained by multiplying the value matrix for those attention weights.\n",
        "\n",
        "The query, key and value matrices are built by multiplying the embedding matrix with the query, key and value weight matrices, which initially are randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "RoFZK1S3mtI5"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += mask * -1e9\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1\n",
        "    )  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlA3inNYQO89"
      },
      "source": [
        "The multi-headed attention allows to improve the performance of the attention mechanism by working with multiple sets of query, key and value weight matrices.\n",
        "\n",
        "These heads work in parallel and process at the same time all the lines of each batch.\n",
        "\n",
        "At the end, the results of all the attention heads are concatenated and multiplied by an additional weight matrix, to adjust the dimension before passing through the final *point_wise_feed_forward_network*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "_UpdBWkVnK02"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        scaled_attention = tf.transpose(\n",
        "            scaled_attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(\n",
        "            scaled_attention, (batch_size, -1, self.d_model)\n",
        "        )  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LkMP7DDAok4y"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Dense(dff, activation=\"relu\"),  # (batch_size, seq_len, dff)\n",
        "            tf.keras.layers.Dense(d_model),  # (batch_size, seq_len, d_model)\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO6yyZPWUON7"
      },
      "source": [
        "Each encoder is constituted by a multi-headed self-attention layer and by a final feed forward layer. \n",
        "\n",
        "Both sub-layers have a residual connection around them and are followed by a layer-normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "Dat64C18otwC"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(\n",
        "            out1 + ffn_output\n",
        "        )  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZslNuSZOKp"
      },
      "source": [
        "The decoder equals the encoder, a part from the fact that it contains a slightly different self-attention layer and an additional attention layer.\n",
        "\n",
        "Indeed, the decoder is characterized by a self-attention layer which focuses only on earlier positions in its input sequence, not looking at the positions which have not been predicted yet.\n",
        "\n",
        "What's more the decoder is also characterized by an attention layer which obtains its key and value matrices from the output of the encoder, while the query matrix is obtained from the output of the previous self-attention in the decoder.\n",
        "\n",
        "The encoder-decoder attention helps the decoder to focus on appropriate positions in the input sequence of the encoder during the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7Vp44lQepI_P"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(\n",
        "            x, x, x, look_ahead_mask\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(\n",
        "            ffn_output + out2\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx_DyJiybrOr"
      },
      "source": [
        "The encoding component is a stack of encoders and the decoding component is a stack of decoders of the same number.\n",
        "\n",
        "At the beginning, in the encoding, each input character is turned into a vector using an embedding algorithm and adding the positional encoding to it.\n",
        "\n",
        "This happens only in the bottom-most encoder, while the following encoders take the output of the encoder which is directly below.\n",
        "\n",
        "The same for the decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "awl9kiESpWBh"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        input_vocab_size,\n",
        "        maximum_position_encoding,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "47tQAEMwpnUj"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        target_vocab_size,\n",
        "        maximum_position_encoding,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](\n",
        "                x, enc_output, training, look_ahead_mask, padding_mask\n",
        "            )\n",
        "\n",
        "            attention_weights[f\"decoder_layer{i+1}_block1\"] = block1\n",
        "            attention_weights[f\"decoder_layer{i+1}_block2\"] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TosN_TpKk1eN"
      },
      "source": [
        "In the transformer, the output of the encoding is passed to the stack of decoders and the output of the decoding is projected by a feed forward network into a vector of logits of dimension equal to the one of the target's vocabulary.\n",
        "\n",
        "Obviously this is done for each character of each line of each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "qCNKKsQ-p99k"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        input_vocab_size,\n",
        "        target_vocab_size,\n",
        "        pe_input,\n",
        "        pe_target,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate\n",
        "        )\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(\n",
        "        self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "    ):\n",
        "\n",
        "        enc_output = self.encoder(\n",
        "            inp, training, enc_padding_mask\n",
        "        )  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        final_output = self.final_layer(\n",
        "            dec_output\n",
        "        )  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LrdL396xqOL4"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "JFCVQIDjqQHv"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "1R9MlFs0qc5U"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcmLAk1Ut8vG"
      },
      "source": [
        "The loss is calculated using Sparse Categorical Crossentropy and the loss of the padding is masked.\n",
        "\n",
        "The same is done for the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "TBAaRBPsqkuo"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "31R26t9wqlLD"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "SkVkWvL7qoYu"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.Mean(name=\"train_accuracy\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "5UE3cWGVqvnS"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64MejGX2Lbk1",
        "outputId": "bf5d9338-4a31-480d-c157-6236e2e445c4"
      },
      "source": [
        "!tar zxvf checkpoints.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints/\n",
            "checkpoints/wordlevel/\n",
            "checkpoints/wordlevel/ckpt-6.index\n",
            "checkpoints/wordlevel/ckpt-6.data-00000-of-00001\n",
            "checkpoints/wordlevel/checkpoint\n",
            "checkpoints/wordlevel/ckpt-3.index\n",
            "checkpoints/wordlevel/ckpt-4.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-2.index\n",
            "checkpoints/wordlevel/ckpt-5.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-4.index\n",
            "checkpoints/wordlevel/ckpt-3.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-2.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-5.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVGgP8Su9MH"
      },
      "source": [
        "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "n_VPs6ZOva15"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "1ce0FAOivleY",
        "outputId": "f094a3e3-6bdc-4785-ab47-4da21f1a1755"
      },
      "source": [
        "EPOCHS = 80\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}\"\n",
        "            )\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f\"Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}\"\n",
        "    )\n",
        "\n",
        "    print(f\"Time taken for 1 epoch: {time.time() - start:.2f} secs\\n\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1461 Accuracy 0.2636\n",
            "Epoch 1 Batch 50 Loss 4.1494 Accuracy 0.2694\n",
            "Epoch 1 Batch 100 Loss 4.1132 Accuracy 0.2729\n",
            "Epoch 1 Batch 150 Loss 4.0796 Accuracy 0.2757\n",
            "Epoch 1 Loss 4.0708 Accuracy 0.2763\n",
            "Time taken for 1 epoch: 23.66 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.9519 Accuracy 0.2881\n",
            "Epoch 2 Batch 50 Loss 3.8830 Accuracy 0.2921\n",
            "Epoch 2 Batch 100 Loss 3.8543 Accuracy 0.2954\n",
            "Epoch 2 Batch 150 Loss 3.8269 Accuracy 0.2979\n",
            "Epoch 2 Loss 3.8177 Accuracy 0.2988\n",
            "Time taken for 1 epoch: 23.62 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.6411 Accuracy 0.3181\n",
            "Epoch 3 Batch 50 Loss 3.5999 Accuracy 0.3257\n",
            "Epoch 3 Batch 100 Loss 3.5549 Accuracy 0.3342\n",
            "Epoch 3 Batch 150 Loss 3.4921 Accuracy 0.3462\n",
            "Epoch 3 Loss 3.4692 Accuracy 0.3509\n",
            "Time taken for 1 epoch: 23.59 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.1155 Accuracy 0.4159\n",
            "Epoch 4 Batch 50 Loss 3.0134 Accuracy 0.4356\n",
            "Epoch 4 Batch 100 Loss 2.8980 Accuracy 0.4565\n",
            "Epoch 4 Batch 150 Loss 2.7884 Accuracy 0.4767\n",
            "Epoch 4 Loss 2.7556 Accuracy 0.4827\n",
            "Time taken for 1 epoch: 23.62 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2393 Accuracy 0.5762\n",
            "Epoch 5 Batch 50 Loss 2.1946 Accuracy 0.5799\n",
            "Epoch 5 Batch 100 Loss 2.0990 Accuracy 0.5967\n",
            "Epoch 5 Batch 150 Loss 2.0173 Accuracy 0.6114\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/wordlevel/ckpt-1\n",
            "Epoch 5 Loss 1.9948 Accuracy 0.6153\n",
            "Time taken for 1 epoch: 24.06 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.5589 Accuracy 0.6887\n",
            "Epoch 6 Batch 50 Loss 1.5200 Accuracy 0.6953\n",
            "Epoch 6 Batch 100 Loss 1.4590 Accuracy 0.7062\n",
            "Epoch 6 Batch 150 Loss 1.3961 Accuracy 0.7178\n",
            "Epoch 6 Loss 1.3783 Accuracy 0.7209\n",
            "Time taken for 1 epoch: 23.66 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1081 Accuracy 0.7639\n",
            "Epoch 7 Batch 50 Loss 1.0124 Accuracy 0.7875\n",
            "Epoch 7 Batch 100 Loss 0.9720 Accuracy 0.7954\n",
            "Epoch 7 Batch 150 Loss 0.9289 Accuracy 0.8034\n",
            "Epoch 7 Loss 0.9167 Accuracy 0.8056\n",
            "Time taken for 1 epoch: 23.61 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6944 Accuracy 0.8496\n",
            "Epoch 8 Batch 50 Loss 0.6513 Accuracy 0.8576\n",
            "Epoch 8 Batch 100 Loss 0.6321 Accuracy 0.8609\n",
            "Epoch 8 Batch 150 Loss 0.6057 Accuracy 0.8665\n",
            "Epoch 8 Loss 0.5999 Accuracy 0.8677\n",
            "Time taken for 1 epoch: 23.30 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4977 Accuracy 0.8871\n",
            "Epoch 9 Batch 50 Loss 0.4180 Accuracy 0.9064\n",
            "Epoch 9 Batch 100 Loss 0.4125 Accuracy 0.9069\n",
            "Epoch 9 Batch 150 Loss 0.4004 Accuracy 0.9091\n",
            "Epoch 9 Loss 0.3965 Accuracy 0.9099\n",
            "Time taken for 1 epoch: 23.32 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3021 Accuracy 0.9293\n",
            "Epoch 10 Batch 50 Loss 0.2854 Accuracy 0.9339\n",
            "Epoch 10 Batch 100 Loss 0.2814 Accuracy 0.9346\n",
            "Epoch 10 Batch 150 Loss 0.2787 Accuracy 0.9349\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/wordlevel/ckpt-2\n",
            "Epoch 10 Loss 0.2776 Accuracy 0.9351\n",
            "Time taken for 1 epoch: 23.75 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2181 Accuracy 0.9491\n",
            "Epoch 11 Batch 50 Loss 0.2150 Accuracy 0.9491\n",
            "Epoch 11 Batch 100 Loss 0.2131 Accuracy 0.9491\n",
            "Epoch 11 Batch 150 Loss 0.2100 Accuracy 0.9495\n",
            "Epoch 11 Loss 0.2087 Accuracy 0.9496\n",
            "Time taken for 1 epoch: 23.31 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1513 Accuracy 0.9651\n",
            "Epoch 12 Batch 50 Loss 0.1666 Accuracy 0.9590\n",
            "Epoch 12 Batch 100 Loss 0.1719 Accuracy 0.9576\n",
            "Epoch 12 Batch 150 Loss 0.1732 Accuracy 0.9573\n",
            "Epoch 12 Loss 0.1736 Accuracy 0.9572\n",
            "Time taken for 1 epoch: 23.28 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1309 Accuracy 0.9667\n",
            "Epoch 13 Batch 50 Loss 0.1443 Accuracy 0.9635\n",
            "Epoch 13 Batch 100 Loss 0.1398 Accuracy 0.9647\n",
            "Epoch 13 Batch 150 Loss 0.1406 Accuracy 0.9645\n",
            "Epoch 13 Loss 0.1410 Accuracy 0.9644\n",
            "Time taken for 1 epoch: 23.26 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1332 Accuracy 0.9650\n",
            "Epoch 14 Batch 50 Loss 0.1229 Accuracy 0.9688\n",
            "Epoch 14 Batch 100 Loss 0.1214 Accuracy 0.9693\n",
            "Epoch 14 Batch 150 Loss 0.1204 Accuracy 0.9695\n",
            "Epoch 14 Loss 0.1207 Accuracy 0.9694\n",
            "Time taken for 1 epoch: 23.21 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1241 Accuracy 0.9645\n",
            "Epoch 15 Batch 50 Loss 0.1203 Accuracy 0.9691\n",
            "Epoch 15 Batch 100 Loss 0.1124 Accuracy 0.9710\n",
            "Epoch 15 Batch 150 Loss 0.1136 Accuracy 0.9708\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/wordlevel/ckpt-3\n",
            "Epoch 15 Loss 0.1133 Accuracy 0.9709\n",
            "Time taken for 1 epoch: 23.69 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1106 Accuracy 0.9731\n",
            "Epoch 16 Batch 50 Loss 0.0965 Accuracy 0.9750\n",
            "Epoch 16 Batch 100 Loss 0.0979 Accuracy 0.9748\n",
            "Epoch 16 Batch 150 Loss 0.1024 Accuracy 0.9738\n",
            "Epoch 16 Loss 0.1022 Accuracy 0.9738\n",
            "Time taken for 1 epoch: 23.24 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0806 Accuracy 0.9793\n",
            "Epoch 17 Batch 50 Loss 0.0881 Accuracy 0.9775\n",
            "Epoch 17 Batch 100 Loss 0.0917 Accuracy 0.9764\n",
            "Epoch 17 Batch 150 Loss 0.0931 Accuracy 0.9760\n",
            "Epoch 17 Loss 0.0937 Accuracy 0.9758\n",
            "Time taken for 1 epoch: 23.29 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0908 Accuracy 0.9752\n",
            "Epoch 18 Batch 50 Loss 0.0851 Accuracy 0.9779\n",
            "Epoch 18 Batch 100 Loss 0.0854 Accuracy 0.9780\n",
            "Epoch 18 Batch 150 Loss 0.0879 Accuracy 0.9774\n",
            "Epoch 18 Loss 0.0881 Accuracy 0.9774\n",
            "Time taken for 1 epoch: 23.28 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0658 Accuracy 0.9819\n",
            "Epoch 19 Batch 50 Loss 0.0778 Accuracy 0.9802\n",
            "Epoch 19 Batch 100 Loss 0.0759 Accuracy 0.9806\n",
            "Epoch 19 Batch 150 Loss 0.0790 Accuracy 0.9798\n",
            "Epoch 19 Loss 0.0796 Accuracy 0.9796\n",
            "Time taken for 1 epoch: 23.29 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0546 Accuracy 0.9837\n",
            "Epoch 20 Batch 50 Loss 0.0762 Accuracy 0.9808\n",
            "Epoch 20 Batch 100 Loss 0.0748 Accuracy 0.9809\n",
            "Epoch 20 Batch 150 Loss 0.0740 Accuracy 0.9810\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/wordlevel/ckpt-4\n",
            "Epoch 20 Loss 0.0738 Accuracy 0.9811\n",
            "Time taken for 1 epoch: 23.87 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0556 Accuracy 0.9838\n",
            "Epoch 21 Batch 50 Loss 0.0714 Accuracy 0.9821\n",
            "Epoch 21 Batch 100 Loss 0.0684 Accuracy 0.9828\n",
            "Epoch 21 Batch 150 Loss 0.0671 Accuracy 0.9831\n",
            "Epoch 21 Loss 0.0666 Accuracy 0.9833\n",
            "Time taken for 1 epoch: 23.33 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0545 Accuracy 0.9867\n",
            "Epoch 22 Batch 50 Loss 0.0565 Accuracy 0.9855\n",
            "Epoch 22 Batch 100 Loss 0.0584 Accuracy 0.9853\n",
            "Epoch 22 Batch 150 Loss 0.0565 Accuracy 0.9857\n",
            "Epoch 22 Loss 0.0560 Accuracy 0.9859\n",
            "Time taken for 1 epoch: 23.30 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0579 Accuracy 0.9833\n",
            "Epoch 23 Batch 50 Loss 0.0516 Accuracy 0.9871\n",
            "Epoch 23 Batch 100 Loss 0.0511 Accuracy 0.9872\n",
            "Epoch 23 Batch 150 Loss 0.0517 Accuracy 0.9871\n",
            "Epoch 23 Loss 0.0519 Accuracy 0.9871\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0557 Accuracy 0.9860\n",
            "Epoch 24 Batch 50 Loss 0.0461 Accuracy 0.9886\n",
            "Epoch 24 Batch 100 Loss 0.0453 Accuracy 0.9887\n",
            "Epoch 24 Batch 150 Loss 0.0452 Accuracy 0.9888\n",
            "Epoch 24 Loss 0.0454 Accuracy 0.9888\n",
            "Time taken for 1 epoch: 23.29 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0598 Accuracy 0.9879\n",
            "Epoch 25 Batch 50 Loss 0.0455 Accuracy 0.9892\n",
            "Epoch 25 Batch 100 Loss 0.0444 Accuracy 0.9892\n",
            "Epoch 25 Batch 150 Loss 0.0418 Accuracy 0.9897\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/wordlevel/ckpt-5\n",
            "Epoch 25 Loss 0.0420 Accuracy 0.9897\n",
            "Time taken for 1 epoch: 23.76 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0403 Accuracy 0.9894\n",
            "Epoch 26 Batch 50 Loss 0.0391 Accuracy 0.9906\n",
            "Epoch 26 Batch 100 Loss 0.0369 Accuracy 0.9910\n",
            "Epoch 26 Batch 150 Loss 0.0378 Accuracy 0.9909\n",
            "Epoch 26 Loss 0.0381 Accuracy 0.9907\n",
            "Time taken for 1 epoch: 23.27 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0350 Accuracy 0.9921\n",
            "Epoch 27 Batch 50 Loss 0.0350 Accuracy 0.9918\n",
            "Epoch 27 Batch 100 Loss 0.0340 Accuracy 0.9919\n",
            "Epoch 27 Batch 150 Loss 0.0349 Accuracy 0.9916\n",
            "Epoch 27 Loss 0.0349 Accuracy 0.9916\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0362 Accuracy 0.9905\n",
            "Epoch 28 Batch 50 Loss 0.0309 Accuracy 0.9925\n",
            "Epoch 28 Batch 100 Loss 0.0309 Accuracy 0.9924\n",
            "Epoch 28 Batch 150 Loss 0.0318 Accuracy 0.9923\n",
            "Epoch 28 Loss 0.0318 Accuracy 0.9923\n",
            "Time taken for 1 epoch: 23.30 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0337 Accuracy 0.9927\n",
            "Epoch 29 Batch 50 Loss 0.0320 Accuracy 0.9923\n",
            "Epoch 29 Batch 100 Loss 0.0309 Accuracy 0.9926\n",
            "Epoch 29 Batch 150 Loss 0.0303 Accuracy 0.9927\n",
            "Epoch 29 Loss 0.0302 Accuracy 0.9927\n",
            "Time taken for 1 epoch: 23.31 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0249 Accuracy 0.9935\n",
            "Epoch 30 Batch 50 Loss 0.0300 Accuracy 0.9930\n",
            "Epoch 30 Batch 100 Loss 0.0288 Accuracy 0.9933\n",
            "Epoch 30 Batch 150 Loss 0.0276 Accuracy 0.9935\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/wordlevel/ckpt-6\n",
            "Epoch 30 Loss 0.0277 Accuracy 0.9935\n",
            "Time taken for 1 epoch: 23.76 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0353 Accuracy 0.9927\n",
            "Epoch 31 Batch 50 Loss 0.0249 Accuracy 0.9940\n",
            "Epoch 31 Batch 100 Loss 0.0269 Accuracy 0.9936\n",
            "Epoch 31 Batch 150 Loss 0.0267 Accuracy 0.9936\n",
            "Epoch 31 Loss 0.0267 Accuracy 0.9936\n",
            "Time taken for 1 epoch: 23.28 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0414 Accuracy 0.9918\n",
            "Epoch 32 Batch 50 Loss 0.0251 Accuracy 0.9939\n",
            "Epoch 32 Batch 100 Loss 0.0237 Accuracy 0.9942\n",
            "Epoch 32 Batch 150 Loss 0.0244 Accuracy 0.9942\n",
            "Epoch 32 Loss 0.0243 Accuracy 0.9942\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0292 Accuracy 0.9930\n",
            "Epoch 33 Batch 50 Loss 0.0228 Accuracy 0.9946\n",
            "Epoch 33 Batch 100 Loss 0.0228 Accuracy 0.9946\n",
            "Epoch 33 Batch 150 Loss 0.0228 Accuracy 0.9946\n",
            "Epoch 33 Loss 0.0228 Accuracy 0.9946\n",
            "Time taken for 1 epoch: 23.38 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0224 Accuracy 0.9938\n",
            "Epoch 34 Batch 50 Loss 0.0239 Accuracy 0.9944\n",
            "Epoch 34 Batch 100 Loss 0.0217 Accuracy 0.9948\n",
            "Epoch 34 Batch 150 Loss 0.0212 Accuracy 0.9949\n",
            "Epoch 34 Loss 0.0214 Accuracy 0.9948\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0127 Accuracy 0.9972\n",
            "Epoch 35 Batch 50 Loss 0.0219 Accuracy 0.9948\n",
            "Epoch 35 Batch 100 Loss 0.0219 Accuracy 0.9949\n",
            "Epoch 35 Batch 150 Loss 0.0217 Accuracy 0.9949\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/wordlevel/ckpt-7\n",
            "Epoch 35 Loss 0.0215 Accuracy 0.9950\n",
            "Time taken for 1 epoch: 23.79 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0237 Accuracy 0.9958\n",
            "Epoch 36 Batch 50 Loss 0.0185 Accuracy 0.9958\n",
            "Epoch 36 Batch 100 Loss 0.0185 Accuracy 0.9957\n",
            "Epoch 36 Batch 150 Loss 0.0183 Accuracy 0.9957\n",
            "Epoch 36 Loss 0.0182 Accuracy 0.9957\n",
            "Time taken for 1 epoch: 23.33 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0130 Accuracy 0.9972\n",
            "Epoch 37 Batch 50 Loss 0.0175 Accuracy 0.9961\n",
            "Epoch 37 Batch 100 Loss 0.0173 Accuracy 0.9959\n",
            "Epoch 37 Batch 150 Loss 0.0183 Accuracy 0.9957\n",
            "Epoch 37 Loss 0.0186 Accuracy 0.9956\n",
            "Time taken for 1 epoch: 23.22 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0211 Accuracy 0.9952\n",
            "Epoch 38 Batch 50 Loss 0.0221 Accuracy 0.9954\n",
            "Epoch 38 Batch 100 Loss 0.0197 Accuracy 0.9957\n",
            "Epoch 38 Batch 150 Loss 0.0186 Accuracy 0.9959\n",
            "Epoch 38 Loss 0.0186 Accuracy 0.9959\n",
            "Time taken for 1 epoch: 23.31 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0177 Accuracy 0.9961\n",
            "Epoch 39 Batch 50 Loss 0.0176 Accuracy 0.9960\n",
            "Epoch 39 Batch 100 Loss 0.0165 Accuracy 0.9962\n",
            "Epoch 39 Batch 150 Loss 0.0169 Accuracy 0.9961\n",
            "Epoch 39 Loss 0.0169 Accuracy 0.9961\n",
            "Time taken for 1 epoch: 23.34 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0230 Accuracy 0.9944\n",
            "Epoch 40 Batch 50 Loss 0.0153 Accuracy 0.9964\n",
            "Epoch 40 Batch 100 Loss 0.0150 Accuracy 0.9965\n",
            "Epoch 40 Batch 150 Loss 0.0148 Accuracy 0.9965\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/wordlevel/ckpt-8\n",
            "Epoch 40 Loss 0.0150 Accuracy 0.9964\n",
            "Time taken for 1 epoch: 23.82 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0189 Accuracy 0.9958\n",
            "Epoch 41 Batch 50 Loss 0.0167 Accuracy 0.9963\n",
            "Epoch 41 Batch 100 Loss 0.0158 Accuracy 0.9964\n",
            "Epoch 41 Batch 150 Loss 0.0155 Accuracy 0.9964\n",
            "Epoch 41 Loss 0.0152 Accuracy 0.9965\n",
            "Time taken for 1 epoch: 23.37 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0103 Accuracy 0.9978\n",
            "Epoch 42 Batch 50 Loss 0.0154 Accuracy 0.9966\n",
            "Epoch 42 Batch 100 Loss 0.0146 Accuracy 0.9966\n",
            "Epoch 42 Batch 150 Loss 0.0145 Accuracy 0.9966\n",
            "Epoch 42 Loss 0.0145 Accuracy 0.9966\n",
            "Time taken for 1 epoch: 23.32 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0141 Accuracy 0.9967\n",
            "Epoch 43 Batch 50 Loss 0.0160 Accuracy 0.9965\n",
            "Epoch 43 Batch 100 Loss 0.0150 Accuracy 0.9967\n",
            "Epoch 43 Batch 150 Loss 0.0140 Accuracy 0.9968\n",
            "Epoch 43 Loss 0.0140 Accuracy 0.9968\n",
            "Time taken for 1 epoch: 23.30 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0150 Accuracy 0.9961\n",
            "Epoch 44 Batch 50 Loss 0.0158 Accuracy 0.9964\n",
            "Epoch 44 Batch 100 Loss 0.0148 Accuracy 0.9966\n",
            "Epoch 44 Batch 150 Loss 0.0144 Accuracy 0.9967\n",
            "Epoch 44 Loss 0.0143 Accuracy 0.9966\n",
            "Time taken for 1 epoch: 23.24 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0098 Accuracy 0.9978\n",
            "Epoch 45 Batch 50 Loss 0.0126 Accuracy 0.9972\n",
            "Epoch 45 Batch 100 Loss 0.0130 Accuracy 0.9971\n",
            "Epoch 45 Batch 150 Loss 0.0132 Accuracy 0.9970\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/wordlevel/ckpt-9\n",
            "Epoch 45 Loss 0.0133 Accuracy 0.9969\n",
            "Time taken for 1 epoch: 23.76 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0074 Accuracy 0.9980\n",
            "Epoch 46 Batch 50 Loss 0.0110 Accuracy 0.9974\n",
            "Epoch 46 Batch 100 Loss 0.0115 Accuracy 0.9973\n",
            "Epoch 46 Batch 150 Loss 0.0113 Accuracy 0.9973\n",
            "Epoch 46 Loss 0.0114 Accuracy 0.9973\n",
            "Time taken for 1 epoch: 23.20 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0136 Accuracy 0.9958\n",
            "Epoch 47 Batch 50 Loss 0.0130 Accuracy 0.9971\n",
            "Epoch 47 Batch 100 Loss 0.0126 Accuracy 0.9972\n",
            "Epoch 47 Batch 150 Loss 0.0124 Accuracy 0.9972\n",
            "Epoch 47 Loss 0.0124 Accuracy 0.9972\n",
            "Time taken for 1 epoch: 23.27 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0143 Accuracy 0.9955\n",
            "Epoch 48 Batch 50 Loss 0.0108 Accuracy 0.9975\n",
            "Epoch 48 Batch 100 Loss 0.0107 Accuracy 0.9975\n",
            "Epoch 48 Batch 150 Loss 0.0115 Accuracy 0.9973\n",
            "Epoch 48 Loss 0.0116 Accuracy 0.9973\n",
            "Time taken for 1 epoch: 23.24 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0153 Accuracy 0.9966\n",
            "Epoch 49 Batch 50 Loss 0.0105 Accuracy 0.9976\n",
            "Epoch 49 Batch 100 Loss 0.0106 Accuracy 0.9976\n",
            "Epoch 49 Batch 150 Loss 0.0104 Accuracy 0.9976\n",
            "Epoch 49 Loss 0.0105 Accuracy 0.9976\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0098 Accuracy 0.9986\n",
            "Epoch 50 Batch 50 Loss 0.0117 Accuracy 0.9974\n",
            "Epoch 50 Batch 100 Loss 0.0114 Accuracy 0.9975\n",
            "Epoch 50 Batch 150 Loss 0.0113 Accuracy 0.9974\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/wordlevel/ckpt-10\n",
            "Epoch 50 Loss 0.0112 Accuracy 0.9974\n",
            "Time taken for 1 epoch: 23.69 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0102 Accuracy 0.9983\n",
            "Epoch 51 Batch 50 Loss 0.0109 Accuracy 0.9976\n",
            "Epoch 51 Batch 100 Loss 0.0106 Accuracy 0.9976\n",
            "Epoch 51 Batch 150 Loss 0.0103 Accuracy 0.9976\n",
            "Epoch 51 Loss 0.0103 Accuracy 0.9976\n",
            "Time taken for 1 epoch: 23.36 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0074 Accuracy 0.9983\n",
            "Epoch 52 Batch 50 Loss 0.0090 Accuracy 0.9979\n",
            "Epoch 52 Batch 100 Loss 0.0099 Accuracy 0.9978\n",
            "Epoch 52 Batch 150 Loss 0.0097 Accuracy 0.9978\n",
            "Epoch 52 Loss 0.0097 Accuracy 0.9978\n",
            "Time taken for 1 epoch: 23.20 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0147 Accuracy 0.9967\n",
            "Epoch 53 Batch 50 Loss 0.0104 Accuracy 0.9976\n",
            "Epoch 53 Batch 100 Loss 0.0103 Accuracy 0.9976\n",
            "Epoch 53 Batch 150 Loss 0.0096 Accuracy 0.9977\n",
            "Epoch 53 Loss 0.0096 Accuracy 0.9977\n",
            "Time taken for 1 epoch: 23.27 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0105 Accuracy 0.9975\n",
            "Epoch 54 Batch 50 Loss 0.0090 Accuracy 0.9979\n",
            "Epoch 54 Batch 100 Loss 0.0088 Accuracy 0.9980\n",
            "Epoch 54 Batch 150 Loss 0.0086 Accuracy 0.9980\n",
            "Epoch 54 Loss 0.0088 Accuracy 0.9979\n",
            "Time taken for 1 epoch: 23.26 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0071 Accuracy 0.9980\n",
            "Epoch 55 Batch 50 Loss 0.0089 Accuracy 0.9981\n",
            "Epoch 55 Batch 100 Loss 0.0090 Accuracy 0.9980\n",
            "Epoch 55 Batch 150 Loss 0.0090 Accuracy 0.9980\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/wordlevel/ckpt-11\n",
            "Epoch 55 Loss 0.0092 Accuracy 0.9979\n",
            "Time taken for 1 epoch: 23.71 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0059 Accuracy 0.9980\n",
            "Epoch 56 Batch 50 Loss 0.0104 Accuracy 0.9978\n",
            "Epoch 56 Batch 100 Loss 0.0094 Accuracy 0.9980\n",
            "Epoch 56 Batch 150 Loss 0.0093 Accuracy 0.9979\n",
            "Epoch 56 Loss 0.0093 Accuracy 0.9979\n",
            "Time taken for 1 epoch: 23.26 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0103 Accuracy 0.9983\n",
            "Epoch 57 Batch 50 Loss 0.0089 Accuracy 0.9982\n",
            "Epoch 57 Batch 100 Loss 0.0091 Accuracy 0.9981\n",
            "Epoch 57 Batch 150 Loss 0.0093 Accuracy 0.9980\n",
            "Epoch 57 Loss 0.0093 Accuracy 0.9980\n",
            "Time taken for 1 epoch: 23.27 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0123 Accuracy 0.9972\n",
            "Epoch 58 Batch 50 Loss 0.0086 Accuracy 0.9981\n",
            "Epoch 58 Batch 100 Loss 0.0103 Accuracy 0.9978\n",
            "Epoch 58 Batch 150 Loss 0.0098 Accuracy 0.9980\n",
            "Epoch 58 Loss 0.0096 Accuracy 0.9980\n",
            "Time taken for 1 epoch: 23.17 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0057 Accuracy 0.9980\n",
            "Epoch 59 Batch 50 Loss 0.0064 Accuracy 0.9985\n",
            "Epoch 59 Batch 100 Loss 0.0077 Accuracy 0.9983\n",
            "Epoch 59 Batch 150 Loss 0.0078 Accuracy 0.9983\n",
            "Epoch 59 Loss 0.0078 Accuracy 0.9983\n",
            "Time taken for 1 epoch: 23.27 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0145 Accuracy 0.9975\n",
            "Epoch 60 Batch 50 Loss 0.0083 Accuracy 0.9981\n",
            "Epoch 60 Batch 100 Loss 0.0082 Accuracy 0.9981\n",
            "Epoch 60 Batch 150 Loss 0.0080 Accuracy 0.9982\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/wordlevel/ckpt-12\n",
            "Epoch 60 Loss 0.0079 Accuracy 0.9982\n",
            "Time taken for 1 epoch: 23.75 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0032 Accuracy 0.9992\n",
            "Epoch 61 Batch 50 Loss 0.0058 Accuracy 0.9987\n",
            "Epoch 61 Batch 100 Loss 0.0057 Accuracy 0.9987\n",
            "Epoch 61 Batch 150 Loss 0.0063 Accuracy 0.9985\n",
            "Epoch 61 Loss 0.0065 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 23.24 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0108 Accuracy 0.9983\n",
            "Epoch 62 Batch 50 Loss 0.0082 Accuracy 0.9980\n",
            "Epoch 62 Batch 100 Loss 0.0081 Accuracy 0.9980\n",
            "Epoch 62 Batch 150 Loss 0.0078 Accuracy 0.9981\n",
            "Epoch 62 Loss 0.0078 Accuracy 0.9982\n",
            "Time taken for 1 epoch: 23.36 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0049 Accuracy 0.9989\n",
            "Epoch 63 Batch 50 Loss 0.0067 Accuracy 0.9983\n",
            "Epoch 63 Batch 100 Loss 0.0070 Accuracy 0.9984\n",
            "Epoch 63 Batch 150 Loss 0.0066 Accuracy 0.9984\n",
            "Epoch 63 Loss 0.0067 Accuracy 0.9984\n",
            "Time taken for 1 epoch: 23.21 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0132 Accuracy 0.9986\n",
            "Epoch 64 Batch 50 Loss 0.0073 Accuracy 0.9984\n",
            "Epoch 64 Batch 100 Loss 0.0068 Accuracy 0.9985\n",
            "Epoch 64 Batch 150 Loss 0.0070 Accuracy 0.9985\n",
            "Epoch 64 Loss 0.0068 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 23.21 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0088 Accuracy 0.9986\n",
            "Epoch 65 Batch 50 Loss 0.0071 Accuracy 0.9983\n",
            "Epoch 65 Batch 100 Loss 0.0065 Accuracy 0.9985\n",
            "Epoch 65 Batch 150 Loss 0.0066 Accuracy 0.9986\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/wordlevel/ckpt-13\n",
            "Epoch 65 Loss 0.0065 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 23.72 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0047 Accuracy 0.9989\n",
            "Epoch 66 Batch 50 Loss 0.0060 Accuracy 0.9987\n",
            "Epoch 66 Batch 100 Loss 0.0063 Accuracy 0.9986\n",
            "Epoch 66 Batch 150 Loss 0.0061 Accuracy 0.9986\n",
            "Epoch 66 Loss 0.0062 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 23.24 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0059 Accuracy 0.9992\n",
            "Epoch 67 Batch 50 Loss 0.0066 Accuracy 0.9985\n",
            "Epoch 67 Batch 100 Loss 0.0064 Accuracy 0.9985\n",
            "Epoch 67 Batch 150 Loss 0.0063 Accuracy 0.9985\n",
            "Epoch 67 Loss 0.0063 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 23.23 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0035 Accuracy 0.9989\n",
            "Epoch 68 Batch 50 Loss 0.0067 Accuracy 0.9985\n",
            "Epoch 68 Batch 100 Loss 0.0064 Accuracy 0.9985\n",
            "Epoch 68 Batch 150 Loss 0.0065 Accuracy 0.9986\n",
            "Epoch 68 Loss 0.0065 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 23.31 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0027 Accuracy 0.9992\n",
            "Epoch 69 Batch 50 Loss 0.0062 Accuracy 0.9986\n",
            "Epoch 69 Batch 100 Loss 0.0060 Accuracy 0.9987\n",
            "Epoch 69 Batch 150 Loss 0.0059 Accuracy 0.9987\n",
            "Epoch 69 Loss 0.0060 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 23.22 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0029 Accuracy 0.9994\n",
            "Epoch 70 Batch 50 Loss 0.0065 Accuracy 0.9985\n",
            "Epoch 70 Batch 100 Loss 0.0066 Accuracy 0.9985\n",
            "Epoch 70 Batch 150 Loss 0.0062 Accuracy 0.9985\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/wordlevel/ckpt-14\n",
            "Epoch 70 Loss 0.0062 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 23.76 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0080 Accuracy 0.9986\n",
            "Epoch 71 Batch 50 Loss 0.0061 Accuracy 0.9987\n",
            "Epoch 71 Batch 100 Loss 0.0061 Accuracy 0.9986\n",
            "Epoch 71 Batch 150 Loss 0.0059 Accuracy 0.9987\n",
            "Epoch 71 Loss 0.0058 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 23.21 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0035 Accuracy 0.9989\n",
            "Epoch 72 Batch 50 Loss 0.0050 Accuracy 0.9990\n",
            "Epoch 72 Batch 100 Loss 0.0058 Accuracy 0.9987\n",
            "Epoch 72 Batch 150 Loss 0.0057 Accuracy 0.9987\n",
            "Epoch 72 Loss 0.0057 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 23.13 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0027 Accuracy 0.9986\n",
            "Epoch 73 Batch 50 Loss 0.0046 Accuracy 0.9989\n",
            "Epoch 73 Batch 100 Loss 0.0050 Accuracy 0.9989\n",
            "Epoch 73 Batch 150 Loss 0.0055 Accuracy 0.9988\n",
            "Epoch 73 Loss 0.0056 Accuracy 0.9988\n",
            "Time taken for 1 epoch: 23.19 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0051 Accuracy 0.9975\n",
            "Epoch 74 Batch 50 Loss 0.0067 Accuracy 0.9986\n",
            "Epoch 74 Batch 100 Loss 0.0057 Accuracy 0.9987\n",
            "Epoch 74 Batch 150 Loss 0.0060 Accuracy 0.9987\n",
            "Epoch 74 Loss 0.0059 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 23.29 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0030 Accuracy 0.9989\n",
            "Epoch 75 Batch 50 Loss 0.0050 Accuracy 0.9990\n",
            "Epoch 75 Batch 100 Loss 0.0053 Accuracy 0.9989\n",
            "Epoch 75 Batch 150 Loss 0.0053 Accuracy 0.9989\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/wordlevel/ckpt-15\n",
            "Epoch 75 Loss 0.0054 Accuracy 0.9989\n",
            "Time taken for 1 epoch: 23.74 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0118 Accuracy 0.9980\n",
            "Epoch 76 Batch 50 Loss 0.0062 Accuracy 0.9988\n",
            "Epoch 76 Batch 100 Loss 0.0063 Accuracy 0.9987\n",
            "Epoch 76 Batch 150 Loss 0.0061 Accuracy 0.9987\n",
            "Epoch 76 Loss 0.0060 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 23.18 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0123 Accuracy 0.9978\n",
            "Epoch 77 Batch 50 Loss 0.0052 Accuracy 0.9990\n",
            "Epoch 77 Batch 100 Loss 0.0048 Accuracy 0.9990\n",
            "Epoch 77 Batch 150 Loss 0.0046 Accuracy 0.9990\n",
            "Epoch 77 Loss 0.0046 Accuracy 0.9990\n",
            "Time taken for 1 epoch: 23.21 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0126 Accuracy 0.9983\n",
            "Epoch 78 Batch 50 Loss 0.0054 Accuracy 0.9987\n",
            "Epoch 78 Batch 100 Loss 0.0052 Accuracy 0.9988\n",
            "Epoch 78 Batch 150 Loss 0.0050 Accuracy 0.9988\n",
            "Epoch 78 Loss 0.0052 Accuracy 0.9988\n",
            "Time taken for 1 epoch: 23.25 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0058 Accuracy 0.9989\n",
            "Epoch 79 Batch 50 Loss 0.0050 Accuracy 0.9988\n",
            "Epoch 79 Batch 100 Loss 0.0047 Accuracy 0.9990\n",
            "Epoch 79 Batch 150 Loss 0.0050 Accuracy 0.9989\n",
            "Epoch 79 Loss 0.0052 Accuracy 0.9989\n",
            "Time taken for 1 epoch: 23.17 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0039 Accuracy 0.9994\n",
            "Epoch 80 Batch 50 Loss 0.0050 Accuracy 0.9990\n",
            "Epoch 80 Batch 100 Loss 0.0049 Accuracy 0.9990\n",
            "Epoch 80 Batch 150 Loss 0.0047 Accuracy 0.9990\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/wordlevel/ckpt-16\n",
            "Epoch 80 Loss 0.0047 Accuracy 0.9990\n",
            "Time taken for 1 epoch: 23.72 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vbeVUzwtoTE",
        "outputId": "8381c219-b80d-4fb4-9de3-5365d60447a6"
      },
      "source": [
        "!tar chvfz checkpoints.tar.gz checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints/\n",
            "checkpoints/wordlevel/\n",
            "checkpoints/wordlevel/checkpoint\n",
            "checkpoints/wordlevel/ckpt-20.index\n",
            "checkpoints/wordlevel/ckpt-19.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-17.index\n",
            "checkpoints/wordlevel/ckpt-17.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-16.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-18.index\n",
            "checkpoints/wordlevel/ckpt-20.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-18.data-00000-of-00001\n",
            "checkpoints/wordlevel/ckpt-19.index\n",
            "checkpoints/wordlevel/ckpt-16.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh7X-rIaJHyl",
        "outputId": "d8b530ee-fe8b-4551-a273-493a51edbc1c"
      },
      "source": [
        "sentence = \"quanto disobediendo intese ir suso;\"\n",
        "encoder_input = preprocess(sentence)\n",
        "encoder_input = input_tokenizer.texts_to_sequences(encoder_input)\n",
        "encoder_input = pad(encoder_input)\n",
        "print(encoder_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    2    87 17042  3716   870  1319     3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "BSNaKtSkvxcJ"
      },
      "source": [
        "def evaluate(sentence, max_length=200):\n",
        "\n",
        "    #encoder_input = preprocess(sentence)\n",
        "    encoder_input = input_tokenizer.texts_to_sequences(sentence)\n",
        "    encoder_input = [x for l in encoder_input for x in l]\n",
        "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
        "    )\n",
        "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "\n",
        "    output = tf.convert_to_tensor([target_tokenizer.word_index[\"^\"]])\n",
        "    output = tf.expand_dims(output, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(600):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        if predicted_id == target_tokenizer.word_index[\"$\"]:\n",
        "          result += \"\\n\"\n",
        "        if result.count(\"$\") == 6:\n",
        "          break\n",
        "\n",
        "    # output.shape (1, tokens)\n",
        "\n",
        "    return result, attention_weights"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "PkCDxGdeyF9f"
      },
      "source": [
        "def print_translation(sentence, result, ground_truth):\n",
        "    print(f'{\"Input:\":15s}: {sentence}')\n",
        "    print(f'{\"Prediction\":15s}: {result}')\n",
        "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbKZbzeL000f",
        "outputId": "0e55e8d2-e26a-4ecc-cbb3-457dc87c7b69"
      },
      "source": [
        "sentence = \"^\"\n",
        "ground_truth = \"\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^\n",
            "Prediction     : sor ri pet to ro giu sto van do len te, $ \n",
            "$ \n",
            "$ \n",
            "$ \n",
            "$ \n",
            "$ \n",
            "\n",
            "Ground truth   : \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlPsnGGTX5Dq",
        "outputId": "5b9f3ab7-d37e-47e1-87ba-6a1e6ab1f993"
      },
      "source": [
        "sentence = \"quanto disobediendo intese ir suso;\"\n",
        "ground_truth = \"|quan|to |di|so|be|dien|do in|te|se ir |su|so;\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : quanto disobediendo intese ir suso;\n",
            "Prediction     : i ca de im mo di scen de o di i pas sa to. $ \n",
            "$ \n",
            "$ \n",
            "$ \n",
            "^ i mo strar si mo vie no e la ri ca sen do, $ \n",
            "^ o di mo vie no vi di ca tan do, e mo la dro e i di ste san do, $ \n",
            "\n",
            "Ground truth   : |quan|to |di|so|be|dien|do in|te|se ir |su|so;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "V33sFY3iyLLE",
        "outputId": "6c31a727-e315-455d-96a7-5fd5816a37e0"
      },
      "source": [
        "sentence = \"Buonasera a tutti\"\n",
        "ground_truth = \"\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : Buonasera a tutti\n",
            "Prediction     : e li ca no o di sde a Ri sto e a Ri col la $ \n",
            "$ \n",
            "$ \n",
            "^ i di rit ta i di sde gna ti a la e a di sde ca di ca ta i di chia me o ca ta i e la sci di sde e i e sti lo san do:“ In di ca tan do:“ In di ca ta a Si sto i ma nu ta $ \n",
            "^ e a di no; $ \n",
            "^ e a di no; $ \n",
            "\n",
            "Ground truth   : \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPLXjcQVriKf",
        "outputId": "771b0555-c5f9-4922-e5e0-296d5cd2c1db"
      },
      "source": [
        "sentence = \"Io non so ben ridir com’ i’ v’intrai,\"\n",
        "ground_truth = \"|Io |non |so |ben |ri|dir |com’ |i’ |v’ in|trai,\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : Io non so ben ridir com’ i’ v’intrai,\n",
            "Prediction     : o ca de u sci ta u sci do u sci ta u sci do $ \n",
            "$ \n",
            "$ \n",
            "$ \n",
            "^ ri no ’ ca de gli, ’ ca ta li ca de ab bar ca. $ \n",
            "^ ri guar dan do mi ca ta quel la i per so no sen ti. $ \n",
            "\n",
            "Ground truth   : |Io |non |so |ben |ri|dir |com’ |i’ |v’ in|trai,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7hFuQB_wD-z",
        "outputId": "2ac4e454-9ad8-4296-faf8-5f410b9a0055"
      },
      "source": [
        "sentence = \"ciao\"\n",
        "ground_truth = \"\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ciao\n",
            "Prediction     : i di sco per a o o i di sco per ta ci i ma $ \n",
            "^ i di sco no ra ta ti a o o o o o i di sco no a o o i di sco per ma ra ta no a o o o i di sco per la to i di sco per di sco per la to a o e i spe sen ti no i ma $ \n",
            "^ i di stan za re e sti lo ra ta re i di sco per di stan za re i di stan za re i di sco per la sci ta di stan za to i di ne i di stan na ti i di stan za re i di ne i di sco per e a o i di sco per di ste i di sco e a o i di stan za re i di stan za bi le ra i di no; o i di ne i di stan za ti i di ne i di sco per di sco no i di sco e a e a o i di sco no ra i di sco per sci ta no i di stan na ti i di stan za ri e a o i di stan za re i di stan za bi li ta no i di ne i di ne i di stan za re i di stan na sce i di sco e a o o o i di sco per sci o i di stan na ti i di sco e sti e a o i di sci ta no i di sci o a o i ma ra i ma ra ta no i ma ra bi le a i di sci o a o a o i di stan na ti i di stan za i ma ra ta no i di sco e sti ca i di stan na ti i di stan za i ma ra ta no i di stan za re i di stan za bi le a o i di sci ta no i di stan za le ra ta no i di stan za ti i di stan za ri e i di stan za ri e i di stan za to i di sco e a i spe ru i di stan za ti i di stan za to i di stan za le a i di stan za ti i di e i e i di stan za to i e i e i di e i e i di sci o i e i di sci o a i e i di sco e a i spi ri e a i di stan za re i di stan za re i di stan za re i di stan za le a i di stan za sci o o i di stan za re i di stan za re i di sco i di stan na ti i di stan na sce i di sco i di stan za bi li ta no i di stan na ti i di sci o i spi ri e a o i spi ri e a o a o a o a e a o i spe ru i di sco e i di sco e a o o i spi ri e a o o o o o i spe ru i spe ru i di stan za ti i spe ru i di stan za ti i spe ru i spe ru bi li a o o \n",
            "Ground truth   : \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}