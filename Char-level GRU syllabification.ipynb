{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Char-level GRU syllabification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RltDojL4d0tf"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "13294996-7c6a-44ab-b8b8-a555db987319"
      },
      "source": [
        "input_text_raw = open(input_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "target_text_raw = open(target_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(\"Length of input text: {} characters\".format(len(input_text_raw)))\n",
        "print(\"Length of target text: {} characters\".format(len(target_text_raw)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input text: 558637 characters\n",
            "Length of target text: 873431 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "l1G45yR9Y1da",
        "outputId": "78331e77-21ec-4598-a8cc-6ece2b86122e"
      },
      "source": [
        "input_vocab = sorted(set(input_text_raw))\n",
        "target_vocab = sorted(set(target_text_raw))\n",
        "input_vocab_size = len(input_vocab)\n",
        "target_vocab_size = len(target_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '«', '»', 'È', 'Ë', 'Ï', 'à', 'ä', 'è', 'é', 'ë', 'ì', 'ï', 'ò', 'ó', 'ö', 'ù', 'ü', '—', '‘', '’', '“', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "d5d0316c-b7dd-45d7-96f1-516add4aa95f"
      },
      "source": [
        "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
        "print(\"Target vocab size: {}\".format(target_vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 79\n",
            "Target vocab size: 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I2Y3wtQ7teq"
      },
      "source": [
        "The *preprocess* function adds the start and end symbols to each line and eliminates the empty ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "SUsvn6SqY1dd"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
        "    \"\"\"\n",
        "    return [\"^\" + line.strip() + \"$\" for line in text.split(\"\\n\") if line.strip() != \"\"]\n",
        "\n",
        "input_text_prepr = preprocess(input_text_raw)\n",
        "target_text_prepr = preprocess(target_text_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqkNvgp_d0tl"
      },
      "source": [
        "The tokenizer encodes each line into a tensor of char-indexes and for simplicity fits only on the target's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "ABb_2K6DY1de"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", char_level=True, lower=False)\n",
        "tokenizer.fit_on_texts(target_text_prepr)\n",
        "\n",
        "input_text_lines_enc = tokenizer.texts_to_sequences(input_text_prepr)\n",
        "target_text_lines_enc = tokenizer.texts_to_sequences(target_text_prepr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBeRajgd0tm"
      },
      "source": [
        "Padding is required in order to have a non-ragged tensor to feed to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "MBOh9LQeY1dg"
      },
      "source": [
        "def pad(x):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "9zV0xz48Y1dh"
      },
      "source": [
        "input_text = pad(input_text_lines_enc)\n",
        "target_text = pad(target_text_lines_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCRLEr63d0tp"
      },
      "source": [
        "## 2. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7xGxZmlPY1dk"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(input_text, target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfoNEMJ3-nWv"
      },
      "source": [
        "The dataset is created by grouping the lines in batches and by shuffling them.\n",
        "\n",
        "Each input's line is in correspondence with its target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_size = len(tokenizer.word_index) + 1 # the +1 is added to take into account the id 0 of the padding\n",
        "\n",
        "max_length_targ, max_length_inp = target_text.shape[1], input_text.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MpnDIrbAoHw"
      },
      "source": [
        "The encoder and decoder are constituted of an embedding layer, followed by a GRU.\n",
        "\n",
        "The decoder takes the final hidden state of the encoder as its initial hidden state and outputs logits of size equal to the one of the target's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QqdYfsxQ2VX"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_state=True)\n",
        "\n",
        "    def call(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if hidden is None:\n",
        "            hidden = self.gru.get_initial_state(x)\n",
        "\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        return output, state\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(         \n",
        "            self.dec_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer=\"glorot_uniform\",)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, enc_hidden):\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x, initial_state=enc_hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZM32bQPRS00",
        "outputId": "58fd7c73-c438-446d-b5e0-4c3955c27cc9"
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSH22jIoETMs"
      },
      "source": [
        "The loss is calculated using Sparse Categorical Crossentropy and the loss of the padding is masked.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "v9HRGDuGY1dn"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eZpCkLXZabl"
      },
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em0w__8gYiO4"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        _, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([tokenizer.word_index[\"^\"]] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwhkVtOWZGOH"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = None\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time()-start:.2f} sec\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxEjvoVLG3ZE"
      },
      "source": [
        "## 3. Translation and Attention Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FV5YkbPHDb5"
      },
      "source": [
        "We define the *evaluate* function to preprocess the sentence in input and to obtain the predicted ids of the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTjMRKjoZvPb"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((target_text.shape[1], input_text.shape[1]))\n",
        "\n",
        "    inputs = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding=\"post\")\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index[\"^\"]], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += tokenizer.index_word[predicted_id] + \" \"\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == \"$\":\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdkpT6B3rG62"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap=\"viridis\")\n",
        "\n",
        "    fontdict = {\"fontsize\": 14}\n",
        "\n",
        "    ax.set_xticklabels([\"\"] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhMAvmdurIsA"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print(\"Input:\", sentence)\n",
        "    print(\"Predicted translation:\", result)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')),:len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvcZiKlt-kwE",
        "outputId": "71d3c99b-93a6-4249-f871-ba62fd8dd1d3"
      },
      "source": [
        "translate(\"^ Nel mezzo del cammin di nostra vita $\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: ^ Nel mezzo del cammin di nostra vita $\n",
            "Predicted translation:   | N e l | r i   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}