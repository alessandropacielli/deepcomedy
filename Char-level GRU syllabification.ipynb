{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {},
    "id": "lsuXc5StY1dY"
   },
   "outputs": [],
   "source": [
    "input_file = \"data/divina_textonly.txt\"\n",
    "target_file = \"data/divina_syll_textonly.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "ACAEUyITY1dY",
    "outputId": "13294996-7c6a-44ab-b8b8-a555db987319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text: 558637 characters\n",
      "Length of target text: 873431 characters\n"
     ]
    }
   ],
   "source": [
    "input_text_raw = open(input_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "target_text_raw = open(target_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(\"Length of input text: {} characters\".format(len(input_text_raw)))\n",
    "print(\"Length of target text: {} characters\".format(len(target_text_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "l1G45yR9Y1da",
    "outputId": "78331e77-21ec-4598-a8cc-6ece2b86122e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '«', '»', 'È', 'Ë', 'Ï', 'à', 'ä', 'è', 'é', 'ë', 'ì', 'ï', 'ò', 'ó', 'ö', 'ù', 'ü', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "input_vocab = sorted(set(input_text_raw))\n",
    "target_vocab = sorted(set(target_text_raw))\n",
    "input_vocab_size = len(input_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "print(input_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "p-w27LhpY1db",
    "outputId": "d5d0316c-b7dd-45d7-96f1-516add4aa95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 79\n",
      "Target vocab size: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
    "print(\"Target vocab size: {}\".format(target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gradient": {},
    "id": "SUsvn6SqY1dd"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
    "    \"\"\"\n",
    "    return [\"^\" + line.strip() + \"$\" for line in text.split(\"\\n\") if line.strip() != \"\"]\n",
    "\n",
    "\n",
    "input_text_prepr = preprocess(input_text_raw)\n",
    "target_text_prepr = preprocess(target_text_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer encodes each line into a tensor of char-indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {},
    "id": "ABb_2K6DY1de"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", char_level=True, lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_text_prepr)\n",
    "\n",
    "input_text_lines_enc = tokenizer.texts_to_sequences(input_text_prepr)\n",
    "target_text_lines_enc = tokenizer.texts_to_sequences(target_text_prepr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding is required in order to have a non-ragged tensor to feed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {},
    "id": "MBOh9LQeY1dg"
   },
   "outputs": [],
   "source": [
    "def pad(x):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {},
    "id": "9zV0xz48Y1dh"
   },
   "outputs": [],
   "source": [
    "input_text = pad(input_text_lines_enc)\n",
    "target_text = pad(target_text_lines_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gradient": {},
    "id": "7xGxZmlPY1dk"
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    input_text, target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {},
    "id": "tZWLq7g3Y1dl"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length_targ, max_length_inp = target_text.shape[1], input_text.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
    "    BUFFER_SIZE\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZ9X_vtcNvWJ",
    "outputId": "6c8eda06-c982-47ff-dd90-7c4aaabb271a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 55), (64, 65)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-QqdYfsxQ2VX"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = self.gru.get_initial_state(x)\n",
    "\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, enc_hidden):\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, initial_state=enc_hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZM32bQPRS00",
    "outputId": "58fd7c73-c438-446d-b5e0-4c3955c27cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch)\n",
    "print(\"Encoder output shape: (batch size, sequence length, units)\", sample_output.shape)\n",
    "print(\"Encoder Hidden state shape: (batch size, units)\", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxzoZj-oXcHY",
    "outputId": "d2b1c1ed-d810-49c3-9b41-5a4ba9786109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 82)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden)\n",
    "\n",
    "print(\"Decoder output shape: (batch_size, vocab size)\", sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gradient": {},
    "id": "v9HRGDuGY1dn"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7eZpCkLXZabl"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "em0w__8gYiO4"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index[\"^\"]] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwhkVtOWZGOH",
    "outputId": "6bc5e115-9752-4598-a222-dc9e35fdc207"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = None\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}\")\n",
    "    print(f\"Time taken for 1 epoch {time.time()-start:.2f} sec\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qTjMRKjoZvPb"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((target_text.shape[1], input_text.shape[1]))\n",
    "\n",
    "    inputs = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [inputs], maxlen=max_length_inp, padding=\"post\"\n",
    "    )\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[\"^\"]], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            dec_input, dec_hidden, enc_out\n",
    "        )\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id] + \" \"\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == \"$\":\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PdkpT6B3rG62"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "\n",
    "    fontdict = {\"fontsize\": 14}\n",
    "\n",
    "    ax.set_xticklabels([\"\"] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lhMAvmdurIsA"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print(\"Input:\", sentence)\n",
    "    print(\"Predicted translation:\", result)\n",
    "\n",
    "    # attention_plot = attention_plot[:len(result.split(' ')),\n",
    "    #:len(sentence.split(' '))]\n",
    "    # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvcZiKlt-kwE",
    "outputId": "71d3c99b-93a6-4249-f871-ba62fd8dd1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ^ Nel mezzo del cammin di nostra vita $\n",
      "Predicted translation:   | N e l | r i   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v i | t a   | v \n"
     ]
    }
   ],
   "source": [
    "translate(\"^ Nel mezzo del cammin di nostra vita $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COVpHWEOEfJi",
    "outputId": "fd9333d7-94ef-4e4e-d854-860e6afc85ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ^ Io non so ben ridir com’ i’ v’intrai, $\n",
      "Predicted translation:   | I o   | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n o n | n \n"
     ]
    }
   ],
   "source": [
    "translate(\"^ Io non so ben ridir com’ i’ v’intrai, $\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
