{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Character-level transformer syllabification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv1r_3HL4Yv8",
        "outputId": "3cf549cf-5ffb-4294-e131-d59c4a34e27c"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import load_verses\n",
        "\n",
        "from deepcomedy.util.predicate import predicate\n",
        "from nlgpoetry.hyphenation import *"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT"
      },
      "source": [
        "## 1. Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = \"data/divina_textonly.txt\"\n",
        "target_file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsaaVHjU4YwK"
      },
      "source": [
        "The `load_verses` function loads the file, splits it into verses, prepends the start_symbol and appends the end_symbol to each verse, then pads each verse to the lenght of the longest verse so that the tensor can be fed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w_i8LZc4YwL"
      },
      "source": [
        "raw_input_text, input_text, input_tokenizer = load_verses(\n",
        "    input_file, char_level=True, pad=True\n",
        ")\n",
        "raw_target_text, target_text, target_tokenizer = load_verses(\n",
        "    target_file, char_level=True, pad=True\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "7a4593c4-6ad2-42e8-d75f-6bdfe2e3c692"
      },
      "source": [
        "print(\"Length of input text: {} characters\".format(len(raw_input_text)))\n",
        "print(\"Length of target text: {} characters\".format(len(raw_target_text)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input text: 578077 characters\n",
            "Length of target text: 892871 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "l1G45yR9Y1da"
      },
      "source": [
        "input_vocab = sorted(set(input_tokenizer.word_index.keys()))\n",
        "target_vocab = sorted(set(target_tokenizer.word_index.keys()))\n",
        "input_vocab_size = len(input_vocab)\n",
        "target_vocab_size = len(target_vocab)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "88521757-7eb8-4927-cef7-d3d7943e783a"
      },
      "source": [
        "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
        "print(\"Target vocab size: {}\".format(target_vocab_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 80\n",
            "Target vocab size: 81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy83U8GH4YwO"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    input_text, target_text\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqUn4eHl4YwO"
      },
      "source": [
        "tokenizer = target_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IN8x175vimK"
      },
      "source": [
        "The dataset is created by grouping the lines in batches and by shuffling them.\n",
        "\n",
        "Each input's line is in correspondence with its target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "num_layers = 6 #con 4 non cambia niente, ma allora dov'è il problema?\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "vocab_size = (\n",
        "    len(tokenizer.word_index) + 1\n",
        ")  # the +1 is added to take into account the padding token\n",
        "\n",
        "max_length_targ, max_length_inp = target_text.shape[1], input_text.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4la7s3va4YwR"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size,\n",
        "    target_vocab_size=vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QilcFBbv4YwR"
      },
      "source": [
        "transformer_trainer = TransformerTrainer(\n",
        "    transformer, checkpoint_save_path=\"./checkpoints/char-level-syll\"\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlFbStpl4YwS",
        "outputId": "f020d1c0-47ff-4454-c314-bcab105b126b"
      },
      "source": [
        "transformer_trainer.train(dataset, 10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.7963 Accuracy 0.0154\n",
            "Epoch 1 Batch 50 Loss 3.9402 Accuracy 0.1653\n",
            "Epoch 1 Batch 100 Loss 3.4893 Accuracy 0.1892\n",
            "Epoch 1 Batch 150 Loss 3.2268 Accuracy 0.2136\n",
            "Epoch 1 Loss 3.1475 Accuracy 0.2244\n",
            "Time taken for 1 epoch: 53.46 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.2236 Accuracy 0.3658\n",
            "Epoch 2 Batch 50 Loss 2.1287 Accuracy 0.3702\n",
            "Epoch 2 Batch 100 Loss 2.0415 Accuracy 0.3868\n",
            "Epoch 2 Batch 150 Loss 1.9786 Accuracy 0.3980\n",
            "Epoch 2 Loss 1.9613 Accuracy 0.4013\n",
            "Time taken for 1 epoch: 40.09 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7778 Accuracy 0.4412\n",
            "Epoch 3 Batch 50 Loss 1.7341 Accuracy 0.4489\n",
            "Epoch 3 Batch 100 Loss 1.6955 Accuracy 0.4588\n",
            "Epoch 3 Batch 150 Loss 1.6598 Accuracy 0.4677\n",
            "Epoch 3 Loss 1.6502 Accuracy 0.4703\n",
            "Time taken for 1 epoch: 40.06 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5191 Accuracy 0.5018\n",
            "Epoch 4 Batch 50 Loss 1.4800 Accuracy 0.5172\n",
            "Epoch 4 Batch 100 Loss 1.4023 Accuracy 0.5433\n",
            "Epoch 4 Batch 150 Loss 1.2776 Accuracy 0.5855\n",
            "Epoch 4 Loss 1.2319 Accuracy 0.6006\n",
            "Time taken for 1 epoch: 39.86 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7112 Accuracy 0.7726\n",
            "Epoch 5 Batch 50 Loss 0.5550 Accuracy 0.8242\n",
            "Epoch 5 Batch 100 Loss 0.4681 Accuracy 0.8513\n",
            "Epoch 5 Batch 150 Loss 0.4054 Accuracy 0.8707\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/char-level-syll/ckpt-1\n",
            "Epoch 5 Loss 0.3900 Accuracy 0.8756\n",
            "Time taken for 1 epoch: 40.27 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2130 Accuracy 0.9344\n",
            "Epoch 6 Batch 50 Loss 0.2058 Accuracy 0.9333\n",
            "Epoch 6 Batch 100 Loss 0.1873 Accuracy 0.9393\n",
            "Epoch 6 Batch 150 Loss 0.1751 Accuracy 0.9430\n",
            "Epoch 6 Loss 0.1728 Accuracy 0.9438\n",
            "Time taken for 1 epoch: 39.84 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1395 Accuracy 0.9544\n",
            "Epoch 7 Batch 50 Loss 0.1268 Accuracy 0.9587\n",
            "Epoch 7 Batch 100 Loss 0.1205 Accuracy 0.9612\n",
            "Epoch 7 Batch 150 Loss 0.1138 Accuracy 0.9632\n",
            "Epoch 7 Loss 0.1124 Accuracy 0.9637\n",
            "Time taken for 1 epoch: 39.79 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0729 Accuracy 0.9781\n",
            "Epoch 8 Batch 50 Loss 0.0899 Accuracy 0.9710\n",
            "Epoch 8 Batch 100 Loss 0.0822 Accuracy 0.9734\n",
            "Epoch 8 Batch 150 Loss 0.0806 Accuracy 0.9741\n",
            "Epoch 8 Loss 0.0803 Accuracy 0.9742\n",
            "Time taken for 1 epoch: 39.85 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0779 Accuracy 0.9764\n",
            "Epoch 9 Batch 50 Loss 0.0634 Accuracy 0.9795\n",
            "Epoch 9 Batch 100 Loss 0.0645 Accuracy 0.9794\n",
            "Epoch 9 Batch 150 Loss 0.0617 Accuracy 0.9804\n",
            "Epoch 9 Loss 0.0609 Accuracy 0.9806\n",
            "Time taken for 1 epoch: 39.76 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0573 Accuracy 0.9790\n",
            "Epoch 10 Batch 50 Loss 0.0485 Accuracy 0.9845\n",
            "Epoch 10 Batch 100 Loss 0.0487 Accuracy 0.9847\n",
            "Epoch 10 Batch 150 Loss 0.0516 Accuracy 0.9840\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/char-level-syll/ckpt-2\n",
            "Epoch 10 Loss 0.0508 Accuracy 0.9841\n",
            "Time taken for 1 epoch: 40.20 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVGgP8Su9MH"
      },
      "source": [
        "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Syllabification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxy5PshY4YwT"
      },
      "source": [
        "def evaluate(sentence, max_length=200):\n",
        "\n",
        "    encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
        "    )\n",
        "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "\n",
        "    output = tf.convert_to_tensor([tokenizer.word_index[\"^\"]])\n",
        "    output = tf.expand_dims(output, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(max_length):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "            break\n",
        "\n",
        "    # output.shape (1, tokens)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "PkCDxGdeyF9f"
      },
      "source": [
        "def print_translation(sentence, result, ground_truth):\n",
        "    print(f'{\"Input:\":15s}: {sentence}')\n",
        "    print(f'{\"Prediction\":15s}: {result}')\n",
        "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5cULw_54F8w",
        "outputId": "b0c6fda8-4a41-4314-f102-fc69c2aee7ce"
      },
      "source": [
        "sentence = \"^E come l’aere, quand’ è ben pïorno,$\"\n",
        "ground_truth = \"|E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\"\n",
        "\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^E come l’aere, quand’ è ben pïorno,$\n",
            "Prediction     : | C e n | d a | p e t | g i a l | v a | b e | m i r | g e | ò | z a | r e n | L r l n | v u | v u $ \n",
            "Ground truth   : |E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVlRB6kxJwGd",
        "outputId": "da8b3e65-44d9-4217-98fc-1f2275e09254"
      },
      "source": [
        "sentence = \"^stasera mi butto, mi butto con te$\"\n",
        "ground_truth = \"\"\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^stasera mi butto, mi butto con te$\n",
            "Prediction     : | c | s i | c a | l i e | p o m | m s n | v e | p o | e z | s n e | d n e | d r e | s a u | p o | z o $ \n",
            "Ground truth   : \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0s94aNBlt9R"
      },
      "source": [
        "## 5. Apply the syllabification to the Orlando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibMIlXjyl7KC"
      },
      "source": [
        "orlando_path = 'data/orlando-textonly.txt'\n",
        "orlando = open(orlando_path, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "orlando = orlando.split('\\n')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj6KIOHqmGWY"
      },
      "source": [
        "@predicate\n",
        "def is_not_empty(string):\n",
        "    \"\"\"\n",
        "    Checks string is not empty\n",
        "    \"\"\"\n",
        "    return string is not None and string != ''\n",
        "\n",
        "@predicate\n",
        "def is_not_number(string):\n",
        "    try:\n",
        "        int(string)\n",
        "        return False\n",
        "    except:\n",
        "        return True\n",
        "\n",
        "@predicate\n",
        "def is_not_chapter(string):\n",
        "    return not re.match(r'CANTO .*', string)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF0ikBc-mJV_"
      },
      "source": [
        "orlando_textonly = list(map(lambda x: x.strip(), orlando))\n",
        "orlando_textonly = list(filter(is_not_empty, orlando_textonly))\n",
        "orlando_textonly = list(filter(is_not_number, orlando_textonly))\n",
        "orlando_textonly = list(filter(is_not_chapter, orlando_textonly))\n",
        "orlando_syll = list(map(hyphenation, orlando_textonly))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daQCC4lamL9s"
      },
      "source": [
        "def preprocess(x):\n",
        "  x = '^' + x + '$'\n",
        "  return x\n",
        "\n",
        "orlando_preprocessed = list(map(preprocess, orlando_textonly))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMUMNvFfmOv-"
      },
      "source": [
        "tot_verses = len(orlando_textonly)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZYE3HCUmRXu"
      },
      "source": [
        "indices = np.random.randint(0, tot_verses, size=1000)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgSYkpwtmTX2"
      },
      "source": [
        "neural_syll = []\n",
        "for i in indices:\n",
        "  neural_syll.append(evaluate(orlando_textonly[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}