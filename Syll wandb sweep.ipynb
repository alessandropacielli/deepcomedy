{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepcomedy/\n",
      "deepcomedy/util/\n",
      "deepcomedy/util/predicate.py\n",
      "deepcomedy/util/__pycache__/\n",
      "deepcomedy/util/__pycache__/predicate.cpython-37.pyc\n",
      "deepcomedy/util/__pycache__/__init__.cpython-37.pyc\n",
      "deepcomedy/util/__init__.py\n",
      "deepcomedy/util/.ipynb_checkpoints/\n",
      "deepcomedy/util/.ipynb_checkpoints/predicate-checkpoint.py\n",
      "deepcomedy/models/\n",
      "deepcomedy/models/layers.py\n",
      "deepcomedy/models/transformer copy.py\n",
      "deepcomedy/models/transformer.py\n",
      "deepcomedy/models/__pycache__/\n",
      "deepcomedy/models/__pycache__/layers.cpython-37.pyc\n",
      "deepcomedy/models/__pycache__/__init__.cpython-37.pyc\n",
      "deepcomedy/models/__pycache__/transformer.cpython-37.pyc\n",
      "deepcomedy/models/__init__.py\n",
      "deepcomedy/models/.ipynb_checkpoints/\n",
      "deepcomedy/models/.ipynb_checkpoints/transformer-checkpoint.py\n",
      "deepcomedy/preprocessing.py\n",
      "deepcomedy/__pycache__/\n",
      "deepcomedy/__pycache__/__init__.cpython-37.pyc\n",
      "deepcomedy/__pycache__/preprocessing.cpython-37.pyc\n",
      "deepcomedy/metrics.py\n",
      "deepcomedy/__init__.py\n",
      "deepcomedy/.ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf deepcomedy.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb04733-34db-4e99-8e8a-50cad74df048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7410fa-bec9-4333-88f0-ff0488ffb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import wandb\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76593e7f-824e-4956-950f-1f06b7a61d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for now only char-level syllabification\n",
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_tercets(raw_syll_text)\n",
    "text = preprocess_tercets(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf882cb6-f737-41cd-ab96-03790dc7b84d",
   "metadata": {},
   "source": [
    "Split preprocessed text into tercets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2498e9c0-1884-4f07-a1a6-13d532685f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \"<EOT>\"\n",
    "input_tercets = [x + sep for x in text.split(sep)][:-1]\n",
    "target_tercets = [x + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4b019-0c83-4b08-9474-0e72195d9cdb",
   "metadata": {},
   "source": [
    "Encode with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d001ff3c-393d-42c9-9b3b-276c351448b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_tercets)\n",
    "enc_input_tercets = tokenizer.texts_to_sequences(input_tercets)\n",
    "enc_target_tercets = tokenizer.texts_to_sequences(target_tercets)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b416c8-8d69-4095-8d52-447b2d612f6d",
   "metadata": {},
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fc9903-a97e-46a7-a2bc-bdf4dcf4d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_input_tercets, padding=\"post\"\n",
    ")\n",
    "target_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    enc_target_tercets, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cd401d-0fc7-4fa1-adcc-b1daeca6ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: flop67q2\n",
      "Sweep URL: https://wandb.ai/alessandropacielli/deepcomedy/sweeps/flop67q2\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep-test-1\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [32]},\n",
    "        \"epochs\": {\"values\": [10, 20, 50, 100]},\n",
    "        \"num_layers\": {\"values\": [6, 8]},\n",
    "        \"num_heads\": {\"values\": [2, 4, 8]},\n",
    "        \"d_model\": {\"values\": [256]},\n",
    "        \"dff\": {\"values\": [512, 1024]},\n",
    "        # TODO include architecture + dataset\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='deepcomedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df5adac-7f30-4e22-b9aa-6d57987de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tercets, target_tercets, batch_size):\n",
    "    buffer_size = len(input_tercets)\n",
    "    batch_size = 64\n",
    "\n",
    "    steps_per_epoch = len(input_tercets) // batch_size\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_tercets, target_tercets)\n",
    "    ).shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6691b191-4dbb-4b6a-8ba6-3504f45ba806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(config):\n",
    "    transformer = Transformer(\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        dff=config[\"dff\"],\n",
    "        input_vocab_size=vocab_size,\n",
    "        target_vocab_size=vocab_size,\n",
    "        pe_input=1000,\n",
    "        pe_target=1000,\n",
    "        rate=0.1,\n",
    "    )\n",
    "    transformer_trainer = TransformerTrainer(transformer, checkpoint_save_path=None)\n",
    "\n",
    "    return transformer, transformer_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fb2e1-1583-46dc-b753-70ecfe5b375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xhn9xi5d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdff: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.29<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">eager-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/alessandropacielli/deepcomedy\" target=\"_blank\">https://wandb.ai/alessandropacielli/deepcomedy</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/alessandropacielli/deepcomedy/sweeps/flop67q2\" target=\"_blank\">https://wandb.ai/alessandropacielli/deepcomedy/sweeps/flop67q2</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/alessandropacielli/deepcomedy/runs/xhn9xi5d\" target=\"_blank\">https://wandb.ai/alessandropacielli/deepcomedy/runs/xhn9xi5d</a><br/>\n",
       "                Run data is saved locally in <code>/notebooks/wandb/run-20210523_224909-xhn9xi5d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.1799 Accuracy 0.0040\n"
     ]
    }
   ],
   "source": [
    "def sweep():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        dataset = make_dataset(input_train, target_train, config[\"batch_size\"])\n",
    "        model, trainer = make_model(config)\n",
    "        trainer.train(dataset, config[\"epochs\"], log_wandb=True)\n",
    "\n",
    "count = 5\n",
    "wandb.agent(sweep_id, function=sweep, count=count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
