{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SybxOt22fBmE",
    "outputId": "f3482536-c8cb-45a5-d8ee-3a4133c8faa1"
   },
   "outputs": [],
   "source": [
    "!pip install wandb strsimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZFNtyeMfmnp",
    "outputId": "6e9f12ff-da81-4814-acce-3cc52c333438"
   },
   "outputs": [],
   "source": [
    "!tar zxvf deepcomedy.tar.gz\n",
    "!tar zxvf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from deepcomedy.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT",
    "tags": []
   },
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jH7n29oxB0z4"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_tercet='')\n",
    "text = preprocess_text(raw_text, end_of_tercet='', word_level= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ASHyaMBC84V"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-IF6sE6FC_4J"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOV>\"\n",
    "input_tercets = [x.lstrip() + sep for x in text.split(sep)][:-1]\n",
    "target_tercets = [x.lstrip() + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdUmYhUKDEuj"
   },
   "source": [
    "Encode with input and target tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-mob1kOzDD4z"
   },
   "outputs": [],
   "source": [
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "input_tokenizer.fit_on_texts(input_tercets)\n",
    "\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "target_tokenizer.fit_on_texts(target_tercets)\n",
    "\n",
    "enc_input_tercets = input_tokenizer.texts_to_sequences(input_tercets)\n",
    "enc_target_tercets = target_tokenizer.texts_to_sequences(target_tercets)\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xDKv92yAL_t8"
   },
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "target_text_tercet = []\n",
    "\n",
    "for line in range(len(enc_input_tercets) - 2):\n",
    "    input_text.append(list(chain(*enc_input_tercets[line : line + 3])))\n",
    "    target_text_tercet.append(list(chain(*enc_target_tercets[line : line + 3])))\n",
    "    target_text.append(list(chain(*enc_target_tercets[line : line + 4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY44HP5lKz2-"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Pq34y57yK3wd"
   },
   "outputs": [],
   "source": [
    "padded_input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_text, padding=\"post\"\n",
    ")\n",
    "padded_target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_text, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LdFUuDTcf0Mr"
   },
   "outputs": [],
   "source": [
    "input_train, input_val, target_train, target_val = train_test_split(padded_input_text, padded_target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6v2e1bIcLRZi"
   },
   "outputs": [],
   "source": [
    "dataset = make_dataset(input_train, target_train)\n",
    "val_dataset = make_dataset(input_val, target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-sGOf__BLZzQ"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\" : 6,\n",
    "    \"d_model\" : 256,\n",
    "    \"num_heads\" : 4,\n",
    "    \"dff\" : 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "miEcmOVmL0Rt"
   },
   "outputs": [],
   "source": [
    "transformer, transformer_trainer = make_transformer_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaR03YUNL_uF",
    "outputId": "2ef63442-189b-400a-d35c-bbfb0f062a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.4091 Accuracy 0.0008\n",
      "Epoch 1 Batch 50 Loss 4.2407 Accuracy 0.1238\n",
      "Epoch 1 Batch 100 Loss 3.6921 Accuracy 0.1638\n",
      "Epoch 1 Batch 150 Loss 3.4692 Accuracy 0.1800\n",
      "Epoch 1 Batch 200 Loss 3.2968 Accuracy 0.1976\n",
      "Epoch 1 Batch 250 Loss 3.1047 Accuracy 0.2225\n",
      "Epoch 1 Batch 300 Loss 2.9436 Accuracy 0.2433\n",
      "Epoch 1 Batch 0 Validation Loss 1.9496 Validation Accuracy 0.3826\n",
      "Epoch 1 Batch 50 Validation Loss 1.9754 Validation Accuracy 0.3752\n",
      "Epoch 1 Batch 100 Validation Loss 1.9726 Validation Accuracy 0.3754\n",
      "Epoch 1 Loss 2.8584 Accuracy 0.2542\n",
      "Time taken for 1 epoch: 96.52 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.0470 Accuracy 0.3592\n",
      "Epoch 2 Batch 50 Loss 2.0151 Accuracy 0.3640\n",
      "Epoch 2 Batch 100 Loss 1.9988 Accuracy 0.3661\n",
      "Epoch 2 Batch 150 Loss 1.9869 Accuracy 0.3678\n",
      "Epoch 2 Batch 200 Loss 1.9747 Accuracy 0.3702\n",
      "Epoch 2 Batch 250 Loss 1.9627 Accuracy 0.3725\n",
      "Epoch 2 Batch 300 Loss 1.9501 Accuracy 0.3754\n",
      "Epoch 2 Batch 0 Validation Loss 1.8128 Validation Accuracy 0.4117\n",
      "Epoch 2 Batch 50 Validation Loss 1.7987 Validation Accuracy 0.4138\n",
      "Epoch 2 Batch 100 Validation Loss 1.7986 Validation Accuracy 0.4144\n",
      "Epoch 2 Loss 1.9414 Accuracy 0.3776\n",
      "Time taken for 1 epoch: 86.19 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.8552 Accuracy 0.3988\n",
      "Epoch 3 Batch 50 Loss 1.8324 Accuracy 0.4043\n",
      "Epoch 3 Batch 100 Loss 1.8106 Accuracy 0.4100\n",
      "Epoch 3 Batch 150 Loss 1.7936 Accuracy 0.4142\n",
      "Epoch 3 Batch 200 Loss 1.7758 Accuracy 0.4191\n",
      "Epoch 3 Batch 250 Loss 1.7585 Accuracy 0.4241\n",
      "Epoch 3 Batch 300 Loss 1.7427 Accuracy 0.4286\n",
      "Epoch 3 Batch 0 Validation Loss 1.5741 Validation Accuracy 0.4841\n",
      "Epoch 3 Batch 50 Validation Loss 1.5734 Validation Accuracy 0.4806\n",
      "Epoch 3 Batch 100 Validation Loss 1.5729 Validation Accuracy 0.4806\n",
      "Epoch 3 Loss 1.7323 Accuracy 0.4317\n",
      "Time taken for 1 epoch: 85.97 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.6177 Accuracy 0.4642\n",
      "Epoch 4 Batch 50 Loss 1.6161 Accuracy 0.4670\n",
      "Epoch 4 Batch 100 Loss 1.6054 Accuracy 0.4720\n",
      "Epoch 4 Batch 150 Loss 1.5949 Accuracy 0.4750\n",
      "Epoch 4 Batch 200 Loss 1.5842 Accuracy 0.4781\n",
      "Epoch 4 Batch 250 Loss 1.5742 Accuracy 0.4811\n",
      "Epoch 4 Batch 300 Loss 1.5638 Accuracy 0.4842\n",
      "Epoch 4 Batch 0 Validation Loss 1.4406 Validation Accuracy 0.5180\n",
      "Epoch 4 Batch 50 Validation Loss 1.4323 Validation Accuracy 0.5234\n",
      "Epoch 4 Batch 100 Validation Loss 1.4345 Validation Accuracy 0.5224\n",
      "Epoch 4 Loss 1.5571 Accuracy 0.4863\n",
      "Time taken for 1 epoch: 86.16 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4798 Accuracy 0.5075\n",
      "Epoch 5 Batch 50 Loss 1.4758 Accuracy 0.5110\n",
      "Epoch 5 Batch 100 Loss 1.4679 Accuracy 0.5128\n",
      "Epoch 5 Batch 150 Loss 1.4606 Accuracy 0.5156\n",
      "Epoch 5 Batch 200 Loss 1.4518 Accuracy 0.5185\n",
      "Epoch 5 Batch 250 Loss 1.4450 Accuracy 0.5207\n",
      "Epoch 5 Batch 300 Loss 1.4384 Accuracy 0.5226\n",
      "Epoch 5 Batch 0 Validation Loss 1.3223 Validation Accuracy 0.5541\n",
      "Epoch 5 Batch 50 Validation Loss 1.3285 Validation Accuracy 0.5583\n",
      "Epoch 5 Batch 100 Validation Loss 1.3288 Validation Accuracy 0.5577\n",
      "Epoch 5 Loss 1.4345 Accuracy 0.5239\n",
      "Time taken for 1 epoch: 86.09 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3859 Accuracy 0.5458\n",
      "Epoch 6 Batch 50 Loss 1.3742 Accuracy 0.5438\n",
      "Epoch 6 Batch 100 Loss 1.3687 Accuracy 0.5456\n",
      "Epoch 6 Batch 150 Loss 1.3616 Accuracy 0.5479\n",
      "Epoch 6 Batch 200 Loss 1.3547 Accuracy 0.5504\n"
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 30, validation_dataset=val_dataset, validation_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgDU7jfoe2UJ"
   },
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAvzChyle2UL"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    transformer,\n",
    "    encoder_input,\n",
    "    decoder_input,\n",
    "    stop_symbol,\n",
    "    max_length=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts the output of the model given the `input_sequence`.\n",
    "    The `input_sequence` is encoded by the Encoder, then its output is fed to the Decoder,\n",
    "    whose output is fed back into the Decoder until the `stop_symbol` token is produced.\n",
    "\n",
    "    This function works with a batch of inputs and stops when all outputs include a stop symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    output = decoder_input\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output\n",
    "    )\n",
    "\n",
    "    enc_output = transformer.encoder(\n",
    "        encoder_input, False, enc_padding_mask\n",
    "    )  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, _ = transformer.decoder(\n",
    "            output, enc_output, False, combined_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = transformer.final_layer(dec_output)\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predicted_ids = tf.argmax(predictions[:, -1:, :], axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [\n",
    "                tf.cast(output, dtype=tf.int32),\n",
    "                tf.cast(predicted_ids, dtype=tf.int32),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "                \n",
    "        if sum(output.numpy()[0] == stop_symbol) == 4:\n",
    "            print('Stopped')\n",
    "            return output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXC7XrTle2UU"
   },
   "outputs": [],
   "source": [
    "def generate(transformer, input_sequence, target_sequence, input_tokenizer, target_tokenizer, steps, start_symbol, stop_symbol):\n",
    "\n",
    "    result = target_tokenizer.sequences_to_texts(target_sequence)[0]\n",
    "    \n",
    "    encoder_input = input_sequence\n",
    "    decoder_input = target_sequence\n",
    "\n",
    "    for _ in range(steps):\n",
    "\n",
    "        encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "        output = evaluate(transformer, encoder_input, decoder_input, stop_symbol)\n",
    "\n",
    "        generated_text = target_tokenizer.sequences_to_texts(output.numpy())[0]\n",
    "        \n",
    "        verses = [line.lstrip() + '<EOV> ' for line in generated_text.split('<EOV>') if line.strip() != '']\n",
    "        \n",
    "        result = ''.join([result, verses[-1]])\n",
    "                \n",
    "        verses = ''.join(verses[-3:])\n",
    "        \n",
    "        decoder_input = target_tokenizer.texts_to_sequences([verses])\n",
    "        \n",
    "        verses = remove_syll_token(verses)\n",
    "        verses = re.sub(r\"[ ]+\", \"\", verses)\n",
    "        verses = re.sub(\"<[^>]*>\", \" \\g<0> \", verses)\n",
    "        verses = re.sub(\"<EOV>  <GO>\", \"<EOV> <GO>\", verses)\n",
    "        verses = verses.strip()\n",
    "\n",
    "        encoder_input = input_tokenizer.texts_to_sequences([verses])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMSbfxwae2UW",
    "outputId": "6af58fe0-8e51-49cc-b0b0-445b4d77f113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
    "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]\n",
    "\n",
    "encoder_input = [input_text[0]]\n",
    "decoder_input = [target_text_tercet[0]]\n",
    "\n",
    "result = generate(transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1uAYbYDe2UX",
    "outputId": "88afde3e-803a-4e2d-d82d-f746658030be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Nel |mez|zo |del |cam|min |di |no|stra |vi|ta\n",
      "|mi |ri|tro|vai |per |u|na |sel|va o|scu|ra,\n",
      "|ché |la |di|rit|ta |via |e|ra |smar|ri|ta.\n",
      "|El|la |so|pra |che ’l |vi|so a |quel|la |gen|te\n",
      "|che |per |lo |suo |av|ver|sa|rio al|trui |man|to,\n",
      "|e |al|tra |vo|ce |mi |pa|rea |più |rat|ta.\n",
      "« |O |tu |che |se’ |che |sì |pres|so |di|sciol|ta»,\n",
      "|dis|se ’l |ma|e|stro,« |quan|to |pos|so |po|co,\n",
      "|se |non |che |tu |se’ |tem|pion |far |non |la|ti,\n"
     ]
    }
   ],
   "source": [
    "print(strip_tokens(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5MorsDLe2UY"
   },
   "source": [
    "## 5. Syllabification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbBb0fLTe2UY"
   },
   "outputs": [],
   "source": [
    "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
    "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huknwnsde2UZ"
   },
   "outputs": [],
   "source": [
    "encoder_input = tf.convert_to_tensor([input_text[0]])\n",
    "decoder_input = tf.convert_to_tensor([[start_symbol]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxTgAWrpe2Ua",
    "outputId": "a00ceade-6cc3-4ac2-8b12-4a51c5491143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "syll_output = evaluate(transformer, encoder_input, decoder_input, stop_symbol, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDqwIrMee2Ua",
    "outputId": "ae315695-cdc1-4428-fdf9-83fbf8c13212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<GO> | c h e <SEP> | d i <SEP> | p e n | s i e r <SEP> | m i <SEP> | s t a | v a <SEP> i n <SEP> | u | n o <SEP> | s t r a | l e , <EOV> <GO> | e <SEP> | d i | c o <SEP> | d i <SEP> | g e n | t e <SEP> a l | t r o <SEP> | c h e <SEP> | p i ù <SEP> | d o l | c e » . <EOV> <GO> | N o i <SEP> | e | r a | v a m <SEP> | n e l <SEP> | s u o <SEP> | a | s p e t | t o <SEP> | b a n | d o <EOV> <GO> | c h e <SEP> | l ’ <SEP> a | n i | m a <SEP> | s u a <SEP> | a v | v e n | t a <SEP> | d i | s t a n | t e , <EOV>']\n"
     ]
    }
   ],
   "source": [
    "print(target_tokenizer.sequences_to_texts(syll_output.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXMfBSRTe2Ub"
   },
   "source": [
    "Potrebbe essere underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzPwlV9ae2Uc"
   },
   "source": [
    "## 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfTkQfWle2Uc"
   },
   "outputs": [],
   "source": [
    "transformer.save_weights('models/w2c-gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acIj_s75e2Ue"
   },
   "outputs": [],
   "source": [
    "new_transformer = Transformer(\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        dff=config[\"dff\"],\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=1000,\n",
    "        pe_target=1000,\n",
    "        rate=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKj4qsd6e2Ue"
   },
   "outputs": [],
   "source": [
    "# In order to load the new weights the model should be called once for the variables to be initialized\n",
    "\n",
    "# Any inp, tar is ok here\n",
    "inp = tf.convert_to_tensor([[start_symbol]])\n",
    "tar = tf.convert_to_tensor([[start_symbol]])\n",
    "\n",
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "\n",
    "new_transformer(inp, tar, False, enc_padding_mask, look_ahead_mask, dec_padding_mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9IMbvrte2Uf"
   },
   "outputs": [],
   "source": [
    "new_transformer.load_weights('models/w2c-gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEpPqSJRe2Uf",
    "outputId": "3d2f2647-6032-4194-cc94-cb2f286dbb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "encoder_input = [input_text[0]]\n",
    "decoder_input = [target_text_tercet[0]]\n",
    "\n",
    "result = generate(new_transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOZR_Zd6e2Ug",
    "outputId": "ede37c97-bd43-4d6c-886d-8cee47ab03e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> | N e l <SEP> | m e z | z o <SEP> | d e l <SEP> | c a m | m i n <SEP> | d i <SEP> | n o | s t r a <SEP> | v i | t a <EOV> <GO> | m i <SEP> | r i | t r o | v a i <SEP> | p e r <SEP> | u | n a <SEP> | s e l | v a <SEP> o | s c u | r a , <EOV> <GO> | c h é <SEP> | l a <SEP> | d i | r i t | t a <SEP> | v i a <SEP> | e | r a <SEP> | s m a r | r i | t a . <EOV><GO> | E l | l a <SEP> | s o | p r a <SEP> | c h e <SEP> ’ l <SEP> | v i | s o <SEP> a <SEP> | q u e l | l a <SEP> | g e n | t e <EOV> <GO> | c h e <SEP> | p e r <SEP> | l o <SEP> | s u o <SEP> | a v | v e r | s a | r i o <SEP> a l | t r u i <SEP> | m a n | t o , <EOV> <GO> | e <SEP> | a l | t r a <SEP> | v o | c e <SEP> | m i <SEP> | p a | r e a <SEP> | p i ù <SEP> | r a t | t a . <EOV> <GO> « <SEP> | O <SEP> | t u <SEP> | c h e <SEP> | s e ’ <SEP> | c h e <SEP> | s ì <SEP> | p r e s | s o <SEP> | d i | s c i o l | t a » , <EOV> <GO> | d i s | s e <SEP> ’ l <SEP> | m a | e | s t r o , « <SEP> | q u a n | t o <SEP> | p o s | s o <SEP> | p o | c o , <EOV> <GO> | s e <SEP> | n o n <SEP> | c h e <SEP> | t u <SEP> | s e ’ <SEP> | t e m | p i o n <SEP> | f a r <SEP> | n o n <SEP> | l a | t i , <EOV> '"
      ]
     },
     "execution_count": 320,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Char transformer generation-WORKING.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
