{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YcDazsVbL_tS"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from deepcomedy.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT",
    "tags": []
   },
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "jH7n29oxB0z4"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_tercet='')\n",
    "text = preprocess_text(raw_text, end_of_tercet='', word_level= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ASHyaMBC84V"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "-IF6sE6FC_4J"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOV>\"\n",
    "input_tercets = [x.lstrip() + sep for x in text.split(sep)][:-1]\n",
    "target_tercets = [x.lstrip() + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdUmYhUKDEuj"
   },
   "source": [
    "Encode with input and target tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "-mob1kOzDD4z"
   },
   "outputs": [],
   "source": [
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "input_tokenizer.fit_on_texts(input_tercets)\n",
    "\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "target_tokenizer.fit_on_texts(target_tercets)\n",
    "\n",
    "enc_input_tercets = input_tokenizer.texts_to_sequences(input_tercets)\n",
    "enc_target_tercets = target_tokenizer.texts_to_sequences(target_tercets)\n",
    "\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "xDKv92yAL_t8"
   },
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "target_text_tercet = []\n",
    "\n",
    "for line in range(len(enc_input_tercets) - 2):\n",
    "    input_text.append(list(chain(*enc_input_tercets[line : line + 3])))\n",
    "    target_text_tercet.append(list(chain(*enc_target_tercets[line : line + 3])))\n",
    "    target_text.append(list(chain(*enc_target_tercets[line : line + 4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY44HP5lKz2-"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "Pq34y57yK3wd"
   },
   "outputs": [],
   "source": [
    "padded_input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_text, padding=\"post\"\n",
    ")\n",
    "padded_target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_text, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6v2e1bIcLRZi"
   },
   "outputs": [],
   "source": [
    "dataset = make_dataset(padded_input_text, padded_target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-sGOf__BLZzQ"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\" : 6,\n",
    "    \"d_model\" : 256,\n",
    "    \"num_heads\" : 4,\n",
    "    \"dff\" : 512,\n",
    "}\n",
    "\n",
    "checkpoint_save_path = \"./checkpoints/word-input_char-output_gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "miEcmOVmL0Rt"
   },
   "outputs": [],
   "source": [
    "transformer, transformer_trainer = make_transformer_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "QaR03YUNL_uF",
    "outputId": "c5ac97f9-0ac6-4cb1-d783-c8653ad3338b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7331 Accuracy 0.7489\n",
      "Epoch 1 Batch 50 Loss 0.7390 Accuracy 0.7471\n",
      "Epoch 1 Batch 100 Loss 0.7410 Accuracy 0.7464\n",
      "Epoch 1 Batch 150 Loss 0.7412 Accuracy 0.7462\n",
      "Epoch 1 Batch 200 Loss 0.7422 Accuracy 0.7459\n",
      "Epoch 1 Batch 250 Loss 0.7431 Accuracy 0.7454\n",
      "Epoch 1 Batch 300 Loss 0.7438 Accuracy 0.7451\n",
      "Epoch 1 Batch 350 Loss 0.7441 Accuracy 0.7449\n",
      "Epoch 1 Batch 400 Loss 0.7439 Accuracy 0.7450\n",
      "Epoch 1 Loss 0.7442 Accuracy 0.7449\n",
      "Time taken for 1 epoch: 75.70 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7389 Accuracy 0.7467\n",
      "Epoch 2 Batch 50 Loss 0.7190 Accuracy 0.7536\n",
      "Epoch 2 Batch 100 Loss 0.7199 Accuracy 0.7531\n",
      "Epoch 2 Batch 150 Loss 0.7229 Accuracy 0.7522\n",
      "Epoch 2 Batch 200 Loss 0.7239 Accuracy 0.7518\n",
      "Epoch 2 Batch 250 Loss 0.7250 Accuracy 0.7515\n",
      "Epoch 2 Batch 300 Loss 0.7255 Accuracy 0.7513\n",
      "Epoch 2 Batch 350 Loss 0.7263 Accuracy 0.7511\n",
      "Epoch 2 Batch 400 Loss 0.7264 Accuracy 0.7510\n",
      "Epoch 2 Loss 0.7267 Accuracy 0.7509\n",
      "Time taken for 1 epoch: 75.69 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6996 Accuracy 0.7569\n",
      "Epoch 3 Batch 50 Loss 0.6997 Accuracy 0.7598\n",
      "Epoch 3 Batch 100 Loss 0.7040 Accuracy 0.7583\n",
      "Epoch 3 Batch 150 Loss 0.7062 Accuracy 0.7575\n",
      "Epoch 3 Batch 200 Loss 0.7068 Accuracy 0.7573\n",
      "Epoch 3 Batch 250 Loss 0.7077 Accuracy 0.7570\n",
      "Epoch 3 Batch 300 Loss 0.7080 Accuracy 0.7569\n",
      "Epoch 3 Batch 350 Loss 0.7084 Accuracy 0.7568\n",
      "Epoch 3 Batch 400 Loss 0.7093 Accuracy 0.7565\n",
      "Epoch 3 Loss 0.7099 Accuracy 0.7563\n",
      "Time taken for 1 epoch: 75.48 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6747 Accuracy 0.7685\n",
      "Epoch 4 Batch 50 Loss 0.6832 Accuracy 0.7652\n",
      "Epoch 4 Batch 100 Loss 0.6855 Accuracy 0.7645\n",
      "Epoch 4 Batch 150 Loss 0.6902 Accuracy 0.7632\n",
      "Epoch 4 Batch 200 Loss 0.6920 Accuracy 0.7626\n",
      "Epoch 4 Batch 250 Loss 0.6928 Accuracy 0.7621\n",
      "Epoch 4 Batch 300 Loss 0.6938 Accuracy 0.7618\n",
      "Epoch 4 Batch 350 Loss 0.6939 Accuracy 0.7618\n",
      "Epoch 4 Batch 400 Loss 0.6943 Accuracy 0.7617\n",
      "Epoch 4 Loss 0.6945 Accuracy 0.7617\n",
      "Time taken for 1 epoch: 75.42 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6706 Accuracy 0.7695\n",
      "Epoch 5 Batch 50 Loss 0.6699 Accuracy 0.7689\n",
      "Epoch 5 Batch 100 Loss 0.6727 Accuracy 0.7688\n",
      "Epoch 5 Batch 150 Loss 0.6746 Accuracy 0.7681\n",
      "Epoch 5 Batch 200 Loss 0.6759 Accuracy 0.7677\n",
      "Epoch 5 Batch 250 Loss 0.6765 Accuracy 0.7673\n",
      "Epoch 5 Batch 300 Loss 0.6776 Accuracy 0.7671\n",
      "Epoch 5 Batch 350 Loss 0.6786 Accuracy 0.7667\n",
      "Epoch 5 Batch 400 Loss 0.6792 Accuracy 0.7665\n",
      "Epoch 5 Loss 0.6799 Accuracy 0.7663\n",
      "Time taken for 1 epoch: 75.70 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6883 Accuracy 0.7622\n",
      "Epoch 6 Batch 50 Loss 0.6567 Accuracy 0.7736\n",
      "Epoch 6 Batch 100 Loss 0.6582 Accuracy 0.7734\n",
      "Epoch 6 Batch 150 Loss 0.6590 Accuracy 0.7735\n",
      "Epoch 6 Batch 200 Loss 0.6609 Accuracy 0.7729\n",
      "Epoch 6 Batch 250 Loss 0.6625 Accuracy 0.7722\n",
      "Epoch 6 Batch 300 Loss 0.6631 Accuracy 0.7719\n",
      "Epoch 6 Batch 350 Loss 0.6639 Accuracy 0.7717\n",
      "Epoch 6 Batch 400 Loss 0.6645 Accuracy 0.7715\n",
      "Epoch 6 Loss 0.6648 Accuracy 0.7714\n",
      "Time taken for 1 epoch: 76.54 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.6319 Accuracy 0.7842\n",
      "Epoch 7 Batch 50 Loss 0.6424 Accuracy 0.7789\n",
      "Epoch 7 Batch 100 Loss 0.6430 Accuracy 0.7789\n",
      "Epoch 7 Batch 150 Loss 0.6449 Accuracy 0.7782\n",
      "Epoch 7 Batch 200 Loss 0.6470 Accuracy 0.7775\n",
      "Epoch 7 Batch 250 Loss 0.6486 Accuracy 0.7770\n",
      "Epoch 7 Batch 300 Loss 0.6497 Accuracy 0.7766\n",
      "Epoch 7 Batch 350 Loss 0.6504 Accuracy 0.7764\n",
      "Epoch 7 Batch 400 Loss 0.6505 Accuracy 0.7763\n",
      "Epoch 7 Loss 0.6507 Accuracy 0.7762\n",
      "Time taken for 1 epoch: 76.37 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6207 Accuracy 0.7872\n",
      "Epoch 8 Batch 50 Loss 0.6232 Accuracy 0.7853\n",
      "Epoch 8 Batch 100 Loss 0.6271 Accuracy 0.7840\n",
      "Epoch 8 Batch 150 Loss 0.6298 Accuracy 0.7834\n",
      "Epoch 8 Batch 200 Loss 0.6325 Accuracy 0.7822\n",
      "Epoch 8 Batch 250 Loss 0.6343 Accuracy 0.7816\n",
      "Epoch 8 Batch 300 Loss 0.6350 Accuracy 0.7812\n",
      "Epoch 8 Batch 350 Loss 0.6363 Accuracy 0.7808\n",
      "Epoch 8 Batch 400 Loss 0.6372 Accuracy 0.7805\n",
      "Epoch 8 Loss 0.6374 Accuracy 0.7804\n",
      "Time taken for 1 epoch: 76.42 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.6098 Accuracy 0.7849\n",
      "Epoch 9 Batch 50 Loss 0.6155 Accuracy 0.7882\n",
      "Epoch 9 Batch 100 Loss 0.6175 Accuracy 0.7872\n",
      "Epoch 9 Batch 150 Loss 0.6195 Accuracy 0.7864\n",
      "Epoch 9 Batch 200 Loss 0.6212 Accuracy 0.7858\n",
      "Epoch 9 Batch 250 Loss 0.6224 Accuracy 0.7855\n",
      "Epoch 9 Batch 300 Loss 0.6230 Accuracy 0.7852\n",
      "Epoch 9 Batch 350 Loss 0.6232 Accuracy 0.7852\n",
      "Epoch 9 Batch 400 Loss 0.6236 Accuracy 0.7852\n",
      "Epoch 9 Loss 0.6241 Accuracy 0.7850\n",
      "Time taken for 1 epoch: 76.63 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6172 Accuracy 0.7912\n",
      "Epoch 10 Batch 50 Loss 0.6011 Accuracy 0.7928\n",
      "Epoch 10 Batch 100 Loss 0.6024 Accuracy 0.7920\n",
      "Epoch 10 Batch 150 Loss 0.6048 Accuracy 0.7913\n",
      "Epoch 10 Batch 200 Loss 0.6065 Accuracy 0.7909\n",
      "Epoch 10 Batch 250 Loss 0.6078 Accuracy 0.7904\n",
      "Epoch 10 Batch 300 Loss 0.6090 Accuracy 0.7901\n",
      "Epoch 10 Batch 350 Loss 0.6092 Accuracy 0.7901\n",
      "Epoch 10 Batch 400 Loss 0.6097 Accuracy 0.7898\n",
      "Epoch 10 Loss 0.6102 Accuracy 0.7896\n",
      "Time taken for 1 epoch: 76.87 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5765 Accuracy 0.8021\n",
      "Epoch 11 Batch 50 Loss 0.5834 Accuracy 0.7990\n",
      "Epoch 11 Batch 100 Loss 0.5880 Accuracy 0.7971\n",
      "Epoch 11 Batch 150 Loss 0.5905 Accuracy 0.7965\n",
      "Epoch 11 Batch 200 Loss 0.5923 Accuracy 0.7959\n",
      "Epoch 11 Batch 250 Loss 0.5946 Accuracy 0.7951\n",
      "Epoch 11 Batch 300 Loss 0.5954 Accuracy 0.7948\n",
      "Epoch 11 Batch 350 Loss 0.5968 Accuracy 0.7944\n",
      "Epoch 11 Batch 400 Loss 0.5976 Accuracy 0.7941\n",
      "Epoch 11 Loss 0.5980 Accuracy 0.7939\n",
      "Time taken for 1 epoch: 76.88 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.5655 Accuracy 0.8064\n",
      "Epoch 12 Batch 50 Loss 0.5763 Accuracy 0.8014\n",
      "Epoch 12 Batch 100 Loss 0.5783 Accuracy 0.8006\n",
      "Epoch 12 Batch 150 Loss 0.5809 Accuracy 0.7996\n",
      "Epoch 12 Batch 200 Loss 0.5824 Accuracy 0.7993\n",
      "Epoch 12 Batch 250 Loss 0.5829 Accuracy 0.7990\n",
      "Epoch 12 Batch 300 Loss 0.5838 Accuracy 0.7987\n",
      "Epoch 12 Batch 350 Loss 0.5851 Accuracy 0.7982\n",
      "Epoch 12 Batch 400 Loss 0.5862 Accuracy 0.7978\n",
      "Epoch 12 Loss 0.5864 Accuracy 0.7976\n",
      "Time taken for 1 epoch: 76.16 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5565 Accuracy 0.8080\n",
      "Epoch 13 Batch 50 Loss 0.5646 Accuracy 0.8055\n",
      "Epoch 13 Batch 100 Loss 0.5671 Accuracy 0.8047\n",
      "Epoch 13 Batch 150 Loss 0.5683 Accuracy 0.8042\n",
      "Epoch 13 Batch 200 Loss 0.5701 Accuracy 0.8033\n",
      "Epoch 13 Batch 250 Loss 0.5712 Accuracy 0.8029\n",
      "Epoch 13 Batch 300 Loss 0.5726 Accuracy 0.8025\n",
      "Epoch 13 Batch 350 Loss 0.5734 Accuracy 0.8021\n",
      "Epoch 13 Batch 400 Loss 0.5744 Accuracy 0.8018\n",
      "Epoch 13 Loss 0.5746 Accuracy 0.8017\n",
      "Time taken for 1 epoch: 75.93 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.5532 Accuracy 0.8065\n",
      "Epoch 14 Batch 50 Loss 0.5502 Accuracy 0.8103\n",
      "Epoch 14 Batch 100 Loss 0.5546 Accuracy 0.8087\n",
      "Epoch 14 Batch 150 Loss 0.5568 Accuracy 0.8079\n",
      "Epoch 14 Batch 200 Loss 0.5594 Accuracy 0.8071\n",
      "Epoch 14 Batch 250 Loss 0.5604 Accuracy 0.8067\n",
      "Epoch 14 Batch 300 Loss 0.5611 Accuracy 0.8065\n",
      "Epoch 14 Batch 350 Loss 0.5620 Accuracy 0.8061\n",
      "Epoch 14 Batch 400 Loss 0.5629 Accuracy 0.8058\n",
      "Epoch 14 Loss 0.5635 Accuracy 0.8056\n",
      "Time taken for 1 epoch: 76.03 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.5574 Accuracy 0.8091\n",
      "Epoch 15 Batch 50 Loss 0.5415 Accuracy 0.8135\n",
      "Epoch 15 Batch 100 Loss 0.5431 Accuracy 0.8129\n",
      "Epoch 15 Batch 150 Loss 0.5455 Accuracy 0.8117\n",
      "Epoch 15 Batch 200 Loss 0.5472 Accuracy 0.8109\n",
      "Epoch 15 Batch 250 Loss 0.5484 Accuracy 0.8105\n",
      "Epoch 15 Batch 300 Loss 0.5493 Accuracy 0.8102\n",
      "Epoch 15 Batch 350 Loss 0.5506 Accuracy 0.8097\n",
      "Epoch 15 Batch 400 Loss 0.5512 Accuracy 0.8095\n",
      "Epoch 15 Loss 0.5520 Accuracy 0.8093\n",
      "Time taken for 1 epoch: 75.67 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5511 Accuracy 0.8087\n",
      "Epoch 16 Batch 50 Loss 0.5331 Accuracy 0.8157\n",
      "Epoch 16 Batch 100 Loss 0.5342 Accuracy 0.8151\n",
      "Epoch 16 Batch 150 Loss 0.5352 Accuracy 0.8149\n",
      "Epoch 16 Batch 200 Loss 0.5367 Accuracy 0.8145\n",
      "Epoch 16 Batch 250 Loss 0.5383 Accuracy 0.8138\n",
      "Epoch 16 Batch 300 Loss 0.5393 Accuracy 0.8135\n",
      "Epoch 16 Batch 350 Loss 0.5401 Accuracy 0.8132\n",
      "Epoch 16 Batch 400 Loss 0.5409 Accuracy 0.8130\n",
      "Epoch 16 Loss 0.5415 Accuracy 0.8128\n",
      "Time taken for 1 epoch: 76.01 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5035 Accuracy 0.8211\n",
      "Epoch 17 Batch 50 Loss 0.5167 Accuracy 0.8210\n",
      "Epoch 17 Batch 100 Loss 0.5200 Accuracy 0.8203\n",
      "Epoch 17 Batch 150 Loss 0.5235 Accuracy 0.8191\n",
      "Epoch 17 Batch 200 Loss 0.5251 Accuracy 0.8184\n",
      "Epoch 17 Batch 250 Loss 0.5263 Accuracy 0.8179\n",
      "Epoch 17 Batch 300 Loss 0.5281 Accuracy 0.8174\n",
      "Epoch 17 Batch 350 Loss 0.5293 Accuracy 0.8169\n",
      "Epoch 17 Batch 400 Loss 0.5304 Accuracy 0.8165\n",
      "Epoch 17 Loss 0.5309 Accuracy 0.8163\n",
      "Time taken for 1 epoch: 75.98 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5063 Accuracy 0.8252\n",
      "Epoch 18 Batch 50 Loss 0.5071 Accuracy 0.8242\n",
      "Epoch 18 Batch 100 Loss 0.5092 Accuracy 0.8243\n",
      "Epoch 18 Batch 150 Loss 0.5137 Accuracy 0.8227\n",
      "Epoch 18 Batch 200 Loss 0.5162 Accuracy 0.8218\n",
      "Epoch 18 Batch 250 Loss 0.5179 Accuracy 0.8212\n",
      "Epoch 18 Batch 300 Loss 0.5194 Accuracy 0.8205\n",
      "Epoch 18 Batch 350 Loss 0.5201 Accuracy 0.8202\n",
      "Epoch 18 Batch 400 Loss 0.5209 Accuracy 0.8198\n",
      "Epoch 18 Loss 0.5214 Accuracy 0.8197\n",
      "Time taken for 1 epoch: 75.97 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5003 Accuracy 0.8271\n",
      "Epoch 19 Batch 50 Loss 0.4977 Accuracy 0.8282\n",
      "Epoch 19 Batch 100 Loss 0.5019 Accuracy 0.8266\n",
      "Epoch 19 Batch 150 Loss 0.5048 Accuracy 0.8256\n",
      "Epoch 19 Batch 200 Loss 0.5066 Accuracy 0.8249\n",
      "Epoch 19 Batch 250 Loss 0.5077 Accuracy 0.8245\n",
      "Epoch 19 Batch 300 Loss 0.5089 Accuracy 0.8241\n",
      "Epoch 19 Batch 350 Loss 0.5102 Accuracy 0.8236\n",
      "Epoch 19 Batch 400 Loss 0.5107 Accuracy 0.8233\n",
      "Epoch 19 Loss 0.5115 Accuracy 0.8231\n",
      "Time taken for 1 epoch: 75.99 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.4571 Accuracy 0.8451\n",
      "Epoch 20 Batch 50 Loss 0.4896 Accuracy 0.8302\n",
      "Epoch 20 Batch 100 Loss 0.4908 Accuracy 0.8298\n",
      "Epoch 20 Batch 150 Loss 0.4944 Accuracy 0.8287\n",
      "Epoch 20 Batch 200 Loss 0.4968 Accuracy 0.8279\n",
      "Epoch 20 Batch 250 Loss 0.4982 Accuracy 0.8274\n",
      "Epoch 20 Batch 300 Loss 0.4994 Accuracy 0.8270\n",
      "Epoch 20 Batch 350 Loss 0.5003 Accuracy 0.8267\n",
      "Epoch 20 Batch 400 Loss 0.5016 Accuracy 0.8263\n",
      "Epoch 20 Loss 0.5023 Accuracy 0.8261\n",
      "Time taken for 1 epoch: 76.03 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4763 Accuracy 0.8331\n",
      "Epoch 21 Batch 50 Loss 0.4820 Accuracy 0.8336\n",
      "Epoch 21 Batch 100 Loss 0.4842 Accuracy 0.8329\n",
      "Epoch 21 Batch 150 Loss 0.4859 Accuracy 0.8320\n",
      "Epoch 21 Batch 200 Loss 0.4879 Accuracy 0.8313\n",
      "Epoch 21 Batch 250 Loss 0.4891 Accuracy 0.8308\n",
      "Epoch 21 Batch 300 Loss 0.4903 Accuracy 0.8303\n",
      "Epoch 21 Batch 350 Loss 0.4912 Accuracy 0.8300\n",
      "Epoch 21 Batch 400 Loss 0.4926 Accuracy 0.8295\n",
      "Epoch 21 Loss 0.4932 Accuracy 0.8293\n",
      "Time taken for 1 epoch: 75.83 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.4627 Accuracy 0.8423\n",
      "Epoch 22 Batch 50 Loss 0.4741 Accuracy 0.8362\n",
      "Epoch 22 Batch 100 Loss 0.4763 Accuracy 0.8346\n",
      "Epoch 22 Batch 150 Loss 0.4780 Accuracy 0.8342\n",
      "Epoch 22 Batch 200 Loss 0.4802 Accuracy 0.8334\n",
      "Epoch 22 Batch 250 Loss 0.4813 Accuracy 0.8332\n",
      "Epoch 22 Batch 300 Loss 0.4821 Accuracy 0.8329\n",
      "Epoch 22 Batch 350 Loss 0.4830 Accuracy 0.8325\n",
      "Epoch 22 Batch 400 Loss 0.4833 Accuracy 0.8324\n",
      "Epoch 22 Loss 0.4841 Accuracy 0.8322\n",
      "Time taken for 1 epoch: 75.89 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.4594 Accuracy 0.8421\n",
      "Epoch 23 Batch 50 Loss 0.4653 Accuracy 0.8384\n",
      "Epoch 23 Batch 100 Loss 0.4660 Accuracy 0.8381\n",
      "Epoch 23 Batch 150 Loss 0.4680 Accuracy 0.8374\n",
      "Epoch 23 Batch 200 Loss 0.4695 Accuracy 0.8369\n",
      "Epoch 23 Batch 250 Loss 0.4709 Accuracy 0.8364\n",
      "Epoch 23 Batch 300 Loss 0.4717 Accuracy 0.8362\n",
      "Epoch 23 Batch 350 Loss 0.4725 Accuracy 0.8359\n",
      "Epoch 23 Batch 400 Loss 0.4736 Accuracy 0.8355\n",
      "Epoch 23 Loss 0.4745 Accuracy 0.8352\n",
      "Time taken for 1 epoch: 76.13 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4405 Accuracy 0.8481\n",
      "Epoch 24 Batch 50 Loss 0.4546 Accuracy 0.8426\n",
      "Epoch 24 Batch 100 Loss 0.4586 Accuracy 0.8412\n",
      "Epoch 24 Batch 150 Loss 0.4604 Accuracy 0.8405\n",
      "Epoch 24 Batch 200 Loss 0.4623 Accuracy 0.8397\n",
      "Epoch 24 Batch 250 Loss 0.4633 Accuracy 0.8393\n",
      "Epoch 24 Batch 300 Loss 0.4644 Accuracy 0.8390\n",
      "Epoch 24 Batch 350 Loss 0.4653 Accuracy 0.8386\n",
      "Epoch 24 Batch 400 Loss 0.4661 Accuracy 0.8383\n",
      "Epoch 24 Loss 0.4669 Accuracy 0.8381\n",
      "Time taken for 1 epoch: 76.09 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.4466 Accuracy 0.8454\n",
      "Epoch 25 Batch 50 Loss 0.4475 Accuracy 0.8453\n",
      "Epoch 25 Batch 100 Loss 0.4511 Accuracy 0.8436\n",
      "Epoch 25 Batch 150 Loss 0.4522 Accuracy 0.8432\n",
      "Epoch 25 Batch 200 Loss 0.4537 Accuracy 0.8426\n",
      "Epoch 25 Batch 250 Loss 0.4550 Accuracy 0.8422\n",
      "Epoch 25 Batch 300 Loss 0.4566 Accuracy 0.8418\n",
      "Epoch 25 Batch 350 Loss 0.4574 Accuracy 0.8415\n",
      "Epoch 25 Batch 400 Loss 0.4585 Accuracy 0.8411\n",
      "Epoch 25 Loss 0.4592 Accuracy 0.8408\n",
      "Time taken for 1 epoch: 76.05 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.4497 Accuracy 0.8427\n",
      "Epoch 26 Batch 50 Loss 0.4359 Accuracy 0.8491\n",
      "Epoch 26 Batch 100 Loss 0.4411 Accuracy 0.8470\n",
      "Epoch 26 Batch 150 Loss 0.4432 Accuracy 0.8461\n",
      "Epoch 26 Batch 200 Loss 0.4454 Accuracy 0.8454\n",
      "Epoch 26 Batch 250 Loss 0.4465 Accuracy 0.8450\n",
      "Epoch 26 Batch 300 Loss 0.4478 Accuracy 0.8446\n",
      "Epoch 26 Batch 350 Loss 0.4487 Accuracy 0.8443\n",
      "Epoch 26 Batch 400 Loss 0.4495 Accuracy 0.8440\n",
      "Epoch 26 Loss 0.4502 Accuracy 0.8438\n",
      "Time taken for 1 epoch: 75.80 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4415 Accuracy 0.8431\n",
      "Epoch 27 Batch 50 Loss 0.4308 Accuracy 0.8518\n",
      "Epoch 27 Batch 100 Loss 0.4331 Accuracy 0.8506\n",
      "Epoch 27 Batch 150 Loss 0.4358 Accuracy 0.8494\n",
      "Epoch 27 Batch 200 Loss 0.4380 Accuracy 0.8485\n",
      "Epoch 27 Batch 250 Loss 0.4392 Accuracy 0.8479\n",
      "Epoch 27 Batch 300 Loss 0.4406 Accuracy 0.8473\n",
      "Epoch 27 Batch 350 Loss 0.4414 Accuracy 0.8469\n",
      "Epoch 27 Batch 400 Loss 0.4423 Accuracy 0.8466\n",
      "Epoch 27 Loss 0.4430 Accuracy 0.8463\n",
      "Time taken for 1 epoch: 75.93 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.4090 Accuracy 0.8514\n",
      "Epoch 28 Batch 50 Loss 0.4235 Accuracy 0.8532\n",
      "Epoch 28 Batch 100 Loss 0.4271 Accuracy 0.8518\n",
      "Epoch 28 Batch 150 Loss 0.4292 Accuracy 0.8510\n",
      "Epoch 28 Batch 200 Loss 0.4299 Accuracy 0.8506\n",
      "Epoch 28 Batch 250 Loss 0.4314 Accuracy 0.8500\n",
      "Epoch 28 Batch 300 Loss 0.4327 Accuracy 0.8496\n",
      "Epoch 28 Batch 350 Loss 0.4336 Accuracy 0.8493\n",
      "Epoch 28 Batch 400 Loss 0.4351 Accuracy 0.8489\n",
      "Epoch 28 Loss 0.4358 Accuracy 0.8486\n",
      "Time taken for 1 epoch: 75.90 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.4044 Accuracy 0.8572\n",
      "Epoch 29 Batch 50 Loss 0.4171 Accuracy 0.8552\n",
      "Epoch 29 Batch 100 Loss 0.4195 Accuracy 0.8543\n",
      "Epoch 29 Batch 150 Loss 0.4209 Accuracy 0.8538\n",
      "Epoch 29 Batch 200 Loss 0.4233 Accuracy 0.8529\n",
      "Epoch 29 Batch 250 Loss 0.4242 Accuracy 0.8526\n",
      "Epoch 29 Batch 300 Loss 0.4253 Accuracy 0.8523\n",
      "Epoch 29 Batch 350 Loss 0.4264 Accuracy 0.8519\n",
      "Epoch 29 Batch 400 Loss 0.4277 Accuracy 0.8515\n",
      "Epoch 29 Loss 0.4285 Accuracy 0.8512\n",
      "Time taken for 1 epoch: 75.86 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.3916 Accuracy 0.8660\n",
      "Epoch 30 Batch 50 Loss 0.4106 Accuracy 0.8573\n",
      "Epoch 30 Batch 100 Loss 0.4129 Accuracy 0.8568\n",
      "Epoch 30 Batch 150 Loss 0.4154 Accuracy 0.8559\n",
      "Epoch 30 Batch 200 Loss 0.4163 Accuracy 0.8556\n",
      "Epoch 30 Batch 250 Loss 0.4181 Accuracy 0.8548\n",
      "Epoch 30 Batch 300 Loss 0.4191 Accuracy 0.8544\n",
      "Epoch 30 Batch 350 Loss 0.4199 Accuracy 0.8541\n",
      "Epoch 30 Batch 400 Loss 0.4208 Accuracy 0.8537\n",
      "Epoch 30 Loss 0.4215 Accuracy 0.8534\n",
      "Time taken for 1 epoch: 75.90 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    transformer,\n",
    "    encoder_input,\n",
    "    decoder_input,\n",
    "    stop_symbol,\n",
    "    max_length=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts the output of the model given the `input_sequence`.\n",
    "    The `input_sequence` is encoded by the Encoder, then its output is fed to the Decoder,\n",
    "    whose output is fed back into the Decoder until the `stop_symbol` token is produced.\n",
    "\n",
    "    This function works with a batch of inputs and stops when all outputs include a stop symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    output = decoder_input\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output\n",
    "    )\n",
    "\n",
    "    enc_output = transformer.encoder(\n",
    "        encoder_input, False, enc_padding_mask\n",
    "    )  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, _ = transformer.decoder(\n",
    "            output, enc_output, False, combined_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = transformer.final_layer(dec_output)\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predicted_ids = tf.argmax(predictions[:, -1:, :], axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [\n",
    "                tf.cast(output, dtype=tf.int32),\n",
    "                tf.cast(predicted_ids, dtype=tf.int32),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "                \n",
    "        if sum(output.numpy()[0] == stop_symbol) == 4:\n",
    "            print('Stopped')\n",
    "            return output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(transformer, input_sequence, target_sequence, input_tokenizer, target_tokenizer, steps, start_symbol, stop_symbol):\n",
    "\n",
    "    result = target_tokenizer.sequences_to_texts(target_sequence)[0]\n",
    "    \n",
    "    encoder_input = input_sequence\n",
    "    decoder_input = target_sequence\n",
    "\n",
    "    for _ in range(steps):\n",
    "\n",
    "        encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "        output = evaluate(transformer, encoder_input, decoder_input, stop_symbol)\n",
    "\n",
    "        generated_text = target_tokenizer.sequences_to_texts(output.numpy())[0]\n",
    "        \n",
    "        verses = [line.lstrip() + '<EOV> ' for line in generated_text.split('<EOV>') if line.strip() != '']\n",
    "        \n",
    "        result = ''.join([result, verses[-1]])\n",
    "                \n",
    "        verses = ''.join(verses[-3:])\n",
    "        \n",
    "        decoder_input = target_tokenizer.texts_to_sequences([verses])\n",
    "        \n",
    "        verses = remove_syll_token(verses)\n",
    "        verses = re.sub(r\"[ ]+\", \"\", verses)\n",
    "        verses = re.sub(\"<[^>]*>\", \" \\g<0> \", verses)\n",
    "        verses = re.sub(\"<EOV>  <GO>\", \"<EOV> <GO>\", verses)\n",
    "        verses = verses.strip()\n",
    "\n",
    "        encoder_input = input_tokenizer.texts_to_sequences([verses])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
    "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]\n",
    "\n",
    "encoder_input = [input_text[0]]\n",
    "decoder_input = [target_text_tercet[0]]\n",
    "\n",
    "result = generate(transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Nel |mez|zo |del |cam|min |di |no|stra |vi|ta\n",
      "|mi |ri|tro|vai |per |u|na |sel|va o|scu|ra,\n",
      "|ché |la |di|rit|ta |via |e|ra |smar|ri|ta.\n",
      "|El|la |so|pra |che ’l |vi|so a |quel|la |gen|te\n",
      "|che |per |lo |suo |av|ver|sa|rio al|trui |man|to,\n",
      "|e |al|tra |vo|ce |mi |pa|rea |più |rat|ta.\n",
      "« |O |tu |che |se’ |che |sì |pres|so |di|sciol|ta»,\n",
      "|dis|se ’l |ma|e|stro,« |quan|to |pos|so |po|co,\n",
      "|se |non |che |tu |se’ |tem|pion |far |non |la|ti,\n"
     ]
    }
   ],
   "source": [
    "print(strip_tokens(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Syllabification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = target_tokenizer.word_index[\"<GO>\"]\n",
    "stop_symbol = target_tokenizer.word_index[\"<EOV>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = tf.convert_to_tensor([input_text[0]])\n",
    "decoder_input = tf.convert_to_tensor([[start_symbol]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "syll_output = evaluate(transformer, encoder_input, decoder_input, stop_symbol, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<GO> | c h e <SEP> | d i <SEP> | p e n | s i e r <SEP> | m i <SEP> | s t a | v a <SEP> i n <SEP> | u | n o <SEP> | s t r a | l e , <EOV> <GO> | e <SEP> | d i | c o <SEP> | d i <SEP> | g e n | t e <SEP> a l | t r o <SEP> | c h e <SEP> | p i ù <SEP> | d o l | c e » . <EOV> <GO> | N o i <SEP> | e | r a | v a m <SEP> | n e l <SEP> | s u o <SEP> | a | s p e t | t o <SEP> | b a n | d o <EOV> <GO> | c h e <SEP> | l ’ <SEP> a | n i | m a <SEP> | s u a <SEP> | a v | v e n | t a <SEP> | d i | s t a n | t e , <EOV>']\n"
     ]
    }
   ],
   "source": [
    "print(target_tokenizer.sequences_to_texts(syll_output.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potrebbe essere underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save_weights('models/w2c-gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transformer = Transformer(\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        dff=config[\"dff\"],\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=1000,\n",
    "        pe_target=1000,\n",
    "        rate=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to load the new weights the model should be called once for the variables to be initialized\n",
    "\n",
    "# Any inp, tar is ok here\n",
    "inp = tf.convert_to_tensor([[start_symbol]])\n",
    "tar = tf.convert_to_tensor([[start_symbol]])\n",
    "\n",
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "\n",
    "new_transformer(inp, tar, False, enc_padding_mask, look_ahead_mask, dec_padding_mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transformer.load_weights('models/w2c-gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n",
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "encoder_input = [input_text[0]]\n",
    "decoder_input = [target_text_tercet[0]]\n",
    "\n",
    "result = generate(new_transformer, encoder_input, decoder_input, input_tokenizer, target_tokenizer, 6, start_symbol, stop_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> | N e l <SEP> | m e z | z o <SEP> | d e l <SEP> | c a m | m i n <SEP> | d i <SEP> | n o | s t r a <SEP> | v i | t a <EOV> <GO> | m i <SEP> | r i | t r o | v a i <SEP> | p e r <SEP> | u | n a <SEP> | s e l | v a <SEP> o | s c u | r a , <EOV> <GO> | c h é <SEP> | l a <SEP> | d i | r i t | t a <SEP> | v i a <SEP> | e | r a <SEP> | s m a r | r i | t a . <EOV><GO> | E l | l a <SEP> | s o | p r a <SEP> | c h e <SEP> ’ l <SEP> | v i | s o <SEP> a <SEP> | q u e l | l a <SEP> | g e n | t e <EOV> <GO> | c h e <SEP> | p e r <SEP> | l o <SEP> | s u o <SEP> | a v | v e r | s a | r i o <SEP> a l | t r u i <SEP> | m a n | t o , <EOV> <GO> | e <SEP> | a l | t r a <SEP> | v o | c e <SEP> | m i <SEP> | p a | r e a <SEP> | p i ù <SEP> | r a t | t a . <EOV> <GO> « <SEP> | O <SEP> | t u <SEP> | c h e <SEP> | s e ’ <SEP> | c h e <SEP> | s ì <SEP> | p r e s | s o <SEP> | d i | s c i o l | t a » , <EOV> <GO> | d i s | s e <SEP> ’ l <SEP> | m a | e | s t r o , « <SEP> | q u a n | t o <SEP> | p o s | s o <SEP> | p o | c o , <EOV> <GO> | s e <SEP> | n o n <SEP> | c h e <SEP> | t u <SEP> | s e ’ <SEP> | t e m | p i o n <SEP> | f a r <SEP> | n o n <SEP> | l a | t i , <EOV> '"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word-input char-output transformer generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
