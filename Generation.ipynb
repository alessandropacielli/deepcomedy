{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep Comedy transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "input_file = 'data/divina_textonly.txt'\n",
        "target_file = 'data/divina_syll_textonly.txt'"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b474c5-4105-46c3-83c4-a29e7b6067f3"
      },
      "source": [
        "input_text_raw = open(input_file, 'rb').read().decode(encoding='utf-8')\n",
        "target_text_raw = open(target_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of input text: {} characters'.format(len(input_text_raw)))\n",
        "print('Length of target text: {} characters'.format(len(target_text_raw)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input text: 578077 characters\n",
            "Length of target text: 892871 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "l1G45yR9Y1da"
      },
      "source": [
        "input_vocab = sorted(set(input_text_raw))\n",
        "target_vocab = sorted(set(target_text_raw))\n",
        "input_vocab_size = len(input_vocab)\n",
        "target_vocab_size = len(target_vocab)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "aa379dbd-aec3-4e03-c4a9-256f0f4f2e2d"
      },
      "source": [
        "print('Input vocab size: {}'.format(input_vocab_size))\n",
        "print('Target vocab size: {}'.format(target_vocab_size))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 80\n",
            "Target vocab size: 81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jmfE9GWu4KW"
      },
      "source": [
        "The *preprocess* function adds the start and end symbols to each line and eliminates the empty ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "SUsvn6SqY1dd"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
        "    \"\"\"\n",
        "    return ['^' + line.strip() + '$' for line in text.split('\\n') if line.strip() != '']\n",
        "\n",
        "input_text_prepr = preprocess(input_text_raw)\n",
        "target_text_prepr = preprocess(target_text_raw)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMrXOt8uvC6j"
      },
      "source": [
        "The tokenizer encodes each line into a tensor of char-indexes and for simplicity fits only on the target's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "ABb_2K6DY1de"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True, lower=False)\n",
        "tokenizer.fit_on_texts(target_text_prepr)\n",
        "\n",
        "input_text_lines_enc = tokenizer.texts_to_sequences(input_text_prepr)\n",
        "target_text_lines_enc = tokenizer.texts_to_sequences(target_text_prepr)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80BNTndBceOL",
        "outputId": "9210a991-1c4a-4b5f-c164-e57af8626fe0"
      },
      "source": [
        "input_text = np.array([np.array(x) for x in input_text_lines_enc])\n",
        "target_text = np.array([np.array(x) for x in target_text_lines_enc])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UT1B2BBUPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8739fe62-e95c-46c5-85de-34fc2eb5fbde"
      },
      "source": [
        "input_text_ = []\n",
        "target_text_ = []\n",
        "\n",
        "for line_number in range(0, len(input_text) - 6):\n",
        "    \n",
        "    input_verses = []\n",
        "    target_verses = []\n",
        "    \n",
        "    for i in range(3):\n",
        "\n",
        "        input_verses += list(input_text[line_number + i])\n",
        "        target_verses += list(target_text[line_number + 3 + i])\n",
        "    \n",
        "    input_text_.append(input_verses)\n",
        "    target_text_.append(target_verses)\n",
        "\n",
        "    line_number+=3\n",
        "    \n",
        "input_text_ = np.array(input_text_)\n",
        "target_text_ = np.array(target_text_)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOqF5QJZc5wp",
        "outputId": "07231657-26e7-4626-ce0d-fe610eaebbd4"
      },
      "source": [
        "x = [tokenizer.index_word[i] for i in list(input_text_[0])]\n",
        "x"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['^',\n",
              " 'N',\n",
              " 'e',\n",
              " 'l',\n",
              " ' ',\n",
              " 'm',\n",
              " 'e',\n",
              " 'z',\n",
              " 'z',\n",
              " 'o',\n",
              " ' ',\n",
              " 'd',\n",
              " 'e',\n",
              " 'l',\n",
              " ' ',\n",
              " 'c',\n",
              " 'a',\n",
              " 'm',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " ' ',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'o',\n",
              " 's',\n",
              " 't',\n",
              " 'r',\n",
              " 'a',\n",
              " ' ',\n",
              " 'v',\n",
              " 'i',\n",
              " 't',\n",
              " 'a',\n",
              " '$',\n",
              " '^',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'r',\n",
              " 'i',\n",
              " 't',\n",
              " 'r',\n",
              " 'o',\n",
              " 'v',\n",
              " 'a',\n",
              " 'i',\n",
              " ' ',\n",
              " 'p',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 'u',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 's',\n",
              " 'e',\n",
              " 'l',\n",
              " 'v',\n",
              " 'a',\n",
              " ' ',\n",
              " 'o',\n",
              " 's',\n",
              " 'c',\n",
              " 'u',\n",
              " 'r',\n",
              " 'a',\n",
              " ',',\n",
              " '$',\n",
              " '^',\n",
              " 'c',\n",
              " 'h',\n",
              " 'é',\n",
              " ' ',\n",
              " 'l',\n",
              " 'a',\n",
              " ' ',\n",
              " 'd',\n",
              " 'i',\n",
              " 'r',\n",
              " 'i',\n",
              " 't',\n",
              " 't',\n",
              " 'a',\n",
              " ' ',\n",
              " 'v',\n",
              " 'i',\n",
              " 'a',\n",
              " ' ',\n",
              " 'e',\n",
              " 'r',\n",
              " 'a',\n",
              " ' ',\n",
              " 's',\n",
              " 'm',\n",
              " 'a',\n",
              " 'r',\n",
              " 'r',\n",
              " 'i',\n",
              " 't',\n",
              " 'a',\n",
              " '.',\n",
              " '$']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrpGRJcDerpJ",
        "outputId": "70342eff-2d85-4855-8861-eb572d83cf10"
      },
      "source": [
        "y = [tokenizer.index_word[i] for i in list(target_text_[0])]\n",
        "y"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['^',\n",
              " '|',\n",
              " 'A',\n",
              " 'h',\n",
              " 'i',\n",
              " ' ',\n",
              " '|',\n",
              " 'q',\n",
              " 'u',\n",
              " 'a',\n",
              " 'n',\n",
              " '|',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 'd',\n",
              " 'i',\n",
              " 'r',\n",
              " ' ',\n",
              " '|',\n",
              " 'q',\n",
              " 'u',\n",
              " 'a',\n",
              " 'l',\n",
              " ' ',\n",
              " '|',\n",
              " 'e',\n",
              " '|',\n",
              " 'r',\n",
              " 'a',\n",
              " ' ',\n",
              " 'è',\n",
              " ' ',\n",
              " '|',\n",
              " 'c',\n",
              " 'o',\n",
              " '|',\n",
              " 's',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 'd',\n",
              " 'u',\n",
              " '|',\n",
              " 'r',\n",
              " 'a',\n",
              " '$',\n",
              " '^',\n",
              " '|',\n",
              " 'e',\n",
              " '|',\n",
              " 's',\n",
              " 't',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 's',\n",
              " 'e',\n",
              " 'l',\n",
              " '|',\n",
              " 'v',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 's',\n",
              " 'e',\n",
              " 'l',\n",
              " '|',\n",
              " 'v',\n",
              " 'a',\n",
              " 'g',\n",
              " '|',\n",
              " 'g',\n",
              " 'i',\n",
              " 'a',\n",
              " ' ',\n",
              " 'e',\n",
              " ' ',\n",
              " '|',\n",
              " 'a',\n",
              " '|',\n",
              " 's',\n",
              " 'p',\n",
              " 'r',\n",
              " 'a',\n",
              " ' ',\n",
              " 'e',\n",
              " ' ',\n",
              " '|',\n",
              " 'f',\n",
              " 'o',\n",
              " 'r',\n",
              " '|',\n",
              " 't',\n",
              " 'e',\n",
              " '$',\n",
              " '^',\n",
              " '|',\n",
              " 'c',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " '|',\n",
              " 'n',\n",
              " 'e',\n",
              " 'l',\n",
              " ' ',\n",
              " '|',\n",
              " 'p',\n",
              " 'e',\n",
              " 'n',\n",
              " '|',\n",
              " 's',\n",
              " 'i',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " '|',\n",
              " 'r',\n",
              " 'i',\n",
              " '|',\n",
              " 'n',\n",
              " 'o',\n",
              " '|',\n",
              " 'v',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 'l',\n",
              " 'a',\n",
              " ' ',\n",
              " '|',\n",
              " 'p',\n",
              " 'a',\n",
              " '|',\n",
              " 'u',\n",
              " '|',\n",
              " 'r',\n",
              " 'a',\n",
              " '!',\n",
              " '$']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bfzvcNvT-y"
      },
      "source": [
        "Padding is required in order to have a non-ragged tensor to feed to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "MBOh9LQeY1dg"
      },
      "source": [
        "def pad(x):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding='post') "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "9zV0xz48Y1dh"
      },
      "source": [
        "_input_text = pad(input_text_)\n",
        "_target_text = pad(target_text_)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7xGxZmlPY1dk"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    _input_text, _target_text\n",
        "    )"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IN8x175vimK"
      },
      "source": [
        "The dataset is created by grouping the lines in batches and by shuffling them.\n",
        "\n",
        "Each input's line is in correspondence with its target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_train)//BATCH_SIZE\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1 # the +1 is added to take into account the id 0 of the padding\n",
        "\n",
        "max_length_targ, max_length_inp = _target_text.shape[1], _input_text.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JvnEW-PflFx",
        "outputId": "a37c660b-0981-4b2f-ab45-e20cdadc0689"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 137), (32, 173)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHNAazT5Rs_"
      },
      "source": [
        "We define the positional encoding to add to the embedding.\n",
        "\n",
        "This allows to take into account the order of the characters in the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "f200V0QnkBBS"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OvnGjGhvkD9R"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500eU4tu6n-g"
      },
      "source": [
        "We define two masks: \n",
        "\n",
        "one is used to mask the padding added to the sequences in the preprocessing step; \n",
        "\n",
        "the other one is used to mask the positions following the current one and not predicted yet;\n",
        "\n",
        "The first mask is used from both the encoder and the decoder, while the last mask is used only in the self-attention of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OVwx6Y4Tku1V"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "3p1-yIYimnvB"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "R-Q4J7EzfuLH"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by\n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxzWROrTM9ib"
      },
      "source": [
        "The *scaled_dot_product_attention* gets the attention weights by applying the softmax to the rescaled dot product between the query matrix and the key matrix, while the output is obtained by multiplying the value matrix for those attention weights.\n",
        "\n",
        "The query, key and value matrices are built by multiplying the embedding matrix with the query, key and value weight matrices, which initially are randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "RoFZK1S3mtI5"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"\n",
        "  Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlA3inNYQO89"
      },
      "source": [
        "The multi-headed attention allows to improve the performance of the attention mechanism by working with multiple sets of query, key and value weight matrices.\n",
        "\n",
        "These heads work in parallel and process at the same time all the lines of each batch.\n",
        "\n",
        "At the end, the results of all the attention heads are concatenated and multiplied by an additional weight matrix, to adjust the dimension before passing through the final *point_wise_feed_forward_network*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "_UpdBWkVnK02"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"\n",
        "    Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LkMP7DDAok4y"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "      ])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO6yyZPWUON7"
      },
      "source": [
        "Each encoder is constituted by a multi-headed self-attention layer and by a final feed forward layer. \n",
        "\n",
        "Both sub-layers have a residual connection around them and are followed by a layer-normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "Dat64C18otwC"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZslNuSZOKp"
      },
      "source": [
        "The decoder equals the encoder, a part from the fact that it contains a slightly different self-attention layer and an additional attention layer.\n",
        "\n",
        "Indeed, the decoder is characterized by a self-attention layer which focuses only on earlier positions in its input sequence, not looking at the positions which have not been predicted yet.\n",
        "\n",
        "What's more the decoder is also characterized by an attention layer which obtains its key and value matrices from the output of the encoder, while the query matrix is obtained from the output of the previous self-attention in the decoder.\n",
        "\n",
        "The encoder-decoder attention helps the decoder to focus on appropriate positions in the input sequence of the encoder during the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7Vp44lQepI_P"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx_DyJiybrOr"
      },
      "source": [
        "The encoding component is a stack of encoders and the decoding component is a stack of decoders of the same number.\n",
        "\n",
        "At the beginning, in the encoding, each input character is turned into a vector using an embedding algorithm and adding the positional encoding to it.\n",
        "\n",
        "This happens only in the bottom-most encoder, while the following encoders take the output of the encoder which is directly below.\n",
        "\n",
        "The same for the decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "awl9kiESpWBh"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "47tQAEMwpnUj"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TosN_TpKk1eN"
      },
      "source": [
        "In the transformer, the output of the encoding is passed to the stack of decoders and the output of the decoding is projected by a feed forward network into a vector of logits of dimension equal to the one of the target's vocabulary.\n",
        "\n",
        "Obviously this is done for each character of each line of each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "qCNKKsQ-p99k"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LrdL396xqOL4"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "JFCVQIDjqQHv"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "1R9MlFs0qc5U"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcmLAk1Ut8vG"
      },
      "source": [
        "The loss is calculated using Sparse Categorical Crossentropy and the loss of the padding is masked.\n",
        "\n",
        "The same is done for the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "TBAaRBPsqkuo"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "31R26t9wqlLD"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "SkVkWvL7qoYu"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "5UE3cWGVqvnS"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size,\n",
        "    target_vocab_size=vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVGgP8Su9MH"
      },
      "source": [
        "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "n_VPs6ZOva15"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "1ce0FAOivleY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daffcc44-bd70-4f63-d9e1-e4d8879fa95e"
      },
      "source": [
        "EPOCHS=50\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(dataset):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.8302 Accuracy 0.0017\n",
            "Epoch 1 Batch 50 Loss 3.8595 Accuracy 0.1387\n",
            "Epoch 1 Batch 100 Loss 3.4567 Accuracy 0.1771\n",
            "Epoch 1 Batch 150 Loss 3.2161 Accuracy 0.2049\n",
            "Epoch 1 Batch 200 Loss 2.9836 Accuracy 0.2372\n",
            "Epoch 1 Batch 250 Loss 2.8079 Accuracy 0.2604\n",
            "Epoch 1 Batch 300 Loss 2.6798 Accuracy 0.2773\n",
            "Epoch 1 Loss 2.6144 Accuracy 0.2860\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9846 Accuracy 0.3574\n",
            "Epoch 2 Batch 50 Loss 1.9700 Accuracy 0.3706\n",
            "Epoch 2 Batch 100 Loss 1.9574 Accuracy 0.3732\n",
            "Epoch 2 Batch 150 Loss 1.9458 Accuracy 0.3760\n",
            "Epoch 2 Batch 200 Loss 1.9321 Accuracy 0.3793\n",
            "Epoch 2 Batch 250 Loss 1.9188 Accuracy 0.3823\n",
            "Epoch 2 Batch 300 Loss 1.9041 Accuracy 0.3860\n",
            "Epoch 2 Loss 1.8934 Accuracy 0.3888\n",
            "Time taken for 1 epoch: 51.47 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7923 Accuracy 0.4119\n",
            "Epoch 3 Batch 50 Loss 1.7700 Accuracy 0.4211\n",
            "Epoch 3 Batch 100 Loss 1.7517 Accuracy 0.4258\n",
            "Epoch 3 Batch 150 Loss 1.7354 Accuracy 0.4307\n",
            "Epoch 3 Batch 200 Loss 1.7211 Accuracy 0.4349\n",
            "Epoch 3 Batch 250 Loss 1.7069 Accuracy 0.4393\n",
            "Epoch 3 Batch 300 Loss 1.6931 Accuracy 0.4434\n",
            "Epoch 3 Loss 1.6849 Accuracy 0.4461\n",
            "Time taken for 1 epoch: 51.74 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5946 Accuracy 0.4820\n",
            "Epoch 4 Batch 50 Loss 1.5831 Accuracy 0.4756\n",
            "Epoch 4 Batch 100 Loss 1.5721 Accuracy 0.4800\n",
            "Epoch 4 Batch 150 Loss 1.5625 Accuracy 0.4835\n",
            "Epoch 4 Batch 200 Loss 1.5520 Accuracy 0.4868\n",
            "Epoch 4 Batch 250 Loss 1.5429 Accuracy 0.4898\n",
            "Epoch 4 Batch 300 Loss 1.5333 Accuracy 0.4927\n",
            "Epoch 4 Loss 1.5282 Accuracy 0.4943\n",
            "Time taken for 1 epoch: 51.59 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4561 Accuracy 0.5059\n",
            "Epoch 5 Batch 50 Loss 1.4628 Accuracy 0.5146\n",
            "Epoch 5 Batch 100 Loss 1.4547 Accuracy 0.5172\n",
            "Epoch 5 Batch 150 Loss 1.4456 Accuracy 0.5198\n",
            "Epoch 5 Batch 200 Loss 1.4384 Accuracy 0.5223\n",
            "Epoch 5 Batch 250 Loss 1.4326 Accuracy 0.5242\n",
            "Epoch 5 Batch 300 Loss 1.4264 Accuracy 0.5259\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 1.4225 Accuracy 0.5273\n",
            "Time taken for 1 epoch: 51.98 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3795 Accuracy 0.5416\n",
            "Epoch 6 Batch 50 Loss 1.3703 Accuracy 0.5434\n",
            "Epoch 6 Batch 100 Loss 1.3671 Accuracy 0.5449\n",
            "Epoch 6 Batch 150 Loss 1.3599 Accuracy 0.5474\n",
            "Epoch 6 Batch 200 Loss 1.3550 Accuracy 0.5493\n",
            "Epoch 6 Batch 250 Loss 1.3495 Accuracy 0.5512\n",
            "Epoch 6 Batch 300 Loss 1.3451 Accuracy 0.5528\n",
            "Epoch 6 Loss 1.3417 Accuracy 0.5539\n",
            "Time taken for 1 epoch: 51.62 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3198 Accuracy 0.5655\n",
            "Epoch 7 Batch 50 Loss 1.2937 Accuracy 0.5709\n",
            "Epoch 7 Batch 100 Loss 1.2949 Accuracy 0.5700\n",
            "Epoch 7 Batch 150 Loss 1.2906 Accuracy 0.5715\n",
            "Epoch 7 Batch 200 Loss 1.2883 Accuracy 0.5719\n",
            "Epoch 7 Batch 250 Loss 1.2837 Accuracy 0.5736\n",
            "Epoch 7 Batch 300 Loss 1.2793 Accuracy 0.5753\n",
            "Epoch 7 Loss 1.2774 Accuracy 0.5760\n",
            "Time taken for 1 epoch: 51.60 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2519 Accuracy 0.5854\n",
            "Epoch 8 Batch 50 Loss 1.2437 Accuracy 0.5873\n",
            "Epoch 8 Batch 100 Loss 1.2387 Accuracy 0.5882\n",
            "Epoch 8 Batch 150 Loss 1.2363 Accuracy 0.5889\n",
            "Epoch 8 Batch 200 Loss 1.2356 Accuracy 0.5894\n",
            "Epoch 8 Batch 250 Loss 1.2332 Accuracy 0.5903\n",
            "Epoch 8 Batch 300 Loss 1.2298 Accuracy 0.5916\n",
            "Epoch 8 Loss 1.2287 Accuracy 0.5920\n",
            "Time taken for 1 epoch: 51.50 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1810 Accuracy 0.6077\n",
            "Epoch 9 Batch 50 Loss 1.1996 Accuracy 0.6022\n",
            "Epoch 9 Batch 100 Loss 1.1991 Accuracy 0.6019\n",
            "Epoch 9 Batch 150 Loss 1.1967 Accuracy 0.6023\n",
            "Epoch 9 Batch 200 Loss 1.1947 Accuracy 0.6027\n",
            "Epoch 9 Batch 250 Loss 1.1922 Accuracy 0.6034\n",
            "Epoch 9 Batch 300 Loss 1.1896 Accuracy 0.6040\n",
            "Epoch 9 Loss 1.1892 Accuracy 0.6042\n",
            "Time taken for 1 epoch: 51.41 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1425 Accuracy 0.6194\n",
            "Epoch 10 Batch 50 Loss 1.1638 Accuracy 0.6122\n",
            "Epoch 10 Batch 100 Loss 1.1617 Accuracy 0.6132\n",
            "Epoch 10 Batch 150 Loss 1.1609 Accuracy 0.6130\n",
            "Epoch 10 Batch 200 Loss 1.1605 Accuracy 0.6132\n",
            "Epoch 10 Batch 250 Loss 1.1591 Accuracy 0.6136\n",
            "Epoch 10 Batch 300 Loss 1.1582 Accuracy 0.6139\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 1.1572 Accuracy 0.6141\n",
            "Time taken for 1 epoch: 51.68 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.1344 Accuracy 0.6188\n",
            "Epoch 11 Batch 50 Loss 1.1348 Accuracy 0.6214\n",
            "Epoch 11 Batch 100 Loss 1.1338 Accuracy 0.6213\n",
            "Epoch 11 Batch 150 Loss 1.1332 Accuracy 0.6213\n",
            "Epoch 11 Batch 200 Loss 1.1328 Accuracy 0.6214\n",
            "Epoch 11 Batch 250 Loss 1.1322 Accuracy 0.6216\n",
            "Epoch 11 Batch 300 Loss 1.1313 Accuracy 0.6220\n",
            "Epoch 11 Loss 1.1307 Accuracy 0.6222\n",
            "Time taken for 1 epoch: 51.43 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1023 Accuracy 0.6265\n",
            "Epoch 12 Batch 50 Loss 1.1102 Accuracy 0.6292\n",
            "Epoch 12 Batch 100 Loss 1.1108 Accuracy 0.6288\n",
            "Epoch 12 Batch 150 Loss 1.1096 Accuracy 0.6290\n",
            "Epoch 12 Batch 200 Loss 1.1098 Accuracy 0.6289\n",
            "Epoch 12 Batch 250 Loss 1.1083 Accuracy 0.6294\n",
            "Epoch 12 Batch 300 Loss 1.1084 Accuracy 0.6293\n",
            "Epoch 12 Loss 1.1083 Accuracy 0.6293\n",
            "Time taken for 1 epoch: 51.48 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.0565 Accuracy 0.6459\n",
            "Epoch 13 Batch 50 Loss 1.0852 Accuracy 0.6361\n",
            "Epoch 13 Batch 100 Loss 1.0878 Accuracy 0.6356\n",
            "Epoch 13 Batch 150 Loss 1.0885 Accuracy 0.6353\n",
            "Epoch 13 Batch 200 Loss 1.0865 Accuracy 0.6360\n",
            "Epoch 13 Batch 250 Loss 1.0859 Accuracy 0.6362\n",
            "Epoch 13 Batch 300 Loss 1.0846 Accuracy 0.6365\n",
            "Epoch 13 Loss 1.0843 Accuracy 0.6366\n",
            "Time taken for 1 epoch: 51.50 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.0664 Accuracy 0.6436\n",
            "Epoch 14 Batch 50 Loss 1.0565 Accuracy 0.6446\n",
            "Epoch 14 Batch 100 Loss 1.0586 Accuracy 0.6444\n",
            "Epoch 14 Batch 150 Loss 1.0590 Accuracy 0.6446\n",
            "Epoch 14 Batch 200 Loss 1.0588 Accuracy 0.6445\n",
            "Epoch 14 Batch 250 Loss 1.0582 Accuracy 0.6445\n",
            "Epoch 14 Batch 300 Loss 1.0591 Accuracy 0.6443\n",
            "Epoch 14 Loss 1.0585 Accuracy 0.6446\n",
            "Time taken for 1 epoch: 51.51 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.0242 Accuracy 0.6504\n",
            "Epoch 15 Batch 50 Loss 1.0336 Accuracy 0.6512\n",
            "Epoch 15 Batch 100 Loss 1.0355 Accuracy 0.6505\n",
            "Epoch 15 Batch 150 Loss 1.0348 Accuracy 0.6515\n",
            "Epoch 15 Batch 200 Loss 1.0351 Accuracy 0.6516\n",
            "Epoch 15 Batch 250 Loss 1.0344 Accuracy 0.6518\n",
            "Epoch 15 Batch 300 Loss 1.0341 Accuracy 0.6519\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.0342 Accuracy 0.6519\n",
            "Time taken for 1 epoch: 51.69 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.0295 Accuracy 0.6557\n",
            "Epoch 16 Batch 50 Loss 1.0079 Accuracy 0.6587\n",
            "Epoch 16 Batch 100 Loss 1.0099 Accuracy 0.6586\n",
            "Epoch 16 Batch 150 Loss 1.0107 Accuracy 0.6585\n",
            "Epoch 16 Batch 200 Loss 1.0120 Accuracy 0.6584\n",
            "Epoch 16 Batch 250 Loss 1.0130 Accuracy 0.6580\n",
            "Epoch 16 Batch 300 Loss 1.0126 Accuracy 0.6583\n",
            "Epoch 16 Loss 1.0130 Accuracy 0.6582\n",
            "Time taken for 1 epoch: 51.37 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.9649 Accuracy 0.6742\n",
            "Epoch 17 Batch 50 Loss 0.9869 Accuracy 0.6662\n",
            "Epoch 17 Batch 100 Loss 0.9927 Accuracy 0.6646\n",
            "Epoch 17 Batch 150 Loss 0.9930 Accuracy 0.6642\n",
            "Epoch 17 Batch 200 Loss 0.9932 Accuracy 0.6642\n",
            "Epoch 17 Batch 250 Loss 0.9936 Accuracy 0.6644\n",
            "Epoch 17 Batch 300 Loss 0.9932 Accuracy 0.6646\n",
            "Epoch 17 Loss 0.9938 Accuracy 0.6644\n",
            "Time taken for 1 epoch: 51.36 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.9631 Accuracy 0.6719\n",
            "Epoch 18 Batch 50 Loss 0.9697 Accuracy 0.6713\n",
            "Epoch 18 Batch 100 Loss 0.9695 Accuracy 0.6717\n",
            "Epoch 18 Batch 150 Loss 0.9715 Accuracy 0.6715\n",
            "Epoch 18 Batch 200 Loss 0.9731 Accuracy 0.6709\n",
            "Epoch 18 Batch 250 Loss 0.9745 Accuracy 0.6704\n",
            "Epoch 18 Batch 300 Loss 0.9756 Accuracy 0.6702\n",
            "Epoch 18 Loss 0.9752 Accuracy 0.6702\n",
            "Time taken for 1 epoch: 51.30 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.9523 Accuracy 0.6733\n",
            "Epoch 19 Batch 50 Loss 0.9510 Accuracy 0.6783\n",
            "Epoch 19 Batch 100 Loss 0.9531 Accuracy 0.6778\n",
            "Epoch 19 Batch 150 Loss 0.9552 Accuracy 0.6771\n",
            "Epoch 19 Batch 200 Loss 0.9554 Accuracy 0.6767\n",
            "Epoch 19 Batch 250 Loss 0.9565 Accuracy 0.6762\n",
            "Epoch 19 Batch 300 Loss 0.9564 Accuracy 0.6763\n",
            "Epoch 19 Loss 0.9577 Accuracy 0.6759\n",
            "Time taken for 1 epoch: 51.33 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.9495 Accuracy 0.6731\n",
            "Epoch 20 Batch 50 Loss 0.9340 Accuracy 0.6842\n",
            "Epoch 20 Batch 100 Loss 0.9394 Accuracy 0.6818\n",
            "Epoch 20 Batch 150 Loss 0.9410 Accuracy 0.6814\n",
            "Epoch 20 Batch 200 Loss 0.9420 Accuracy 0.6809\n",
            "Epoch 20 Batch 250 Loss 0.9427 Accuracy 0.6805\n",
            "Epoch 20 Batch 300 Loss 0.9420 Accuracy 0.6807\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 0.9417 Accuracy 0.6808\n",
            "Time taken for 1 epoch: 51.69 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.9089 Accuracy 0.6915\n",
            "Epoch 21 Batch 50 Loss 0.9234 Accuracy 0.6862\n",
            "Epoch 21 Batch 100 Loss 0.9230 Accuracy 0.6867\n",
            "Epoch 21 Batch 150 Loss 0.9233 Accuracy 0.6869\n",
            "Epoch 21 Batch 200 Loss 0.9240 Accuracy 0.6866\n",
            "Epoch 21 Batch 250 Loss 0.9245 Accuracy 0.6861\n",
            "Epoch 21 Batch 300 Loss 0.9253 Accuracy 0.6858\n",
            "Epoch 21 Loss 0.9257 Accuracy 0.6857\n",
            "Time taken for 1 epoch: 51.42 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.9107 Accuracy 0.6899\n",
            "Epoch 22 Batch 50 Loss 0.9034 Accuracy 0.6915\n",
            "Epoch 22 Batch 100 Loss 0.9046 Accuracy 0.6914\n",
            "Epoch 22 Batch 150 Loss 0.9089 Accuracy 0.6898\n",
            "Epoch 22 Batch 200 Loss 0.9100 Accuracy 0.6896\n",
            "Epoch 22 Batch 250 Loss 0.9104 Accuracy 0.6897\n",
            "Epoch 22 Batch 300 Loss 0.9115 Accuracy 0.6896\n",
            "Epoch 22 Loss 0.9124 Accuracy 0.6894\n",
            "Time taken for 1 epoch: 51.43 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.8948 Accuracy 0.6932\n",
            "Epoch 23 Batch 50 Loss 0.8891 Accuracy 0.6967\n",
            "Epoch 23 Batch 100 Loss 0.8936 Accuracy 0.6958\n",
            "Epoch 23 Batch 150 Loss 0.8949 Accuracy 0.6953\n",
            "Epoch 23 Batch 200 Loss 0.8952 Accuracy 0.6952\n",
            "Epoch 23 Batch 250 Loss 0.8967 Accuracy 0.6948\n",
            "Epoch 23 Batch 300 Loss 0.8969 Accuracy 0.6948\n",
            "Epoch 23 Loss 0.8970 Accuracy 0.6947\n",
            "Time taken for 1 epoch: 51.40 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.8699 Accuracy 0.7025\n",
            "Epoch 24 Batch 50 Loss 0.8767 Accuracy 0.7024\n",
            "Epoch 24 Batch 100 Loss 0.8807 Accuracy 0.7009\n",
            "Epoch 24 Batch 150 Loss 0.8821 Accuracy 0.7001\n",
            "Epoch 24 Batch 200 Loss 0.8818 Accuracy 0.6999\n",
            "Epoch 24 Batch 250 Loss 0.8834 Accuracy 0.6994\n",
            "Epoch 24 Batch 300 Loss 0.8843 Accuracy 0.6991\n",
            "Epoch 24 Loss 0.8843 Accuracy 0.6989\n",
            "Time taken for 1 epoch: 51.36 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.8587 Accuracy 0.7025\n",
            "Epoch 25 Batch 50 Loss 0.8617 Accuracy 0.7051\n",
            "Epoch 25 Batch 100 Loss 0.8645 Accuracy 0.7048\n",
            "Epoch 25 Batch 150 Loss 0.8646 Accuracy 0.7048\n",
            "Epoch 25 Batch 200 Loss 0.8661 Accuracy 0.7043\n",
            "Epoch 25 Batch 250 Loss 0.8676 Accuracy 0.7038\n",
            "Epoch 25 Batch 300 Loss 0.8695 Accuracy 0.7033\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 0.8703 Accuracy 0.7032\n",
            "Time taken for 1 epoch: 51.64 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.8447 Accuracy 0.7154\n",
            "Epoch 26 Batch 50 Loss 0.8467 Accuracy 0.7113\n",
            "Epoch 26 Batch 100 Loss 0.8511 Accuracy 0.7100\n",
            "Epoch 26 Batch 150 Loss 0.8542 Accuracy 0.7085\n",
            "Epoch 26 Batch 200 Loss 0.8554 Accuracy 0.7082\n",
            "Epoch 26 Batch 250 Loss 0.8571 Accuracy 0.7075\n",
            "Epoch 26 Batch 300 Loss 0.8580 Accuracy 0.7070\n",
            "Epoch 26 Loss 0.8582 Accuracy 0.7069\n",
            "Time taken for 1 epoch: 51.29 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.8229 Accuracy 0.7151\n",
            "Epoch 27 Batch 50 Loss 0.8352 Accuracy 0.7148\n",
            "Epoch 27 Batch 100 Loss 0.8383 Accuracy 0.7137\n",
            "Epoch 27 Batch 150 Loss 0.8418 Accuracy 0.7128\n",
            "Epoch 27 Batch 200 Loss 0.8441 Accuracy 0.7121\n",
            "Epoch 27 Batch 250 Loss 0.8456 Accuracy 0.7116\n",
            "Epoch 27 Batch 300 Loss 0.8463 Accuracy 0.7113\n",
            "Epoch 27 Loss 0.8465 Accuracy 0.7111\n",
            "Time taken for 1 epoch: 51.27 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.8201 Accuracy 0.7177\n",
            "Epoch 28 Batch 50 Loss 0.8292 Accuracy 0.7169\n",
            "Epoch 28 Batch 100 Loss 0.8301 Accuracy 0.7166\n",
            "Epoch 28 Batch 150 Loss 0.8305 Accuracy 0.7163\n",
            "Epoch 28 Batch 200 Loss 0.8324 Accuracy 0.7155\n",
            "Epoch 28 Batch 250 Loss 0.8334 Accuracy 0.7152\n",
            "Epoch 28 Batch 300 Loss 0.8344 Accuracy 0.7148\n",
            "Epoch 28 Loss 0.8347 Accuracy 0.7146\n",
            "Time taken for 1 epoch: 51.34 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.8139 Accuracy 0.7212\n",
            "Epoch 29 Batch 50 Loss 0.8140 Accuracy 0.7219\n",
            "Epoch 29 Batch 100 Loss 0.8172 Accuracy 0.7210\n",
            "Epoch 29 Batch 150 Loss 0.8186 Accuracy 0.7202\n",
            "Epoch 29 Batch 200 Loss 0.8198 Accuracy 0.7196\n",
            "Epoch 29 Batch 250 Loss 0.8215 Accuracy 0.7191\n",
            "Epoch 29 Batch 300 Loss 0.8232 Accuracy 0.7185\n",
            "Epoch 29 Loss 0.8232 Accuracy 0.7184\n",
            "Time taken for 1 epoch: 51.40 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.8176 Accuracy 0.7158\n",
            "Epoch 30 Batch 50 Loss 0.7997 Accuracy 0.7263\n",
            "Epoch 30 Batch 100 Loss 0.8033 Accuracy 0.7250\n",
            "Epoch 30 Batch 150 Loss 0.8055 Accuracy 0.7243\n",
            "Epoch 30 Batch 200 Loss 0.8089 Accuracy 0.7233\n",
            "Epoch 30 Batch 250 Loss 0.8105 Accuracy 0.7229\n",
            "Epoch 30 Batch 300 Loss 0.8112 Accuracy 0.7225\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.8121 Accuracy 0.7222\n",
            "Time taken for 1 epoch: 51.71 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.7805 Accuracy 0.7266\n",
            "Epoch 31 Batch 50 Loss 0.7935 Accuracy 0.7283\n",
            "Epoch 31 Batch 100 Loss 0.7947 Accuracy 0.7276\n",
            "Epoch 31 Batch 150 Loss 0.7946 Accuracy 0.7275\n",
            "Epoch 31 Batch 200 Loss 0.7984 Accuracy 0.7265\n",
            "Epoch 31 Batch 250 Loss 0.7994 Accuracy 0.7261\n",
            "Epoch 31 Batch 300 Loss 0.8003 Accuracy 0.7258\n",
            "Epoch 31 Loss 0.8015 Accuracy 0.7254\n",
            "Time taken for 1 epoch: 51.34 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.7578 Accuracy 0.7428\n",
            "Epoch 32 Batch 50 Loss 0.7798 Accuracy 0.7324\n",
            "Epoch 32 Batch 100 Loss 0.7833 Accuracy 0.7311\n",
            "Epoch 32 Batch 150 Loss 0.7847 Accuracy 0.7310\n",
            "Epoch 32 Batch 200 Loss 0.7870 Accuracy 0.7306\n",
            "Epoch 32 Batch 250 Loss 0.7880 Accuracy 0.7303\n",
            "Epoch 32 Batch 300 Loss 0.7898 Accuracy 0.7297\n",
            "Epoch 32 Loss 0.7909 Accuracy 0.7292\n",
            "Time taken for 1 epoch: 51.31 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.7573 Accuracy 0.7368\n",
            "Epoch 33 Batch 50 Loss 0.7694 Accuracy 0.7361\n",
            "Epoch 33 Batch 100 Loss 0.7710 Accuracy 0.7361\n",
            "Epoch 33 Batch 150 Loss 0.7740 Accuracy 0.7350\n",
            "Epoch 33 Batch 200 Loss 0.7765 Accuracy 0.7342\n",
            "Epoch 33 Batch 250 Loss 0.7782 Accuracy 0.7335\n",
            "Epoch 33 Batch 300 Loss 0.7799 Accuracy 0.7330\n",
            "Epoch 33 Loss 0.7805 Accuracy 0.7327\n",
            "Time taken for 1 epoch: 51.29 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.7548 Accuracy 0.7451\n",
            "Epoch 34 Batch 50 Loss 0.7624 Accuracy 0.7387\n",
            "Epoch 34 Batch 100 Loss 0.7652 Accuracy 0.7378\n",
            "Epoch 34 Batch 150 Loss 0.7660 Accuracy 0.7373\n",
            "Epoch 34 Batch 200 Loss 0.7684 Accuracy 0.7365\n",
            "Epoch 34 Batch 250 Loss 0.7701 Accuracy 0.7361\n",
            "Epoch 34 Batch 300 Loss 0.7714 Accuracy 0.7358\n",
            "Epoch 34 Loss 0.7722 Accuracy 0.7355\n",
            "Time taken for 1 epoch: 51.27 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.7269 Accuracy 0.7545\n",
            "Epoch 35 Batch 50 Loss 0.7470 Accuracy 0.7442\n",
            "Epoch 35 Batch 100 Loss 0.7510 Accuracy 0.7422\n",
            "Epoch 35 Batch 150 Loss 0.7549 Accuracy 0.7409\n",
            "Epoch 35 Batch 200 Loss 0.7580 Accuracy 0.7399\n",
            "Epoch 35 Batch 250 Loss 0.7601 Accuracy 0.7392\n",
            "Epoch 35 Batch 300 Loss 0.7617 Accuracy 0.7387\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.7621 Accuracy 0.7385\n",
            "Time taken for 1 epoch: 51.59 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.7734 Accuracy 0.7373\n",
            "Epoch 36 Batch 50 Loss 0.7413 Accuracy 0.7468\n",
            "Epoch 36 Batch 100 Loss 0.7451 Accuracy 0.7446\n",
            "Epoch 36 Batch 150 Loss 0.7475 Accuracy 0.7439\n",
            "Epoch 36 Batch 200 Loss 0.7495 Accuracy 0.7431\n",
            "Epoch 36 Batch 250 Loss 0.7505 Accuracy 0.7427\n",
            "Epoch 36 Batch 300 Loss 0.7529 Accuracy 0.7418\n",
            "Epoch 36 Loss 0.7532 Accuracy 0.7417\n",
            "Time taken for 1 epoch: 51.31 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.7419 Accuracy 0.7419\n",
            "Epoch 37 Batch 50 Loss 0.7294 Accuracy 0.7492\n",
            "Epoch 37 Batch 100 Loss 0.7350 Accuracy 0.7475\n",
            "Epoch 37 Batch 150 Loss 0.7387 Accuracy 0.7468\n",
            "Epoch 37 Batch 200 Loss 0.7399 Accuracy 0.7462\n",
            "Epoch 37 Batch 250 Loss 0.7420 Accuracy 0.7455\n",
            "Epoch 37 Batch 300 Loss 0.7431 Accuracy 0.7452\n",
            "Epoch 37 Loss 0.7443 Accuracy 0.7448\n",
            "Time taken for 1 epoch: 51.36 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.7280 Accuracy 0.7480\n",
            "Epoch 38 Batch 50 Loss 0.7188 Accuracy 0.7541\n",
            "Epoch 38 Batch 100 Loss 0.7244 Accuracy 0.7521\n",
            "Epoch 38 Batch 150 Loss 0.7280 Accuracy 0.7503\n",
            "Epoch 38 Batch 200 Loss 0.7305 Accuracy 0.7493\n",
            "Epoch 38 Batch 250 Loss 0.7324 Accuracy 0.7486\n",
            "Epoch 38 Batch 300 Loss 0.7338 Accuracy 0.7481\n",
            "Epoch 38 Loss 0.7352 Accuracy 0.7477\n",
            "Time taken for 1 epoch: 51.41 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.7259 Accuracy 0.7504\n",
            "Epoch 39 Batch 50 Loss 0.7165 Accuracy 0.7547\n",
            "Epoch 39 Batch 100 Loss 0.7188 Accuracy 0.7537\n",
            "Epoch 39 Batch 150 Loss 0.7223 Accuracy 0.7523\n",
            "Epoch 39 Batch 200 Loss 0.7243 Accuracy 0.7515\n",
            "Epoch 39 Batch 250 Loss 0.7258 Accuracy 0.7511\n",
            "Epoch 39 Batch 300 Loss 0.7266 Accuracy 0.7508\n",
            "Epoch 39 Loss 0.7280 Accuracy 0.7503\n",
            "Time taken for 1 epoch: 51.38 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.7123 Accuracy 0.7546\n",
            "Epoch 40 Batch 50 Loss 0.7076 Accuracy 0.7572\n",
            "Epoch 40 Batch 100 Loss 0.7112 Accuracy 0.7562\n",
            "Epoch 40 Batch 150 Loss 0.7137 Accuracy 0.7554\n",
            "Epoch 40 Batch 200 Loss 0.7157 Accuracy 0.7547\n",
            "Epoch 40 Batch 250 Loss 0.7168 Accuracy 0.7542\n",
            "Epoch 40 Batch 300 Loss 0.7184 Accuracy 0.7536\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.7202 Accuracy 0.7529\n",
            "Time taken for 1 epoch: 51.81 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.7053 Accuracy 0.7555\n",
            "Epoch 41 Batch 50 Loss 0.6974 Accuracy 0.7604\n",
            "Epoch 41 Batch 100 Loss 0.7005 Accuracy 0.7595\n",
            "Epoch 41 Batch 150 Loss 0.7042 Accuracy 0.7586\n",
            "Epoch 41 Batch 200 Loss 0.7078 Accuracy 0.7572\n",
            "Epoch 41 Batch 250 Loss 0.7092 Accuracy 0.7568\n",
            "Epoch 41 Batch 300 Loss 0.7107 Accuracy 0.7562\n",
            "Epoch 41 Loss 0.7119 Accuracy 0.7558\n",
            "Time taken for 1 epoch: 51.29 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.6771 Accuracy 0.7665\n",
            "Epoch 42 Batch 50 Loss 0.6909 Accuracy 0.7621\n",
            "Epoch 42 Batch 100 Loss 0.6938 Accuracy 0.7614\n",
            "Epoch 42 Batch 150 Loss 0.6959 Accuracy 0.7609\n",
            "Epoch 42 Batch 200 Loss 0.6995 Accuracy 0.7596\n",
            "Epoch 42 Batch 250 Loss 0.7016 Accuracy 0.7590\n",
            "Epoch 42 Batch 300 Loss 0.7025 Accuracy 0.7585\n",
            "Epoch 42 Loss 0.7035 Accuracy 0.7582\n",
            "Time taken for 1 epoch: 51.26 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.6557 Accuracy 0.7680\n",
            "Epoch 43 Batch 50 Loss 0.6858 Accuracy 0.7647\n",
            "Epoch 43 Batch 100 Loss 0.6899 Accuracy 0.7630\n",
            "Epoch 43 Batch 150 Loss 0.6913 Accuracy 0.7628\n",
            "Epoch 43 Batch 200 Loss 0.6936 Accuracy 0.7618\n",
            "Epoch 43 Batch 250 Loss 0.6944 Accuracy 0.7616\n",
            "Epoch 43 Batch 300 Loss 0.6963 Accuracy 0.7609\n",
            "Epoch 43 Loss 0.6971 Accuracy 0.7608\n",
            "Time taken for 1 epoch: 51.26 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.6946 Accuracy 0.7603\n",
            "Epoch 44 Batch 50 Loss 0.6745 Accuracy 0.7677\n",
            "Epoch 44 Batch 100 Loss 0.6790 Accuracy 0.7665\n",
            "Epoch 44 Batch 150 Loss 0.6813 Accuracy 0.7657\n",
            "Epoch 44 Batch 200 Loss 0.6851 Accuracy 0.7647\n",
            "Epoch 44 Batch 250 Loss 0.6867 Accuracy 0.7640\n",
            "Epoch 44 Batch 300 Loss 0.6890 Accuracy 0.7633\n",
            "Epoch 44 Loss 0.6900 Accuracy 0.7630\n",
            "Time taken for 1 epoch: 51.26 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.6717 Accuracy 0.7795\n",
            "Epoch 45 Batch 50 Loss 0.6669 Accuracy 0.7719\n",
            "Epoch 45 Batch 100 Loss 0.6723 Accuracy 0.7698\n",
            "Epoch 45 Batch 150 Loss 0.6754 Accuracy 0.7684\n",
            "Epoch 45 Batch 200 Loss 0.6773 Accuracy 0.7678\n",
            "Epoch 45 Batch 250 Loss 0.6794 Accuracy 0.7671\n",
            "Epoch 45 Batch 300 Loss 0.6825 Accuracy 0.7660\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.6827 Accuracy 0.7658\n",
            "Time taken for 1 epoch: 51.63 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.6370 Accuracy 0.7838\n",
            "Epoch 46 Batch 50 Loss 0.6601 Accuracy 0.7727\n",
            "Epoch 46 Batch 100 Loss 0.6652 Accuracy 0.7714\n",
            "Epoch 46 Batch 150 Loss 0.6681 Accuracy 0.7705\n",
            "Epoch 46 Batch 200 Loss 0.6711 Accuracy 0.7695\n",
            "Epoch 46 Batch 250 Loss 0.6736 Accuracy 0.7685\n",
            "Epoch 46 Batch 300 Loss 0.6753 Accuracy 0.7679\n",
            "Epoch 46 Loss 0.6760 Accuracy 0.7675\n",
            "Time taken for 1 epoch: 51.40 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.6600 Accuracy 0.7741\n",
            "Epoch 47 Batch 50 Loss 0.6607 Accuracy 0.7730\n",
            "Epoch 47 Batch 100 Loss 0.6622 Accuracy 0.7721\n",
            "Epoch 47 Batch 150 Loss 0.6655 Accuracy 0.7712\n",
            "Epoch 47 Batch 200 Loss 0.6661 Accuracy 0.7711\n",
            "Epoch 47 Batch 250 Loss 0.6679 Accuracy 0.7704\n",
            "Epoch 47 Batch 300 Loss 0.6692 Accuracy 0.7700\n",
            "Epoch 47 Loss 0.6699 Accuracy 0.7699\n",
            "Time taken for 1 epoch: 51.39 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.6390 Accuracy 0.7787\n",
            "Epoch 48 Batch 50 Loss 0.6488 Accuracy 0.7775\n",
            "Epoch 48 Batch 100 Loss 0.6523 Accuracy 0.7764\n",
            "Epoch 48 Batch 150 Loss 0.6564 Accuracy 0.7749\n",
            "Epoch 48 Batch 200 Loss 0.6591 Accuracy 0.7738\n",
            "Epoch 48 Batch 250 Loss 0.6610 Accuracy 0.7732\n",
            "Epoch 48 Batch 300 Loss 0.6625 Accuracy 0.7727\n",
            "Epoch 48 Loss 0.6637 Accuracy 0.7723\n",
            "Time taken for 1 epoch: 51.35 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.6631 Accuracy 0.7707\n",
            "Epoch 49 Batch 50 Loss 0.6416 Accuracy 0.7807\n",
            "Epoch 49 Batch 100 Loss 0.6461 Accuracy 0.7782\n",
            "Epoch 49 Batch 150 Loss 0.6495 Accuracy 0.7769\n",
            "Epoch 49 Batch 200 Loss 0.6515 Accuracy 0.7762\n",
            "Epoch 49 Batch 250 Loss 0.6539 Accuracy 0.7754\n",
            "Epoch 49 Batch 300 Loss 0.6564 Accuracy 0.7745\n",
            "Epoch 49 Loss 0.6570 Accuracy 0.7744\n",
            "Time taken for 1 epoch: 51.30 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.6592 Accuracy 0.7689\n",
            "Epoch 50 Batch 50 Loss 0.6376 Accuracy 0.7812\n",
            "Epoch 50 Batch 100 Loss 0.6375 Accuracy 0.7809\n",
            "Epoch 50 Batch 150 Loss 0.6418 Accuracy 0.7792\n",
            "Epoch 50 Batch 200 Loss 0.6459 Accuracy 0.7779\n",
            "Epoch 50 Batch 250 Loss 0.6479 Accuracy 0.7774\n",
            "Epoch 50 Batch 300 Loss 0.6503 Accuracy 0.7766\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.6515 Accuracy 0.7762\n",
            "Time taken for 1 epoch: 51.58 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "BSNaKtSkvxcJ"
      },
      "source": [
        "def translate(sentence, max_length=200):\n",
        "  \n",
        "  encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "  encoder_input = tf.keras.preprocessing.sequence.pad_sequences([encoder_input],\n",
        "                                                         maxlen=max_length,\n",
        "                                                         padding='post')\n",
        "  encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "\n",
        "  output = tf.convert_to_tensor([tokenizer.word_index['^']])\n",
        "  output = tf.expand_dims(output, 0)\n",
        "  result = ''\n",
        "\n",
        "  for i in range(max_length):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "\n",
        "    # select the last character from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "    output = tf.concat([tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)], axis=-1)\n",
        "    result += tokenizer.index_word[predicted_id.numpy()[0][0]] + ' '\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "      \n",
        "      break\n",
        "\n",
        "  # output.shape (1, tokens)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-UvZOCbkK2d"
      },
      "source": [
        "def print_translation(sentence, result, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {result}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R26smp7BkSZu",
        "outputId": "f28066fd-dc7b-4b35-885f-0fad4ec483d9"
      },
      "source": [
        "sentence = \"^E come l’aere, quand’ è ben pïorno,$\"\n",
        "ground_truth = \"|E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\"\n",
        "\n",
        "\n",
        "translated_text = translate(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^E come l’aere, quand’ è ben pïorno,$\n",
            "Prediction     : | c h e   | l a   | m i a   | v i | s t a   | m i   | f é   | p a | r e a   | m a | l e $ \n",
            "Ground truth   : |E |co|me |l’ ae|re, |quan|d’ è |ben |pï|or|no,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "  \n",
        "  #encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "  encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "  #decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "  result = ''\n",
        "\n",
        "  for i in range(200):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "\n",
        "    # select the last character from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "    output = tf.concat([tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)], axis=-1)\n",
        "    result += tokenizer.index_word[predicted_id.numpy()[0][0]] + ' '\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "      result += \"\\n\"\n",
        "    if result.count(\"$\") == 3:\n",
        "      return result\n",
        "\n",
        "  # output.shape (1, tokens)\n",
        "\n",
        "  "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "3285a1fb-689a-437e-fd57-2e8e38e43411"
      },
      "source": [
        "encoder_input = [tokenizer.word_index['^']]\n",
        "decoder_input = [tokenizer.word_index['^']]\n",
        "\n",
        "generated_text = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| c h e   | l a   | m i a   | v i | s t a   | m i   | f é   | p a | r e a   | m a | l e $ \n",
            "^ | c h e   | l ’   a | b i | t o   | d e   | l ’   a r | g o | m e n | t o   | s e | g n o . $ \n",
            "^ | E   | q u e l | l a   | c h e   | p r o | p r i e | n e   i l   | c a | l o r   | m i o $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "  \n",
        "  encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  output_ = []\n",
        "\n",
        "  terces = 0\n",
        "  \n",
        "  for i in range(200):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "\n",
        "    # select the last character from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "    predictions, indices = tf.math.top_k(predictions,k=k)\n",
        "\n",
        "    predictions /= temperature\n",
        "    predictions = np.squeeze(predictions, axis=0)\n",
        "    indices = np.squeeze(indices, axis=0)\n",
        "    indices = np.squeeze(indices, axis=0)\n",
        "    pred=tf.random.categorical(predictions, num_samples=1)\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    predicted_id = indices[predicted_id]\n",
        "\n",
        "    predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "    predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    result += tokenizer.index_word[predicted_id.numpy()[0][0]] + ' '\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "      result += \"\\n\"\n",
        "\n",
        "    if result.count(\"$\") == 3:\n",
        "      terces += 1\n",
        "    \n",
        "    if terces == 3:\n",
        "      return result\n",
        "\n",
        "  # output.shape (1, tokens)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_jjxe1az9K6",
        "outputId": "117c4c41-1b95-4506-f0d0-e8a0d75bef80"
      },
      "source": [
        "encoder_input = [tokenizer.word_index['^']]\n",
        "decoder_input = [tokenizer.word_index['^']]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| s o | v r a   | l e   | s t e l | l e   | s t e l | l e   | d e l   | s u o   | v e r | b o , $ \n",
            "^ | p e r   | c h e   | l a   | v i | v a   | l u | c e   | c o n | v i e n   | c a | r e . $ \n",
            "^ | E   | q u e | s t o   | s i   | f e | c e   | c o n | v i e n   | c h e   | v a n | n o $ \n",
            "^ ^ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNngr1tM2PgW",
        "outputId": "52e96f58-3659-4635-af78-b096a50d0561"
      },
      "source": [
        "sentence = 'ciao'\n",
        "encoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "decoder_input = [tokenizer.word_index[i] for i in list(map(str, sentence))]\n",
        "\n",
        "generated_text = generate_topk(encoder_input, decoder_input)\n",
        "print(generated_text)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| v e   | g i à   | m a i   | n o n   | f u   | m a i   | n é   | r i | s t r e t | t a . $ \n",
            "^ | O r   | s a i   | t u   | d i e | t r o ,   e   | n o n   | t i   | p a r | l a | v a | r o : $ \n",
            "^ | p e r   | c h e   | l e   | s t e l | l e   | c h e   ’ n   | s u   | l a   | p r o | p r i a $ \n",
            "^ | \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}