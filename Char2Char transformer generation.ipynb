{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnkjwFedVSEH"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "zt8hrBu7VWUU",
    "outputId": "c4c795ae-fc59-4dac-dd6e-e64ec498256b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2df9b6bb-709f-4562-900d-2d22a34bf2e6\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2df9b6bb-709f-4562-900d-2d22a34bf2e6\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data.zip to data.zip\n",
      "Saving deepcomedy.zip to deepcomedy.zip\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    \n",
    "    files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFeWLZJfVaro",
    "outputId": "30d6cb99-97b7-4c47-dd7e-c16b7e1748f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 2.9MB/s \n",
      "\u001b[?25hCollecting configparser>=3.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
      "Collecting subprocess32>=3.5.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
      "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 20.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting pathtools\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
      "Collecting GitPython>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 18.6MB/s \n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: subprocess32, pathtools\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=31caef3ebe835e319122388e5730ab7480fb41ed2168193ce589c2b961ebee61\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=89037e1e0b2349eaff16e42db55d022ef529ed05ab918fc329da3de263732c2b\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "Successfully built subprocess32 pathtools\n",
      "Installing collected packages: configparser, subprocess32, sentry-sdk, pathtools, smmap, gitdb, GitPython, shortuuid, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n",
      "Archive:  deepcomedy.zip\n",
      "   creating: deepcomedy/models/\n",
      "  inflating: deepcomedy/models/layers.py  \n",
      "  inflating: deepcomedy/models/transformer.py  \n",
      "  inflating: deepcomedy/preprocessing.py  \n",
      "   creating: deepcomedy/util/\n",
      "  inflating: deepcomedy/util/predicate.py  \n",
      "Archive:  data.zip\n",
      "  inflating: data/divina_syll_textonly.txt  \n",
      "  inflating: data/divina_textonly.txt  \n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "#!tar zxvf deepcomedy.tar.gz\n",
    "!unzip deepcomedy.zip\n",
    "#!tar zxvf data.tar.gz\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "id": "54j16swJY1dW"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from deepcomedy.models.transformer import *\n",
    "from deepcomedy.preprocessing import *\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RuMqNB4ujuT",
    "tags": []
   },
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21xRrPMVVhWC"
   },
   "outputs": [],
   "source": [
    "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    "raw_syll_text = (\n",
    "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
    ")\n",
    "syll_text = preprocess_text(raw_syll_text, end_of_verse = \"\")\n",
    "text = preprocess_text(raw_text, end_of_verse = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgsnqceVVjdt"
   },
   "source": [
    "Split preprocessed text into verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POlFEZVCVs5W"
   },
   "outputs": [],
   "source": [
    "sep = \"<EOT>\"\n",
    "input_tercets = [x + sep for x in text.split(sep)][:-1]\n",
    "target_tercets = [x + sep for x in syll_text.split(sep)][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5uICmgWVyYt"
   },
   "source": [
    "Encode with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAgXmGaMV-CB"
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False, filters=\"\", lower=False\n",
    ")\n",
    "tokenizer.fit_on_texts(target_tercets)\n",
    "enc_input_tercets = tokenizer.texts_to_sequences(input_tercets)\n",
    "enc_target_tercets = tokenizer.texts_to_sequences(target_tercets)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67gMpsbzVSEV"
   },
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for line in range(len(enc_input_tercets) - 2):\n",
    "    input_text.append(list(chain(*enc_input_tercets[line : line + 2])))\n",
    "    target_text.append(list(chain(*enc_target_tercets[line + 1 : line + 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFR77khCWAHX"
   },
   "source": [
    "Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zfrhxn_KWDV-"
   },
   "outputs": [],
   "source": [
    "padded_input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_text, padding=\"post\"\n",
    ")\n",
    "padded_target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_text, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-deMFBQAe-Zr"
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(\n",
    "    padded_input_text, padded_target_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GVc41zvvdR9"
   },
   "source": [
    "## 2. The Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNlSr1CqIYtF"
   },
   "outputs": [],
   "source": [
    "def make_dataset(input_train, target_train, batch_size=32):\n",
    "    buffer_size = len(input_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSBdk2MpfQZ6"
   },
   "outputs": [],
   "source": [
    "dataset = make_dataset(input_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5nYoP1IfTE5"
   },
   "outputs": [],
   "source": [
    "def make_transformer_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path = None):\n",
    "    transformer = Transformer(\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        dff=config[\"dff\"],\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=1000,\n",
    "        pe_target=1000,\n",
    "        rate=0.1,\n",
    "    )\n",
    "    transformer_trainer = TransformerTrainer(transformer, checkpoint_save_path= checkpoint_save_path)\n",
    "\n",
    "    return transformer, transformer_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsfUp1XTfVst"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\" : 6,\n",
    "    \"d_model\" : 256,\n",
    "    \"num_heads\" : 8,\n",
    "    \"dff\" : 1024\n",
    "}\n",
    "\n",
    "checkpoint_save_path = \"./checkpoints/char-level_gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkDsSnzffdfg"
   },
   "outputs": [],
   "source": [
    "transformer, transformer_trainer = make_transformer_model(config, vocab_size, vocab_size, checkpoint_save_path= checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PLTOETK4_m6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F81qguJCVSEZ",
    "outputId": "4d5e78de-5954-4fcc-bc8d-8afa4f49bfd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.3591 Accuracy 0.0009\n",
      "Epoch 1 Batch 50 Loss 4.1508 Accuracy 0.1494\n",
      "Epoch 1 Batch 100 Loss 3.6142 Accuracy 0.1795\n",
      "Epoch 1 Loss 3.5534 Accuracy 0.1830\n",
      "Time taken for 1 epoch: 153.76 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.9743 Accuracy 0.2160\n",
      "Epoch 2 Batch 50 Loss 2.8866 Accuracy 0.2321\n",
      "Epoch 2 Batch 100 Loss 2.6464 Accuracy 0.2682\n",
      "Epoch 2 Loss 2.6055 Accuracy 0.2743\n",
      "Time taken for 1 epoch: 134.99 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.1914 Accuracy 0.3351\n",
      "Epoch 3 Batch 50 Loss 2.1387 Accuracy 0.3418\n",
      "Epoch 3 Batch 100 Loss 2.0983 Accuracy 0.3467\n",
      "Epoch 3 Loss 2.0907 Accuracy 0.3477\n",
      "Time taken for 1 epoch: 134.40 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.0155 Accuracy 0.3615\n",
      "Epoch 4 Batch 50 Loss 2.0054 Accuracy 0.3582\n",
      "Epoch 4 Batch 100 Loss 1.9949 Accuracy 0.3597\n",
      "Epoch 4 Loss 1.9920 Accuracy 0.3602\n",
      "Time taken for 1 epoch: 134.51 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.9939 Accuracy 0.3549\n",
      "Epoch 5 Batch 50 Loss 1.9572 Accuracy 0.3673\n",
      "Epoch 5 Batch 100 Loss 1.9522 Accuracy 0.3685\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/char-level_gen/ckpt-1\n",
      "Epoch 5 Loss 1.9504 Accuracy 0.3690\n",
      "Time taken for 1 epoch: 135.18 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.9353 Accuracy 0.3799\n",
      "Epoch 6 Batch 50 Loss 1.9203 Accuracy 0.3777\n",
      "Epoch 6 Batch 100 Loss 1.9079 Accuracy 0.3815\n",
      "Epoch 6 Loss 1.9045 Accuracy 0.3823\n",
      "Time taken for 1 epoch: 134.57 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.8670 Accuracy 0.3958\n",
      "Epoch 7 Batch 50 Loss 1.8561 Accuracy 0.3961\n",
      "Epoch 7 Batch 100 Loss 1.8409 Accuracy 0.4006\n",
      "Epoch 7 Loss 1.8365 Accuracy 0.4015\n",
      "Time taken for 1 epoch: 134.49 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.7978 Accuracy 0.4068\n",
      "Epoch 8 Batch 50 Loss 1.7740 Accuracy 0.4176\n",
      "Epoch 8 Batch 100 Loss 1.7561 Accuracy 0.4221\n",
      "Epoch 8 Loss 1.7523 Accuracy 0.4230\n",
      "Time taken for 1 epoch: 134.47 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.7146 Accuracy 0.4327\n",
      "Epoch 9 Batch 50 Loss 1.6927 Accuracy 0.4398\n",
      "Epoch 9 Batch 100 Loss 1.6768 Accuracy 0.4452\n",
      "Epoch 9 Loss 1.6736 Accuracy 0.4461\n",
      "Time taken for 1 epoch: 134.44 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.6415 Accuracy 0.4555\n",
      "Epoch 10 Batch 50 Loss 1.6283 Accuracy 0.4605\n",
      "Epoch 10 Batch 100 Loss 1.6176 Accuracy 0.4641\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/char-level_gen/ckpt-2\n",
      "Epoch 10 Loss 1.6154 Accuracy 0.4648\n",
      "Time taken for 1 epoch: 134.98 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.5900 Accuracy 0.4786\n",
      "Epoch 11 Batch 50 Loss 1.5733 Accuracy 0.4792\n",
      "Epoch 11 Batch 100 Loss 1.5608 Accuracy 0.4828\n",
      "Epoch 11 Loss 1.5588 Accuracy 0.4833\n",
      "Time taken for 1 epoch: 134.72 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.5384 Accuracy 0.4873\n",
      "Epoch 12 Batch 50 Loss 1.5183 Accuracy 0.4955\n",
      "Epoch 12 Batch 100 Loss 1.5070 Accuracy 0.4989\n",
      "Epoch 12 Loss 1.5037 Accuracy 0.5001\n",
      "Time taken for 1 epoch: 134.71 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.4506 Accuracy 0.5108\n",
      "Epoch 13 Batch 50 Loss 1.4639 Accuracy 0.5119\n",
      "Epoch 13 Batch 100 Loss 1.4573 Accuracy 0.5143\n",
      "Epoch 13 Loss 1.4561 Accuracy 0.5147\n",
      "Time taken for 1 epoch: 134.75 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.4268 Accuracy 0.5238\n",
      "Epoch 14 Batch 50 Loss 1.4202 Accuracy 0.5258\n",
      "Epoch 14 Batch 100 Loss 1.4134 Accuracy 0.5282\n",
      "Epoch 14 Loss 1.4126 Accuracy 0.5287\n",
      "Time taken for 1 epoch: 134.71 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.4195 Accuracy 0.5294\n",
      "Epoch 15 Batch 50 Loss 1.3823 Accuracy 0.5389\n",
      "Epoch 15 Batch 100 Loss 1.3738 Accuracy 0.5418\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/char-level_gen/ckpt-3\n",
      "Epoch 15 Loss 1.3729 Accuracy 0.5423\n",
      "Time taken for 1 epoch: 135.20 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.3330 Accuracy 0.5596\n",
      "Epoch 16 Batch 50 Loss 1.3445 Accuracy 0.5512\n",
      "Epoch 16 Batch 100 Loss 1.3353 Accuracy 0.5550\n",
      "Epoch 16 Loss 1.3342 Accuracy 0.5555\n",
      "Time taken for 1 epoch: 134.29 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.3169 Accuracy 0.5616\n",
      "Epoch 17 Batch 50 Loss 1.2988 Accuracy 0.5677\n",
      "Epoch 17 Batch 100 Loss 1.2942 Accuracy 0.5696\n",
      "Epoch 17 Loss 1.2926 Accuracy 0.5702\n",
      "Time taken for 1 epoch: 134.20 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.2924 Accuracy 0.5684\n",
      "Epoch 18 Batch 50 Loss 1.2612 Accuracy 0.5809\n",
      "Epoch 18 Batch 100 Loss 1.2550 Accuracy 0.5831\n",
      "Epoch 18 Loss 1.2532 Accuracy 0.5838\n",
      "Time taken for 1 epoch: 134.15 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.2355 Accuracy 0.5906\n",
      "Epoch 19 Batch 50 Loss 1.2185 Accuracy 0.5956\n",
      "Epoch 19 Batch 100 Loss 1.2156 Accuracy 0.5971\n",
      "Epoch 19 Loss 1.2153 Accuracy 0.5972\n",
      "Time taken for 1 epoch: 134.29 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.1886 Accuracy 0.6027\n",
      "Epoch 20 Batch 50 Loss 1.1844 Accuracy 0.6062\n",
      "Epoch 20 Batch 100 Loss 1.1818 Accuracy 0.6078\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/char-level_gen/ckpt-4\n",
      "Epoch 20 Loss 1.1816 Accuracy 0.6079\n",
      "Time taken for 1 epoch: 134.96 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.1855 Accuracy 0.6066\n",
      "Epoch 21 Batch 50 Loss 1.1573 Accuracy 0.6159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e47ada918cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/deepcomedy/models/transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer_trainer.train(dataset, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz4YwsF04YEI"
   },
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akQ7PnRmidiS"
   },
   "outputs": [],
   "source": [
    "def generate_greedy(encoder_input, decoder_input):\n",
    "\n",
    "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    result = \"<GO> \"\n",
    "    tokenized_result = [tokenizer.word_index[\"<GO>\"]]\n",
    "\n",
    "    for i in range(200):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat(\n",
    "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
    "            axis=-1,\n",
    "        )\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
    "\n",
    "        if predicted_id == tokenizer.word_index[\"<EOT>\"]:\n",
    "            return result, tokenized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ij2lr385ystg"
   },
   "outputs": [],
   "source": [
    "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
    "\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    result = \"<GO> \"\n",
    "    tokenized_result = [tokenizer.word_index[\"<GO>\"]]\n",
    "\n",
    "    for i in range(200):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask,\n",
    "        )\n",
    "\n",
    "        # select the last character from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
    "\n",
    "        predictions /= temperature\n",
    "        predictions = np.squeeze(predictions, axis=0)\n",
    "        indices = np.squeeze(indices, axis=0)\n",
    "        indices = np.squeeze(indices, axis=0)\n",
    "        pred = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        predicted_id = indices[predicted_id]\n",
    "\n",
    "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
    "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
    "\n",
    "        if predicted_id == tokenizer.word_index[\"<EOT>\"]:\n",
    "            return result, tokenized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcp7CnJvgdsK"
   },
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "  x = re.sub(r'\\| \\b', '', x)\n",
    "  x = re.sub(r'\\b \\|', '', x)\n",
    "  x = re.sub(r'\\|', '', x)\n",
    "  x = re.sub(r'[ ]+', ' ', x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8Y1nl5Ng4Z6"
   },
   "source": [
    "Feeding the encoder the last output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhPCGhJKgZFM",
    "outputId": "196afcf2-e767-4f8c-968a-d59334893744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> | e <SEP> | q u e l | l a <SEP> | c h e <SEP> | s i <SEP> | p i a n | t e <SEP> | d i | s c e r | n a | t a <GO> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | d i <SEP> | l a | s c i a | t a <SEP> | s p e | r a <GO> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | d i <SEP> | l u | c i <SEP> | s i <SEP> | p i a | t a . <EOT> \n",
      "<GO> e <SEP> q u e l l a <SEP> c h e <SEP> s i <SEP> p i a n t e <SEP> d i s c e r n a t a <GO> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> d i <SEP> l a s c i a t a <SEP> s p e r a <GO> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> d i <SEP> l u c i <SEP> s i <SEP> p i a t a . <EOT> \n"
     ]
    }
   ],
   "source": [
    "encoder_input = [tokenizer.word_index[\"<GO>\"]]\n",
    "decoder_input = [tokenizer.word_index[\"<GO>\"]]\n",
    "\n",
    "generated_text, generated_tokenized = generate_greedy(encoder_input, decoder_input)\n",
    "print(generated_text)\n",
    "print(clean(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc_fRKREQWbG",
    "outputId": "b8236ca3-f8dd-4b3c-acbe-f156ce61cb2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 3, 2, 26, 15, 3, 9, 9, 4, 2, 12, 22, 3, 2, 11, 5, 2, 17, 5, 4, 7, 10, 3, 2, 13, 5, 11, 12, 3, 8, 7, 4, 10, 4, 14, 13, 5, 2, 26, 15, 3, 9, 9, 4, 2, 12, 22, 3, 2, 17, 5, 31, 2, 13, 5, 2, 9, 4, 11, 12, 5, 4, 10, 4, 2, 11, 17, 3, 8, 4, 14, 13, 5, 2, 26, 15, 3, 9, 9, 4, 2, 12, 22, 3, 2, 17, 5, 31, 2, 13, 5, 2, 9, 15, 12, 5, 2, 11, 5, 2, 17, 5, 4, 10, 4, 25, 24]\n"
     ]
    }
   ],
   "source": [
    "tokenized_generated = tokenizer.texts_to_sequences([clean(generated_text)])\n",
    "print(tokenized_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FC7aKY9SQdcy",
    "outputId": "5e43b7fc-04d0-4029-c440-95557a8145c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> | c h é <SEP> | l a <SEP> | p i ù <SEP> | d i | s t a n | t a <SEP> | p i ù <SEP> | d i | s t a n | t a <GO> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | p i a n | t a <GO> | c h e <SEP> | p i ù <SEP> | d i <SEP> | s é <SEP> | l a <SEP> | p i a n | g e <SEP> | s i <SEP> | s p i | g a n | t a . <EOT> \n",
      "<GO> c h é <SEP> l a <SEP> p i ù <SEP> d i s t a n t a <SEP> p i ù <SEP> d i s t a n t a <GO> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> p i a n t a <GO> c h e <SEP> p i ù <SEP> d i <SEP> s é <SEP> l a <SEP> p i a n g e <SEP> s i <SEP> s p i g a n t a . <EOT> \n"
     ]
    }
   ],
   "source": [
    "generated_text_2, _ = generate_greedy(tokenized_generated[0], decoder_input)\n",
    "print(generated_text_2)\n",
    "print(clean(generated_text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89-zXyMpg73L"
   },
   "source": [
    "Feeding the decoder the last output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqGSZeJqgvta",
    "outputId": "40e44dcd-0a5e-4662-e0c5-3f41291a4f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> | c h é <SEP> | l a <SEP> | p i ù <SEP> | d i | s t a n | t a <SEP> | p i ù <SEP> | d i | s t a n | t a <GO> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | d i <SEP> | q u e l | l a <SEP> | c h e <SEP> | p i ù <SEP> | p i a n | t a <GO> | c h e <SEP> | p i ù <SEP> | d i <SEP> | s é <SEP> | l a <SEP> | p i a n | g e <SEP> | s i <SEP> | s p i | g a n | t a . <EOT> \n",
      "<GO> c h é <SEP> l a <SEP> p i ù <SEP> d i s t a n t a <SEP> p i ù <SEP> d i s t a n t a <GO> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> d i <SEP> q u e l l a <SEP> c h e <SEP> p i ù <SEP> p i a n t a <GO> c h e <SEP> p i ù <SEP> d i <SEP> s é <SEP> l a <SEP> p i a n g e <SEP> s i <SEP> s p i g a n t a . <EOT> \n"
     ]
    }
   ],
   "source": [
    "generated_text_3, _ = generate_greedy(encoder_input, generated_tokenized)\n",
    "print(generated_text_2)\n",
    "print(clean(generated_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMc9vGJfQoc2",
    "outputId": "bb2b9603-17fe-4f09-92f1-b6b064e6714e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/\n",
      "checkpoints/char-level_gen/\n",
      "checkpoints/char-level_gen/ckpt-3.index\n",
      "checkpoints/char-level_gen/ckpt-2.index\n",
      "checkpoints/char-level_gen/ckpt-1.index\n",
      "checkpoints/char-level_gen/ckpt-2.data-00000-of-00001\n",
      "checkpoints/char-level_gen/ckpt-1.data-00000-of-00001\n",
      "checkpoints/char-level_gen/checkpoint\n",
      "checkpoints/char-level_gen/ckpt-3.data-00000-of-00001\n",
      "checkpoints/char-level_gen/ckpt-4.data-00000-of-00001\n",
      "checkpoints/char-level_gen/ckpt-4.index\n"
     ]
    }
   ],
   "source": [
    "!tar zcvf checkpoints.tar.gz checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1tszdH_AQsEb",
    "outputId": "8f69d790-b9c9-4de1-98ad-657ad4baf2f3"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_fc6335a8-5d28-4432-99f6-b9472a564fc3\", \"checkpoints.tar.gz\", 491299243)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    files.download('checkpoints.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character-level transformer generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
