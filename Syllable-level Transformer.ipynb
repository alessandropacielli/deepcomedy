{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Syllable-level Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRKGBdAtVIfZ"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/syllablelevel\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "8RuMqNB4ujuT"
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "lsuXc5StY1dY"
      },
      "source": [
        "file = \"data/divina_syll_textonly.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "ACAEUyITY1dY",
        "outputId": "c7f4d6e3-4925-4c19-de0d-2223d14680a3"
      },
      "source": [
        "text_raw = open(file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(\"Length of text: {} characters\".format(len(text_raw)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 892871 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "RDRNHkB-VIfc"
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    For each line in the file, add start symbol \"^\" in the beginning and end symbol \"$\" in the end\n",
        "    \"\"\"\n",
        "    return [\"^ \" + line.strip() + \" $\" for line in text.split(\"\\n\") if line.strip() != \"\"]\n",
        "\n",
        "text_prepr = preprocess(text_raw)\n",
        "text_prepr = list(map(lambda x: re.sub('\\|', ' ', x), text_prepr))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "vgFtCA9wVIfe"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", split=' ', lower=False, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(text_prepr)\n",
        "\n",
        "text_lines_enc = tokenizer.texts_to_sequences(text_prepr)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "qu_z0ozzVIff"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "p-w27LhpY1db",
        "outputId": "185e3c84-5cde-41e5-b3e2-dc71a08b6ede"
      },
      "source": [
        "print(\"Vocab size: {}\".format(vocab_size))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 4191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bfzvcNvT-y"
      },
      "source": [
        "Padding is required in order to have a non-ragged tensor to feed to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "MBOh9LQeY1dg"
      },
      "source": [
        "def pad(x):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(x, padding=\"post\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "9zV0xz48Y1dh"
      },
      "source": [
        "text = pad(text_lines_enc)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "FE_LiRvcVIfi"
      },
      "source": [
        "input_text_ = []\n",
        "target_text_ = []\n",
        "\n",
        "for line_number in range(0, len(text) - 4):\n",
        "    \n",
        "    input_verses = []\n",
        "    target_verses = []\n",
        "    \n",
        "    for i in range(4):\n",
        "        input_verses += list(text[line_number + i])\n",
        "        target_verses += list(text[line_number + i])\n",
        "    \n",
        "    input_text_.append(input_verses)\n",
        "    target_text_.append(target_verses)\n",
        "    \n",
        "input_text_ = np.array(input_text_)\n",
        "target_text_ = np.array(target_text_)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7xGxZmlPY1dk"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    input_text_, target_text_\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IN8x175vimK"
      },
      "source": [
        "The dataset is created by grouping the lines in batches and by shuffling them.\n",
        "\n",
        "Each input's line is in correspondence with its target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "tZWLq7g3Y1dl"
      },
      "source": [
        "BUFFER_SIZE = len(input_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "\n",
        "vocab_size = (\n",
        "    len(tokenizer.word_index) + 1\n",
        ")  # the +1 is added to take into account the id 0 of the padding\n",
        "\n",
        "max_length = text.shape[1]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(\n",
        "    BUFFER_SIZE\n",
        ")\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHNAazT5Rs_"
      },
      "source": [
        "We define the positional encoding to add to the embedding.\n",
        "\n",
        "This allows to take into account the order of the characters in the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "f200V0QnkBBS"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OvnGjGhvkD9R"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500eU4tu6n-g"
      },
      "source": [
        "We define two masks: \n",
        "\n",
        "one is used to mask the padding added to the sequences in the preprocessing step; \n",
        "\n",
        "the other one is used to mask the positions following the current one and not predicted yet;\n",
        "\n",
        "The first mask is used from both the encoder and the decoder, while the last mask is used only in the self-attention of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "OVwx6Y4Tku1V"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "3p1-yIYimnvB"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "R-Q4J7EzfuLH"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxzWROrTM9ib"
      },
      "source": [
        "The *scaled_dot_product_attention* gets the attention weights by applying the softmax to the rescaled dot product between the query matrix and the key matrix, while the output is obtained by multiplying the value matrix for those attention weights.\n",
        "\n",
        "The query, key and value matrices are built by multiplying the embedding matrix with the query, key and value weight matrices, which initially are randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "RoFZK1S3mtI5"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += mask * -1e9\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1\n",
        "    )  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlA3inNYQO89"
      },
      "source": [
        "The multi-headed attention allows to improve the performance of the attention mechanism by working with multiple sets of query, key and value weight matrices.\n",
        "\n",
        "These heads work in parallel and process at the same time all the lines of each batch.\n",
        "\n",
        "At the end, the results of all the attention heads are concatenated and multiplied by an additional weight matrix, to adjust the dimension before passing through the final *point_wise_feed_forward_network*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "_UpdBWkVnK02"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        scaled_attention = tf.transpose(\n",
        "            scaled_attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(\n",
        "            scaled_attention, (batch_size, -1, self.d_model)\n",
        "        )  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LkMP7DDAok4y"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Dense(dff, activation=\"relu\"),  # (batch_size, seq_len, dff)\n",
        "            tf.keras.layers.Dense(d_model),  # (batch_size, seq_len, d_model)\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO6yyZPWUON7"
      },
      "source": [
        "Each encoder is constituted by a multi-headed self-attention layer and by a final feed forward layer. \n",
        "\n",
        "Both sub-layers have a residual connection around them and are followed by a layer-normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "Dat64C18otwC"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(\n",
        "            out1 + ffn_output\n",
        "        )  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZslNuSZOKp"
      },
      "source": [
        "The decoder equals the encoder, a part from the fact that it contains a slightly different self-attention layer and an additional attention layer.\n",
        "\n",
        "Indeed, the decoder is characterized by a self-attention layer which focuses only on earlier positions in its input sequence, not looking at the positions which have not been predicted yet.\n",
        "\n",
        "What's more the decoder is also characterized by an attention layer which obtains its key and value matrices from the output of the encoder, while the query matrix is obtained from the output of the previous self-attention in the decoder.\n",
        "\n",
        "The encoder-decoder attention helps the decoder to focus on appropriate positions in the input sequence of the encoder during the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "7Vp44lQepI_P"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(\n",
        "            x, x, x, look_ahead_mask\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(\n",
        "            ffn_output + out2\n",
        "        )  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx_DyJiybrOr"
      },
      "source": [
        "The encoding component is a stack of encoders and the decoding component is a stack of decoders of the same number.\n",
        "\n",
        "At the beginning, in the encoding, each input character is turned into a vector using an embedding algorithm and adding the positional encoding to it.\n",
        "\n",
        "This happens only in the bottom-most encoder, while the following encoders take the output of the encoder which is directly below.\n",
        "\n",
        "The same for the decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "awl9kiESpWBh"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        input_vocab_size,\n",
        "        maximum_position_encoding,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "47tQAEMwpnUj"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        target_vocab_size,\n",
        "        maximum_position_encoding,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](\n",
        "                x, enc_output, training, look_ahead_mask, padding_mask\n",
        "            )\n",
        "\n",
        "            attention_weights[f\"decoder_layer{i+1}_block1\"] = block1\n",
        "            attention_weights[f\"decoder_layer{i+1}_block2\"] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TosN_TpKk1eN"
      },
      "source": [
        "In the transformer, the output of the encoding is passed to the stack of decoders and the output of the decoding is projected by a feed forward network into a vector of logits of dimension equal to the one of the target's vocabulary.\n",
        "\n",
        "Obviously this is done for each character of each line of each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "qCNKKsQ-p99k"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dff,\n",
        "        input_vocab_size,\n",
        "        target_vocab_size,\n",
        "        pe_input,\n",
        "        pe_target,\n",
        "        rate=0.1,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate\n",
        "        )\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(\n",
        "        self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "    ):\n",
        "\n",
        "        enc_output = self.encoder(\n",
        "            inp, training, enc_padding_mask\n",
        "        )  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        final_output = self.final_layer(\n",
        "            dec_output\n",
        "        )  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "LrdL396xqOL4"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "JFCVQIDjqQHv"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "1R9MlFs0qc5U"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcmLAk1Ut8vG"
      },
      "source": [
        "The loss is calculated using Sparse Categorical Crossentropy and the loss of the padding is masked.\n",
        "\n",
        "The same is done for the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "TBAaRBPsqkuo"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "31R26t9wqlLD"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "SkVkWvL7qoYu"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.Mean(name=\"train_accuracy\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "5UE3cWGVqvnS"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size,\n",
        "    target_vocab_size=vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64MejGX2Lbk1"
      },
      "source": [
        "#!tar zxvf checkpoints.tar.gz"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "nSE2Rh-_qzo7"
      },
      "source": [
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVGgP8Su9MH"
      },
      "source": [
        "To train the decoder we use teacher forcing, calculating the loss between the predicted logits and the real id of the character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "n_VPs6ZOva15"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "1ce0FAOivleY",
        "outputId": "dd420f2e-cb04-4c83-c734-f8f60038ebe8"
      },
      "source": [
        "EPOCHS = 80\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}\"\n",
        "            )\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f\"Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}\"\n",
        "    )\n",
        "\n",
        "    print(f\"Time taken for 1 epoch: {time.time() - start:.2f} secs\\n\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.4200 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.2206 Accuracy 0.0261\n",
            "Epoch 1 Batch 100 Loss 7.9707 Accuracy 0.0488\n",
            "Epoch 1 Batch 150 Loss 7.7513 Accuracy 0.0580\n",
            "Epoch 1 Loss 7.6775 Accuracy 0.0641\n",
            "Time taken for 1 epoch: 36.00 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 6.8157 Accuracy 0.1245\n",
            "Epoch 2 Batch 50 Loss 6.5085 Accuracy 0.1259\n",
            "Epoch 2 Batch 100 Loss 6.2296 Accuracy 0.1267\n",
            "Epoch 2 Batch 150 Loss 6.0238 Accuracy 0.1320\n",
            "Epoch 2 Loss 5.9743 Accuracy 0.1338\n",
            "Time taken for 1 epoch: 24.85 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 5.4399 Accuracy 0.1537\n",
            "Epoch 3 Batch 50 Loss 5.3929 Accuracy 0.1608\n",
            "Epoch 3 Batch 100 Loss 5.3277 Accuracy 0.1678\n",
            "Epoch 3 Batch 150 Loss 5.2554 Accuracy 0.1757\n",
            "Epoch 3 Loss 5.2331 Accuracy 0.1781\n",
            "Time taken for 1 epoch: 25.20 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.9497 Accuracy 0.2089\n",
            "Epoch 4 Batch 50 Loss 4.8842 Accuracy 0.2130\n",
            "Epoch 4 Batch 100 Loss 4.8102 Accuracy 0.2194\n",
            "Epoch 4 Batch 150 Loss 4.7373 Accuracy 0.2261\n",
            "Epoch 4 Loss 4.7158 Accuracy 0.2280\n",
            "Time taken for 1 epoch: 25.47 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.4742 Accuracy 0.2460\n",
            "Epoch 5 Batch 50 Loss 4.4096 Accuracy 0.2561\n",
            "Epoch 5 Batch 100 Loss 4.3520 Accuracy 0.2608\n",
            "Epoch 5 Batch 150 Loss 4.2959 Accuracy 0.2666\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/syllablelevel/ckpt-1\n",
            "Epoch 5 Loss 4.2783 Accuracy 0.2688\n",
            "Time taken for 1 epoch: 26.17 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.0649 Accuracy 0.2842\n",
            "Epoch 6 Batch 50 Loss 3.9405 Accuracy 0.3178\n",
            "Epoch 6 Batch 100 Loss 3.7576 Accuracy 0.3551\n",
            "Epoch 6 Batch 150 Loss 3.4702 Accuracy 0.4111\n",
            "Epoch 6 Loss 3.3653 Accuracy 0.4304\n",
            "Time taken for 1 epoch: 25.95 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.2144 Accuracy 0.6465\n",
            "Epoch 7 Batch 50 Loss 1.7698 Accuracy 0.7171\n",
            "Epoch 7 Batch 100 Loss 1.4903 Accuracy 0.7685\n",
            "Epoch 7 Batch 150 Loss 1.2823 Accuracy 0.8049\n",
            "Epoch 7 Loss 1.2286 Accuracy 0.8141\n",
            "Time taken for 1 epoch: 26.18 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7162 Accuracy 0.8984\n",
            "Epoch 8 Batch 50 Loss 0.5793 Accuracy 0.9210\n",
            "Epoch 8 Batch 100 Loss 0.5166 Accuracy 0.9301\n",
            "Epoch 8 Batch 150 Loss 0.4658 Accuracy 0.9375\n",
            "Epoch 8 Loss 0.4533 Accuracy 0.9393\n",
            "Time taken for 1 epoch: 26.39 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2795 Accuracy 0.9666\n",
            "Epoch 9 Batch 50 Loss 0.2696 Accuracy 0.9651\n",
            "Epoch 9 Batch 100 Loss 0.2529 Accuracy 0.9673\n",
            "Epoch 9 Batch 150 Loss 0.2337 Accuracy 0.9697\n",
            "Epoch 9 Loss 0.2298 Accuracy 0.9702\n",
            "Time taken for 1 epoch: 26.39 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1592 Accuracy 0.9781\n",
            "Epoch 10 Batch 50 Loss 0.1552 Accuracy 0.9802\n",
            "Epoch 10 Batch 100 Loss 0.1461 Accuracy 0.9812\n",
            "Epoch 10 Batch 150 Loss 0.1384 Accuracy 0.9820\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/syllablelevel/ckpt-2\n",
            "Epoch 10 Loss 0.1365 Accuracy 0.9822\n",
            "Time taken for 1 epoch: 26.70 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.1159 Accuracy 0.9852\n",
            "Epoch 11 Batch 50 Loss 0.0980 Accuracy 0.9872\n",
            "Epoch 11 Batch 100 Loss 0.0954 Accuracy 0.9875\n",
            "Epoch 11 Batch 150 Loss 0.0928 Accuracy 0.9878\n",
            "Epoch 11 Loss 0.0916 Accuracy 0.9879\n",
            "Time taken for 1 epoch: 26.57 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0845 Accuracy 0.9881\n",
            "Epoch 12 Batch 50 Loss 0.0729 Accuracy 0.9907\n",
            "Epoch 12 Batch 100 Loss 0.0704 Accuracy 0.9909\n",
            "Epoch 12 Batch 150 Loss 0.0668 Accuracy 0.9914\n",
            "Epoch 12 Loss 0.0661 Accuracy 0.9915\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0430 Accuracy 0.9933\n",
            "Epoch 13 Batch 50 Loss 0.0506 Accuracy 0.9937\n",
            "Epoch 13 Batch 100 Loss 0.0523 Accuracy 0.9934\n",
            "Epoch 13 Batch 150 Loss 0.0510 Accuracy 0.9935\n",
            "Epoch 13 Loss 0.0506 Accuracy 0.9935\n",
            "Time taken for 1 epoch: 26.70 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0477 Accuracy 0.9944\n",
            "Epoch 14 Batch 50 Loss 0.0360 Accuracy 0.9953\n",
            "Epoch 14 Batch 100 Loss 0.0380 Accuracy 0.9950\n",
            "Epoch 14 Batch 150 Loss 0.0377 Accuracy 0.9950\n",
            "Epoch 14 Loss 0.0374 Accuracy 0.9950\n",
            "Time taken for 1 epoch: 26.67 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0363 Accuracy 0.9952\n",
            "Epoch 15 Batch 50 Loss 0.0315 Accuracy 0.9959\n",
            "Epoch 15 Batch 100 Loss 0.0340 Accuracy 0.9954\n",
            "Epoch 15 Batch 150 Loss 0.0343 Accuracy 0.9954\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/syllablelevel/ckpt-3\n",
            "Epoch 15 Loss 0.0339 Accuracy 0.9954\n",
            "Time taken for 1 epoch: 27.21 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0250 Accuracy 0.9958\n",
            "Epoch 16 Batch 50 Loss 0.0312 Accuracy 0.9955\n",
            "Epoch 16 Batch 100 Loss 0.0295 Accuracy 0.9958\n",
            "Epoch 16 Batch 150 Loss 0.0294 Accuracy 0.9958\n",
            "Epoch 16 Loss 0.0299 Accuracy 0.9957\n",
            "Time taken for 1 epoch: 26.68 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0248 Accuracy 0.9958\n",
            "Epoch 17 Batch 50 Loss 0.0311 Accuracy 0.9954\n",
            "Epoch 17 Batch 100 Loss 0.0282 Accuracy 0.9959\n",
            "Epoch 17 Batch 150 Loss 0.0289 Accuracy 0.9958\n",
            "Epoch 17 Loss 0.0291 Accuracy 0.9957\n",
            "Time taken for 1 epoch: 26.58 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0190 Accuracy 0.9972\n",
            "Epoch 18 Batch 50 Loss 0.0271 Accuracy 0.9960\n",
            "Epoch 18 Batch 100 Loss 0.0265 Accuracy 0.9960\n",
            "Epoch 18 Batch 150 Loss 0.0268 Accuracy 0.9960\n",
            "Epoch 18 Loss 0.0269 Accuracy 0.9960\n",
            "Time taken for 1 epoch: 26.91 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0184 Accuracy 0.9966\n",
            "Epoch 19 Batch 50 Loss 0.0260 Accuracy 0.9961\n",
            "Epoch 19 Batch 100 Loss 0.0262 Accuracy 0.9961\n",
            "Epoch 19 Batch 150 Loss 0.0268 Accuracy 0.9960\n",
            "Epoch 19 Loss 0.0273 Accuracy 0.9960\n",
            "Time taken for 1 epoch: 26.67 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0201 Accuracy 0.9964\n",
            "Epoch 20 Batch 50 Loss 0.0249 Accuracy 0.9965\n",
            "Epoch 20 Batch 100 Loss 0.0239 Accuracy 0.9965\n",
            "Epoch 20 Batch 150 Loss 0.0239 Accuracy 0.9964\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/syllablelevel/ckpt-4\n",
            "Epoch 20 Loss 0.0254 Accuracy 0.9962\n",
            "Time taken for 1 epoch: 27.08 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0278 Accuracy 0.9947\n",
            "Epoch 21 Batch 50 Loss 0.0276 Accuracy 0.9960\n",
            "Epoch 21 Batch 100 Loss 0.0271 Accuracy 0.9961\n",
            "Epoch 21 Batch 150 Loss 0.0267 Accuracy 0.9961\n",
            "Epoch 21 Loss 0.0268 Accuracy 0.9961\n",
            "Time taken for 1 epoch: 26.85 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0201 Accuracy 0.9975\n",
            "Epoch 22 Batch 50 Loss 0.0266 Accuracy 0.9959\n",
            "Epoch 22 Batch 100 Loss 0.0251 Accuracy 0.9962\n",
            "Epoch 22 Batch 150 Loss 0.0259 Accuracy 0.9961\n",
            "Epoch 22 Loss 0.0259 Accuracy 0.9962\n",
            "Time taken for 1 epoch: 26.60 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0236 Accuracy 0.9955\n",
            "Epoch 23 Batch 50 Loss 0.0266 Accuracy 0.9962\n",
            "Epoch 23 Batch 100 Loss 0.0249 Accuracy 0.9963\n",
            "Epoch 23 Batch 150 Loss 0.0263 Accuracy 0.9962\n",
            "Epoch 23 Loss 0.0262 Accuracy 0.9962\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0289 Accuracy 0.9941\n",
            "Epoch 24 Batch 50 Loss 0.0232 Accuracy 0.9964\n",
            "Epoch 24 Batch 100 Loss 0.0248 Accuracy 0.9963\n",
            "Epoch 24 Batch 150 Loss 0.0247 Accuracy 0.9963\n",
            "Epoch 24 Loss 0.0249 Accuracy 0.9963\n",
            "Time taken for 1 epoch: 26.58 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0292 Accuracy 0.9955\n",
            "Epoch 25 Batch 50 Loss 0.0260 Accuracy 0.9962\n",
            "Epoch 25 Batch 100 Loss 0.0255 Accuracy 0.9963\n",
            "Epoch 25 Batch 150 Loss 0.0249 Accuracy 0.9964\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/syllablelevel/ckpt-5\n",
            "Epoch 25 Loss 0.0246 Accuracy 0.9964\n",
            "Time taken for 1 epoch: 27.11 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0122 Accuracy 0.9978\n",
            "Epoch 26 Batch 50 Loss 0.0259 Accuracy 0.9963\n",
            "Epoch 26 Batch 100 Loss 0.0247 Accuracy 0.9965\n",
            "Epoch 26 Batch 150 Loss 0.0230 Accuracy 0.9967\n",
            "Epoch 26 Loss 0.0227 Accuracy 0.9967\n",
            "Time taken for 1 epoch: 26.64 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0229 Accuracy 0.9975\n",
            "Epoch 27 Batch 50 Loss 0.0230 Accuracy 0.9967\n",
            "Epoch 27 Batch 100 Loss 0.0231 Accuracy 0.9967\n",
            "Epoch 27 Batch 150 Loss 0.0220 Accuracy 0.9968\n",
            "Epoch 27 Loss 0.0213 Accuracy 0.9969\n",
            "Time taken for 1 epoch: 26.61 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0299 Accuracy 0.9963\n",
            "Epoch 28 Batch 50 Loss 0.0186 Accuracy 0.9972\n",
            "Epoch 28 Batch 100 Loss 0.0180 Accuracy 0.9974\n",
            "Epoch 28 Batch 150 Loss 0.0183 Accuracy 0.9973\n",
            "Epoch 28 Loss 0.0183 Accuracy 0.9973\n",
            "Time taken for 1 epoch: 26.59 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0160 Accuracy 0.9966\n",
            "Epoch 29 Batch 50 Loss 0.0173 Accuracy 0.9974\n",
            "Epoch 29 Batch 100 Loss 0.0181 Accuracy 0.9973\n",
            "Epoch 29 Batch 150 Loss 0.0176 Accuracy 0.9974\n",
            "Epoch 29 Loss 0.0174 Accuracy 0.9975\n",
            "Time taken for 1 epoch: 26.71 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0129 Accuracy 0.9983\n",
            "Epoch 30 Batch 50 Loss 0.0142 Accuracy 0.9979\n",
            "Epoch 30 Batch 100 Loss 0.0135 Accuracy 0.9981\n",
            "Epoch 30 Batch 150 Loss 0.0147 Accuracy 0.9978\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/syllablelevel/ckpt-6\n",
            "Epoch 30 Loss 0.0147 Accuracy 0.9978\n",
            "Time taken for 1 epoch: 27.07 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0082 Accuracy 0.9989\n",
            "Epoch 31 Batch 50 Loss 0.0180 Accuracy 0.9976\n",
            "Epoch 31 Batch 100 Loss 0.0166 Accuracy 0.9977\n",
            "Epoch 31 Batch 150 Loss 0.0158 Accuracy 0.9978\n",
            "Epoch 31 Loss 0.0156 Accuracy 0.9978\n",
            "Time taken for 1 epoch: 26.68 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0231 Accuracy 0.9966\n",
            "Epoch 32 Batch 50 Loss 0.0128 Accuracy 0.9980\n",
            "Epoch 32 Batch 100 Loss 0.0137 Accuracy 0.9980\n",
            "Epoch 32 Batch 150 Loss 0.0141 Accuracy 0.9980\n",
            "Epoch 32 Loss 0.0139 Accuracy 0.9980\n",
            "Time taken for 1 epoch: 26.69 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0133 Accuracy 0.9978\n",
            "Epoch 33 Batch 50 Loss 0.0131 Accuracy 0.9981\n",
            "Epoch 33 Batch 100 Loss 0.0123 Accuracy 0.9982\n",
            "Epoch 33 Batch 150 Loss 0.0128 Accuracy 0.9981\n",
            "Epoch 33 Loss 0.0127 Accuracy 0.9981\n",
            "Time taken for 1 epoch: 26.62 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0125 Accuracy 0.9983\n",
            "Epoch 34 Batch 50 Loss 0.0116 Accuracy 0.9984\n",
            "Epoch 34 Batch 100 Loss 0.0115 Accuracy 0.9982\n",
            "Epoch 34 Batch 150 Loss 0.0116 Accuracy 0.9982\n",
            "Epoch 34 Loss 0.0124 Accuracy 0.9981\n",
            "Time taken for 1 epoch: 26.66 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0114 Accuracy 0.9980\n",
            "Epoch 35 Batch 50 Loss 0.0132 Accuracy 0.9982\n",
            "Epoch 35 Batch 100 Loss 0.0125 Accuracy 0.9982\n",
            "Epoch 35 Batch 150 Loss 0.0117 Accuracy 0.9983\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/syllablelevel/ckpt-7\n",
            "Epoch 35 Loss 0.0116 Accuracy 0.9983\n",
            "Time taken for 1 epoch: 27.01 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0159 Accuracy 0.9983\n",
            "Epoch 36 Batch 50 Loss 0.0105 Accuracy 0.9986\n",
            "Epoch 36 Batch 100 Loss 0.0118 Accuracy 0.9983\n",
            "Epoch 36 Batch 150 Loss 0.0126 Accuracy 0.9982\n",
            "Epoch 36 Loss 0.0123 Accuracy 0.9983\n",
            "Time taken for 1 epoch: 26.64 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0080 Accuracy 0.9989\n",
            "Epoch 37 Batch 50 Loss 0.0117 Accuracy 0.9984\n",
            "Epoch 37 Batch 100 Loss 0.0111 Accuracy 0.9985\n",
            "Epoch 37 Batch 150 Loss 0.0107 Accuracy 0.9985\n",
            "Epoch 37 Loss 0.0108 Accuracy 0.9984\n",
            "Time taken for 1 epoch: 26.59 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0123 Accuracy 0.9986\n",
            "Epoch 38 Batch 50 Loss 0.0116 Accuracy 0.9983\n",
            "Epoch 38 Batch 100 Loss 0.0107 Accuracy 0.9984\n",
            "Epoch 38 Batch 150 Loss 0.0100 Accuracy 0.9985\n",
            "Epoch 38 Loss 0.0098 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 26.57 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0065 Accuracy 0.9983\n",
            "Epoch 39 Batch 50 Loss 0.0105 Accuracy 0.9986\n",
            "Epoch 39 Batch 100 Loss 0.0111 Accuracy 0.9984\n",
            "Epoch 39 Batch 150 Loss 0.0108 Accuracy 0.9984\n",
            "Epoch 39 Loss 0.0107 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 26.53 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0033 Accuracy 0.9994\n",
            "Epoch 40 Batch 50 Loss 0.0106 Accuracy 0.9985\n",
            "Epoch 40 Batch 100 Loss 0.0110 Accuracy 0.9984\n",
            "Epoch 40 Batch 150 Loss 0.0106 Accuracy 0.9985\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/syllablelevel/ckpt-8\n",
            "Epoch 40 Loss 0.0104 Accuracy 0.9985\n",
            "Time taken for 1 epoch: 26.95 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0084 Accuracy 0.9980\n",
            "Epoch 41 Batch 50 Loss 0.0082 Accuracy 0.9986\n",
            "Epoch 41 Batch 100 Loss 0.0082 Accuracy 0.9987\n",
            "Epoch 41 Batch 150 Loss 0.0086 Accuracy 0.9987\n",
            "Epoch 41 Loss 0.0083 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 26.61 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0017 Accuracy 0.9997\n",
            "Epoch 42 Batch 50 Loss 0.0106 Accuracy 0.9985\n",
            "Epoch 42 Batch 100 Loss 0.0100 Accuracy 0.9986\n",
            "Epoch 42 Batch 150 Loss 0.0096 Accuracy 0.9986\n",
            "Epoch 42 Loss 0.0096 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 26.55 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0077 Accuracy 0.9994\n",
            "Epoch 43 Batch 50 Loss 0.0083 Accuracy 0.9987\n",
            "Epoch 43 Batch 100 Loss 0.0092 Accuracy 0.9986\n",
            "Epoch 43 Batch 150 Loss 0.0087 Accuracy 0.9987\n",
            "Epoch 43 Loss 0.0086 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 26.55 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0027 Accuracy 0.9994\n",
            "Epoch 44 Batch 50 Loss 0.0086 Accuracy 0.9987\n",
            "Epoch 44 Batch 100 Loss 0.0086 Accuracy 0.9987\n",
            "Epoch 44 Batch 150 Loss 0.0083 Accuracy 0.9988\n",
            "Epoch 44 Loss 0.0084 Accuracy 0.9987\n",
            "Time taken for 1 epoch: 26.56 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0064 Accuracy 0.9989\n",
            "Epoch 45 Batch 50 Loss 0.0090 Accuracy 0.9985\n",
            "Epoch 45 Batch 100 Loss 0.0088 Accuracy 0.9986\n",
            "Epoch 45 Batch 150 Loss 0.0085 Accuracy 0.9986\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/syllablelevel/ckpt-9\n",
            "Epoch 45 Loss 0.0084 Accuracy 0.9986\n",
            "Time taken for 1 epoch: 27.07 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0072 Accuracy 0.9992\n",
            "Epoch 46 Batch 50 Loss 0.0071 Accuracy 0.9989\n",
            "Epoch 46 Batch 100 Loss 0.0072 Accuracy 0.9989\n",
            "Epoch 46 Batch 150 Loss 0.0073 Accuracy 0.9989\n",
            "Epoch 46 Loss 0.0072 Accuracy 0.9989\n",
            "Time taken for 1 epoch: 26.58 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0076 Accuracy 0.9992\n",
            "Epoch 47 Batch 50 Loss 0.0073 Accuracy 0.9991\n",
            "Epoch 47 Batch 100 Loss 0.0072 Accuracy 0.9990\n",
            "Epoch 47 Batch 150 Loss 0.0077 Accuracy 0.9989\n",
            "Epoch 47 Loss 0.0076 Accuracy 0.9989\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0041 Accuracy 0.9994\n",
            "Epoch 48 Batch 50 Loss 0.0058 Accuracy 0.9991\n",
            "Epoch 48 Batch 100 Loss 0.0059 Accuracy 0.9991\n",
            "Epoch 48 Batch 150 Loss 0.0061 Accuracy 0.9991\n",
            "Epoch 48 Loss 0.0064 Accuracy 0.9990\n",
            "Time taken for 1 epoch: 26.50 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0169 Accuracy 0.9980\n",
            "Epoch 49 Batch 50 Loss 0.0080 Accuracy 0.9989\n",
            "Epoch 49 Batch 100 Loss 0.0074 Accuracy 0.9990\n",
            "Epoch 49 Batch 150 Loss 0.0075 Accuracy 0.9989\n",
            "Epoch 49 Loss 0.0074 Accuracy 0.9989\n",
            "Time taken for 1 epoch: 26.53 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0037 Accuracy 0.9992\n",
            "Epoch 50 Batch 50 Loss 0.0059 Accuracy 0.9991\n",
            "Epoch 50 Batch 100 Loss 0.0057 Accuracy 0.9992\n",
            "Epoch 50 Batch 150 Loss 0.0057 Accuracy 0.9992\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/syllablelevel/ckpt-10\n",
            "Epoch 50 Loss 0.0056 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 26.92 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0002 Accuracy 1.0000\n",
            "Epoch 51 Batch 50 Loss 0.0061 Accuracy 0.9990\n",
            "Epoch 51 Batch 100 Loss 0.0063 Accuracy 0.9990\n",
            "Epoch 51 Batch 150 Loss 0.0058 Accuracy 0.9991\n",
            "Epoch 51 Loss 0.0056 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 26.52 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0056 Accuracy 0.9994\n",
            "Epoch 52 Batch 50 Loss 0.0063 Accuracy 0.9992\n",
            "Epoch 52 Batch 100 Loss 0.0061 Accuracy 0.9992\n",
            "Epoch 52 Batch 150 Loss 0.0059 Accuracy 0.9992\n",
            "Epoch 52 Loss 0.0061 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 26.52 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0059 Accuracy 0.9994\n",
            "Epoch 53 Batch 50 Loss 0.0082 Accuracy 0.9990\n",
            "Epoch 53 Batch 100 Loss 0.0085 Accuracy 0.9989\n",
            "Epoch 53 Batch 150 Loss 0.0075 Accuracy 0.9990\n",
            "Epoch 53 Loss 0.0072 Accuracy 0.9991\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0027 Accuracy 0.9994\n",
            "Epoch 54 Batch 50 Loss 0.0044 Accuracy 0.9994\n",
            "Epoch 54 Batch 100 Loss 0.0054 Accuracy 0.9993\n",
            "Epoch 54 Batch 150 Loss 0.0053 Accuracy 0.9993\n",
            "Epoch 54 Loss 0.0054 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0028 Accuracy 0.9994\n",
            "Epoch 55 Batch 50 Loss 0.0050 Accuracy 0.9993\n",
            "Epoch 55 Batch 100 Loss 0.0048 Accuracy 0.9993\n",
            "Epoch 55 Batch 150 Loss 0.0048 Accuracy 0.9993\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/syllablelevel/ckpt-11\n",
            "Epoch 55 Loss 0.0049 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 26.90 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0082 Accuracy 0.9994\n",
            "Epoch 56 Batch 50 Loss 0.0055 Accuracy 0.9992\n",
            "Epoch 56 Batch 100 Loss 0.0046 Accuracy 0.9993\n",
            "Epoch 56 Batch 150 Loss 0.0048 Accuracy 0.9993\n",
            "Epoch 56 Loss 0.0050 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 26.54 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0090 Accuracy 0.9994\n",
            "Epoch 57 Batch 50 Loss 0.0048 Accuracy 0.9993\n",
            "Epoch 57 Batch 100 Loss 0.0042 Accuracy 0.9994\n",
            "Epoch 57 Batch 150 Loss 0.0045 Accuracy 0.9994\n",
            "Epoch 57 Loss 0.0045 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.53 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0064 Accuracy 0.9992\n",
            "Epoch 58 Batch 50 Loss 0.0052 Accuracy 0.9994\n",
            "Epoch 58 Batch 100 Loss 0.0058 Accuracy 0.9992\n",
            "Epoch 58 Batch 150 Loss 0.0058 Accuracy 0.9992\n",
            "Epoch 58 Loss 0.0057 Accuracy 0.9992\n",
            "Time taken for 1 epoch: 26.54 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0112 Accuracy 0.9986\n",
            "Epoch 59 Batch 50 Loss 0.0056 Accuracy 0.9992\n",
            "Epoch 59 Batch 100 Loss 0.0048 Accuracy 0.9993\n",
            "Epoch 59 Batch 150 Loss 0.0044 Accuracy 0.9994\n",
            "Epoch 59 Loss 0.0044 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0058 Accuracy 0.9989\n",
            "Epoch 60 Batch 50 Loss 0.0043 Accuracy 0.9994\n",
            "Epoch 60 Batch 100 Loss 0.0047 Accuracy 0.9993\n",
            "Epoch 60 Batch 150 Loss 0.0049 Accuracy 0.9993\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/syllablelevel/ckpt-12\n",
            "Epoch 60 Loss 0.0048 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 26.91 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0014 Accuracy 0.9994\n",
            "Epoch 61 Batch 50 Loss 0.0075 Accuracy 0.9988\n",
            "Epoch 61 Batch 100 Loss 0.0074 Accuracy 0.9988\n",
            "Epoch 61 Batch 150 Loss 0.0060 Accuracy 0.9990\n",
            "Epoch 61 Loss 0.0057 Accuracy 0.9991\n",
            "Time taken for 1 epoch: 26.56 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0033 Accuracy 0.9992\n",
            "Epoch 62 Batch 50 Loss 0.0033 Accuracy 0.9995\n",
            "Epoch 62 Batch 100 Loss 0.0042 Accuracy 0.9994\n",
            "Epoch 62 Batch 150 Loss 0.0042 Accuracy 0.9994\n",
            "Epoch 62 Loss 0.0042 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.47 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0068 Accuracy 0.9997\n",
            "Epoch 63 Batch 50 Loss 0.0042 Accuracy 0.9995\n",
            "Epoch 63 Batch 100 Loss 0.0045 Accuracy 0.9994\n",
            "Epoch 63 Batch 150 Loss 0.0046 Accuracy 0.9994\n",
            "Epoch 63 Loss 0.0046 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.50 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0029 Accuracy 0.9994\n",
            "Epoch 64 Batch 50 Loss 0.0033 Accuracy 0.9995\n",
            "Epoch 64 Batch 100 Loss 0.0034 Accuracy 0.9995\n",
            "Epoch 64 Batch 150 Loss 0.0034 Accuracy 0.9995\n",
            "Epoch 64 Loss 0.0034 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.44 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0019 Accuracy 0.9994\n",
            "Epoch 65 Batch 50 Loss 0.0052 Accuracy 0.9994\n",
            "Epoch 65 Batch 100 Loss 0.0047 Accuracy 0.9994\n",
            "Epoch 65 Batch 150 Loss 0.0043 Accuracy 0.9994\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/syllablelevel/ckpt-13\n",
            "Epoch 65 Loss 0.0042 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.98 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0063 Accuracy 0.9994\n",
            "Epoch 66 Batch 50 Loss 0.0038 Accuracy 0.9995\n",
            "Epoch 66 Batch 100 Loss 0.0043 Accuracy 0.9994\n",
            "Epoch 66 Batch 150 Loss 0.0045 Accuracy 0.9994\n",
            "Epoch 66 Loss 0.0046 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.67 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0064 Accuracy 0.9992\n",
            "Epoch 67 Batch 50 Loss 0.0041 Accuracy 0.9994\n",
            "Epoch 67 Batch 100 Loss 0.0042 Accuracy 0.9994\n",
            "Epoch 67 Batch 150 Loss 0.0044 Accuracy 0.9994\n",
            "Epoch 67 Loss 0.0048 Accuracy 0.9993\n",
            "Time taken for 1 epoch: 26.53 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0042 Accuracy 0.9992\n",
            "Epoch 68 Batch 50 Loss 0.0036 Accuracy 0.9994\n",
            "Epoch 68 Batch 100 Loss 0.0040 Accuracy 0.9994\n",
            "Epoch 68 Batch 150 Loss 0.0039 Accuracy 0.9994\n",
            "Epoch 68 Loss 0.0039 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.56 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0043 Accuracy 0.9994\n",
            "Epoch 69 Batch 50 Loss 0.0061 Accuracy 0.9991\n",
            "Epoch 69 Batch 100 Loss 0.0048 Accuracy 0.9993\n",
            "Epoch 69 Batch 150 Loss 0.0044 Accuracy 0.9994\n",
            "Epoch 69 Loss 0.0043 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.50 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0016 Accuracy 0.9994\n",
            "Epoch 70 Batch 50 Loss 0.0056 Accuracy 0.9992\n",
            "Epoch 70 Batch 100 Loss 0.0045 Accuracy 0.9993\n",
            "Epoch 70 Batch 150 Loss 0.0041 Accuracy 0.9994\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/syllablelevel/ckpt-14\n",
            "Epoch 70 Loss 0.0039 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.91 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0003 Accuracy 1.0000\n",
            "Epoch 71 Batch 50 Loss 0.0034 Accuracy 0.9995\n",
            "Epoch 71 Batch 100 Loss 0.0043 Accuracy 0.9994\n",
            "Epoch 71 Batch 150 Loss 0.0045 Accuracy 0.9993\n",
            "Epoch 71 Loss 0.0044 Accuracy 0.9994\n",
            "Time taken for 1 epoch: 26.56 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0012 Accuracy 0.9997\n",
            "Epoch 72 Batch 50 Loss 0.0033 Accuracy 0.9995\n",
            "Epoch 72 Batch 100 Loss 0.0033 Accuracy 0.9995\n",
            "Epoch 72 Batch 150 Loss 0.0029 Accuracy 0.9996\n",
            "Epoch 72 Loss 0.0029 Accuracy 0.9996\n",
            "Time taken for 1 epoch: 26.51 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0025 Accuracy 0.9997\n",
            "Epoch 73 Batch 50 Loss 0.0032 Accuracy 0.9995\n",
            "Epoch 73 Batch 100 Loss 0.0026 Accuracy 0.9996\n",
            "Epoch 73 Batch 150 Loss 0.0032 Accuracy 0.9996\n",
            "Epoch 73 Loss 0.0032 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.52 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0011 Accuracy 0.9997\n",
            "Epoch 74 Batch 50 Loss 0.0035 Accuracy 0.9995\n",
            "Epoch 74 Batch 100 Loss 0.0036 Accuracy 0.9995\n",
            "Epoch 74 Batch 150 Loss 0.0032 Accuracy 0.9995\n",
            "Epoch 74 Loss 0.0033 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.54 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0001 Accuracy 1.0000\n",
            "Epoch 75 Batch 50 Loss 0.0040 Accuracy 0.9995\n",
            "Epoch 75 Batch 100 Loss 0.0037 Accuracy 0.9995\n",
            "Epoch 75 Batch 150 Loss 0.0036 Accuracy 0.9995\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/syllablelevel/ckpt-15\n",
            "Epoch 75 Loss 0.0035 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.91 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0024 Accuracy 0.9997\n",
            "Epoch 76 Batch 50 Loss 0.0051 Accuracy 0.9994\n",
            "Epoch 76 Batch 100 Loss 0.0039 Accuracy 0.9995\n",
            "Epoch 76 Batch 150 Loss 0.0037 Accuracy 0.9995\n",
            "Epoch 76 Loss 0.0035 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.57 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0035 Accuracy 0.9997\n",
            "Epoch 77 Batch 50 Loss 0.0024 Accuracy 0.9997\n",
            "Epoch 77 Batch 100 Loss 0.0027 Accuracy 0.9996\n",
            "Epoch 77 Batch 150 Loss 0.0029 Accuracy 0.9996\n",
            "Epoch 77 Loss 0.0029 Accuracy 0.9996\n",
            "Time taken for 1 epoch: 26.57 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0045 Accuracy 0.9994\n",
            "Epoch 78 Batch 50 Loss 0.0034 Accuracy 0.9995\n",
            "Epoch 78 Batch 100 Loss 0.0037 Accuracy 0.9995\n",
            "Epoch 78 Batch 150 Loss 0.0034 Accuracy 0.9995\n",
            "Epoch 78 Loss 0.0033 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.56 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0003 Accuracy 1.0000\n",
            "Epoch 79 Batch 50 Loss 0.0032 Accuracy 0.9996\n",
            "Epoch 79 Batch 100 Loss 0.0032 Accuracy 0.9996\n",
            "Epoch 79 Batch 150 Loss 0.0031 Accuracy 0.9996\n",
            "Epoch 79 Loss 0.0030 Accuracy 0.9996\n",
            "Time taken for 1 epoch: 26.48 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0046 Accuracy 0.9991\n",
            "Epoch 80 Batch 50 Loss 0.0027 Accuracy 0.9996\n",
            "Epoch 80 Batch 100 Loss 0.0032 Accuracy 0.9995\n",
            "Epoch 80 Batch 150 Loss 0.0033 Accuracy 0.9995\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/syllablelevel/ckpt-16\n",
            "Epoch 80 Loss 0.0033 Accuracy 0.9995\n",
            "Time taken for 1 epoch: 26.88 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vbeVUzwtoTE",
        "outputId": "a676ede2-b64a-4918-99bb-8f0f2c7b200e"
      },
      "source": [
        "!tar chvfz checkpoints.tar.gz checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints/\n",
            "checkpoints/syllablelevel/\n",
            "checkpoints/syllablelevel/checkpoint\n",
            "checkpoints/syllablelevel/ckpt-15.data-00000-of-00001\n",
            "checkpoints/syllablelevel/ckpt-13.index\n",
            "checkpoints/syllablelevel/ckpt-14.index\n",
            "checkpoints/syllablelevel/ckpt-14.data-00000-of-00001\n",
            "checkpoints/syllablelevel/ckpt-12.index\n",
            "checkpoints/syllablelevel/ckpt-16.data-00000-of-00001\n",
            "checkpoints/syllablelevel/ckpt-15.index\n",
            "checkpoints/syllablelevel/ckpt-12.data-00000-of-00001\n",
            "checkpoints/syllablelevel/ckpt-13.data-00000-of-00001\n",
            "checkpoints/syllablelevel/ckpt-16.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44l1saVuebS"
      },
      "source": [
        "We define the *evaluate* function to preprocess the sentence in input to the encoder and to get the predicted ids of the translation.\n",
        "\n",
        "The ids of the translation are obtained by applying *argmax* to the predicted logits of the decoder.\n",
        "\n",
        "We begin feeding the decoder with the id of the start symbol and, at each new step, we pass to the decoder the sequence it has just thrown out.\n",
        "\n",
        "The translation stops when the end symbol is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh7X-rIaJHyl",
        "outputId": "d8b530ee-fe8b-4551-a273-493a51edbc1c"
      },
      "source": [
        "sentence = \"quanto disobediendo intese ir suso;\"\n",
        "encoder_input = preprocess(sentence)\n",
        "encoder_input = input_tokenizer.texts_to_sequences(encoder_input)\n",
        "encoder_input = pad(encoder_input)\n",
        "print(encoder_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    2    87 17042  3716   870  1319     3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "BSNaKtSkvxcJ"
      },
      "source": [
        "def evaluate(sentence, max_length=200):\n",
        "\n",
        "    #encoder_input = preprocess(sentence)\n",
        "    encoder_input = tokenizer.texts_to_sequences(sentence)\n",
        "    encoder_input = [x for l in encoder_input for x in l]\n",
        "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [encoder_input], maxlen=max_length, padding=\"post\"\n",
        "    )\n",
        "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "\n",
        "    output = tf.convert_to_tensor([tokenizer.word_index[\"^\"]])\n",
        "    output = tf.expand_dims(output, 0)\n",
        "    result = \"\"\n",
        "\n",
        "    for i in range(600):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "\n",
        "        if predicted_id == tokenizer.word_index[\"$\"]:\n",
        "          result += \"\\n\"\n",
        "        if result.count(\"$\") == 6:\n",
        "          break\n",
        "\n",
        "    # output.shape (1, tokens)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "PkCDxGdeyF9f"
      },
      "source": [
        "def print_translation(sentence, result):\n",
        "    print(f'{\"Input:\":15s}: {sentence}')\n",
        "    print(f'{\"Prediction\":15s}: {result}')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbKZbzeL000f",
        "outputId": "5ff5a04b-e381-4e62-a952-e8622c7041fe"
      },
      "source": [
        "sentence = \"^\"\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^\n",
            "Prediction     : Non far, tr’ Ho gë sù sù Ma feo; $ \n",
            "^ Non gë $ \n",
            "^ Ma $ \n",
            "^ Non no?». Non cò gë gë Ma tri:« sù nò:« Non gë sù Ma sur $ \n",
            "^ Ma $ \n",
            "^ Non no?». sù cò sù gë Tem tri:« sù vuo’ sù Non feo; $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlPsnGGTX5Dq",
        "outputId": "8c481a85-8110-45cb-b391-fbd5c82dde57"
      },
      "source": [
        "sentence = \"^quanto disobediendo intese ir suso;\"\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^quanto disobediendo intese ir suso;\n",
            "Prediction     : fu u a fu fu o fu i mio, o mai: e bur i e svel $ \n",
            "^ mai diè e e Pie e i fu fé u ghi o mio, $ \n",
            "^ o $ \n",
            "^ o mai: $ \n",
            "^ i svel le”. $ \n",
            "^ o diè o se:— o fu rum $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gradient": {},
        "id": "V33sFY3iyLLE",
        "outputId": "a4e99d91-a24d-43b1-858c-d08fe805f2fd"
      },
      "source": [
        "sentence = \"^Buonasera a tutti\"\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : ^Buonasera a tutti\n",
            "Prediction     : diè u o re; a va”. e fui; a a fuo u fu $ \n",
            "^ i a fuo u fu cea,« $ \n",
            "^ a fu u fu va”. i a fuo fu $ \n",
            "^ a $ \n",
            "^ a u ma». $ \n",
            "^ a le”. $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPLXjcQVriKf",
        "outputId": "2c35aa59-bb8b-4b8b-f987-99905781a1bd"
      },
      "source": [
        "sentence = \"Io non so ben ridir com’ i’ v’intrai,\"\n",
        "\n",
        "translated_text = evaluate(sentence)\n",
        "print_translation(sentence, translated_text)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : Io non so ben ridir com’ i’ v’intrai,\n",
            "Prediction     : I gra gra o gra o gra e Bo gra i gra i $ \n",
            "^ o ’ in gra ’ sien ’ i cre gra gra a i ve puoi», i da!»; $ \n",
            "^ $ \n",
            "^ ’ $ \n",
            "^ ’ gra gra Ia gra $ \n",
            "^ ’ e ’ gra mai qui men $ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}