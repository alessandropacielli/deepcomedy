{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word-input char-output transformer generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcDazsVbL_tS"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "aBRRcbZMBZ5U",
        "outputId": "fc4583b8-ed14-4bb3-84a6-af080ed1acb6"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import files\n",
        "    \n",
        "    files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05816771-b212-46b4-b5b6-a038a6fd77e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-05816771-b212-46b4-b5b6-a038a6fd77e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.zip to data.zip\n",
            "Saving deepcomedy.zip to deepcomedy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTxmMPGM04O",
        "outputId": "907722ea-1f4f-45a3-a91b-82745a079ea0"
      },
      "source": [
        "!pip install wandb\n",
        "#!tar zxvf deepcomedy.tar.gz\n",
        "!unzip deepcomedy.zip\n",
        "#!tar zxvf data.tar.gz\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.1MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=5b7d624cae9136dfa39a121bc48199566b480d215afa8e3273781e9f9f2df1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=864f44b711cf4dffc01cd415f9ae4903fc77f2652148a456bd6ec649b3827867\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: pathtools, shortuuid, subprocess32, docker-pycreds, configparser, sentry-sdk, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.30\n",
            "Archive:  deepcomedy.zip\n",
            "   creating: deepcomedy/models/\n",
            "  inflating: deepcomedy/models/layers.py  \n",
            "  inflating: deepcomedy/models/transformer.py  \n",
            "  inflating: deepcomedy/preprocessing.py  \n",
            "   creating: deepcomedy/util/\n",
            "  inflating: deepcomedy/util/predicate.py  \n",
            "Archive:  data.zip\n",
            "  inflating: data/divina_syll_textonly.txt  \n",
            "  inflating: data/divina_textonly.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {},
        "id": "54j16swJY1dW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from deepcomedy.models.transformer import *\n",
        "from deepcomedy.preprocessing import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMqNB4ujuT",
        "tags": []
      },
      "source": [
        "## 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7n29oxB0z4"
      },
      "source": [
        "raw_text = open(\"./data/divina_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        "raw_syll_text = (\n",
        "    open(\"./data/divina_syll_textonly.txt\", \"rb\").read().decode(encoding=\"utf-8\")\n",
        ")\n",
        "syll_text = preprocess_text(raw_syll_text, end_of_verse = \"\")\n",
        "text = preprocess_text(raw_text, end_of_verse = \"\", word_level= True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ASHyaMBC84V"
      },
      "source": [
        "Split preprocessed text into verses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IF6sE6FC_4J"
      },
      "source": [
        "sep = \"<EOT>\"\n",
        "input_tercets = [x + sep for x in text.split(sep)][:-1]\n",
        "target_tercets = [x + sep for x in syll_text.split(sep)][:-1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdUmYhUKDEuj"
      },
      "source": [
        "Encode with input and target tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mob1kOzDD4z"
      },
      "source": [
        "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=False, filters=\"\", lower=False\n",
        ")\n",
        "input_tokenizer.fit_on_texts(input_tercets)\n",
        "\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    char_level=False, filters=\"\", lower=False\n",
        ")\n",
        "target_tokenizer.fit_on_texts(target_tercets)\n",
        "\n",
        "enc_input_tercets = input_tokenizer.texts_to_sequences(input_tercets)\n",
        "enc_target_tercets = target_tokenizer.texts_to_sequences(target_tercets)\n",
        "\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKv92yAL_t8"
      },
      "source": [
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "for line in range(len(enc_input_tercets) - 2):\n",
        "    input_text.append(list(chain(*enc_input_tercets[line : line + 2])))\n",
        "    target_text.append(list(chain(*enc_target_tercets[line + 1 : line + 3])))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY44HP5lKz2-"
      },
      "source": [
        "Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq34y57yK3wd"
      },
      "source": [
        "padded_input_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_text, padding=\"post\"\n",
        ")\n",
        "padded_target_text = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_text, padding=\"post\"\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjAxjw_8L_uB"
      },
      "source": [
        "input_train, input_test, target_train, target_test = train_test_split(\n",
        "    padded_input_text, padded_target_text\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GVc41zvvdR9"
      },
      "source": [
        "## 2. The Transformer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8YfOoJqLNDJ"
      },
      "source": [
        "def make_dataset(input_verses, target_verses, batch_size):\n",
        "    buffer_size = len(input_verses)\n",
        "\n",
        "    steps_per_epoch = len(input_verses) // batch_size\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (input_train, target_train)\n",
        "    ).shuffle(buffer_size)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v2e1bIcLRZi"
      },
      "source": [
        "batch_size = 32\n",
        "dataset = make_dataset(input_train, target_train, batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ANsBVkyLZE6"
      },
      "source": [
        "def make_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path = None):\n",
        "    transformer = Transformer(\n",
        "        num_layers=config[\"num_layers\"],\n",
        "        d_model=config[\"d_model\"],\n",
        "        num_heads=config[\"num_heads\"],\n",
        "        dff=config[\"dff\"],\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        pe_input=1000,\n",
        "        pe_target=1000,\n",
        "        rate=0.1,\n",
        "    )\n",
        "    transformer_trainer = TransformerTrainer(transformer, checkpoint_save_path= checkpoint_save_path)\n",
        "\n",
        "    return transformer, transformer_trainer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sGOf__BLZzQ"
      },
      "source": [
        "config = {\n",
        "    \"num_layers\" : 6,\n",
        "    \"d_model\" : 256,\n",
        "    \"num_heads\" : 8,\n",
        "    \"dff\" : 1024\n",
        "}\n",
        "\n",
        "checkpoint_save_path = \"./checkpoints/word-input_char-output_gen\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miEcmOVmL0Rt"
      },
      "source": [
        "transformer, transformer_trainer = make_model(config, input_vocab_size, target_vocab_size, checkpoint_save_path= checkpoint_save_path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PLTOETK4_m6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QaR03YUNL_uF",
        "outputId": "46a89330-ed3a-49f2-f74b-fcee55ff6622"
      },
      "source": [
        "transformer_trainer.train(dataset, 100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.1266 Accuracy 0.0092\n",
            "Epoch 1 Batch 50 Loss 3.9077 Accuracy 0.1471\n",
            "Epoch 1 Batch 100 Loss 3.4774 Accuracy 0.1799\n",
            "Epoch 1 Loss 3.4281 Accuracy 0.1836\n",
            "Time taken for 1 epoch: 57.82 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.9811 Accuracy 0.2214\n",
            "Epoch 2 Batch 50 Loss 2.8181 Accuracy 0.2403\n",
            "Epoch 2 Batch 100 Loss 2.6015 Accuracy 0.2721\n",
            "Epoch 2 Loss 2.5633 Accuracy 0.2777\n",
            "Time taken for 1 epoch: 46.03 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.2175 Accuracy 0.3313\n",
            "Epoch 3 Batch 50 Loss 2.1317 Accuracy 0.3420\n",
            "Epoch 3 Batch 100 Loss 2.0948 Accuracy 0.3460\n",
            "Epoch 3 Loss 2.0879 Accuracy 0.3467\n",
            "Time taken for 1 epoch: 45.73 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.0237 Accuracy 0.3517\n",
            "Epoch 4 Batch 50 Loss 2.0013 Accuracy 0.3578\n",
            "Epoch 4 Batch 100 Loss 1.9907 Accuracy 0.3594\n",
            "Epoch 4 Loss 1.9890 Accuracy 0.3595\n",
            "Time taken for 1 epoch: 45.86 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.9899 Accuracy 0.3602\n",
            "Epoch 5 Batch 50 Loss 1.9557 Accuracy 0.3658\n",
            "Epoch 5 Batch 100 Loss 1.9474 Accuracy 0.3682\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/word-input_char-output_gen/ckpt-1\n",
            "Epoch 5 Loss 1.9461 Accuracy 0.3689\n",
            "Time taken for 1 epoch: 46.76 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.9284 Accuracy 0.3778\n",
            "Epoch 6 Batch 50 Loss 1.9127 Accuracy 0.3776\n",
            "Epoch 6 Batch 100 Loss 1.9001 Accuracy 0.3812\n",
            "Epoch 6 Loss 1.8978 Accuracy 0.3821\n",
            "Time taken for 1 epoch: 45.87 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.8764 Accuracy 0.3886\n",
            "Epoch 7 Batch 50 Loss 1.8489 Accuracy 0.3961\n",
            "Epoch 7 Batch 100 Loss 1.8292 Accuracy 0.4017\n",
            "Epoch 7 Loss 1.8246 Accuracy 0.4030\n",
            "Time taken for 1 epoch: 45.69 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.7847 Accuracy 0.4153\n",
            "Epoch 8 Batch 50 Loss 1.7554 Accuracy 0.4210\n",
            "Epoch 8 Batch 100 Loss 1.7389 Accuracy 0.4270\n",
            "Epoch 8 Loss 1.7343 Accuracy 0.4283\n",
            "Time taken for 1 epoch: 45.82 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.7117 Accuracy 0.4359\n",
            "Epoch 9 Batch 50 Loss 1.6783 Accuracy 0.4460\n",
            "Epoch 9 Batch 100 Loss 1.6667 Accuracy 0.4504\n",
            "Epoch 9 Loss 1.6642 Accuracy 0.4512\n",
            "Time taken for 1 epoch: 45.76 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.6411 Accuracy 0.4628\n",
            "Epoch 10 Batch 50 Loss 1.6116 Accuracy 0.4686\n",
            "Epoch 10 Batch 100 Loss 1.6025 Accuracy 0.4708\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/word-input_char-output_gen/ckpt-2\n",
            "Epoch 10 Loss 1.6002 Accuracy 0.4717\n",
            "Time taken for 1 epoch: 46.46 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.5585 Accuracy 0.4881\n",
            "Epoch 11 Batch 50 Loss 1.5539 Accuracy 0.4854\n",
            "Epoch 11 Batch 100 Loss 1.5409 Accuracy 0.4899\n",
            "Epoch 11 Loss 1.5381 Accuracy 0.4908\n",
            "Time taken for 1 epoch: 45.83 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.5168 Accuracy 0.4997\n",
            "Epoch 12 Batch 50 Loss 1.4950 Accuracy 0.5036\n",
            "Epoch 12 Batch 100 Loss 1.4883 Accuracy 0.5053\n",
            "Epoch 12 Loss 1.4873 Accuracy 0.5055\n",
            "Time taken for 1 epoch: 45.67 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.4548 Accuracy 0.5135\n",
            "Epoch 13 Batch 50 Loss 1.4562 Accuracy 0.5150\n",
            "Epoch 13 Batch 100 Loss 1.4457 Accuracy 0.5187\n",
            "Epoch 13 Loss 1.4434 Accuracy 0.5196\n",
            "Time taken for 1 epoch: 45.77 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.4238 Accuracy 0.5227\n",
            "Epoch 14 Batch 50 Loss 1.4111 Accuracy 0.5309\n",
            "Epoch 14 Batch 100 Loss 1.4040 Accuracy 0.5323\n",
            "Epoch 14 Loss 1.4017 Accuracy 0.5330\n",
            "Time taken for 1 epoch: 45.76 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.3689 Accuracy 0.5447\n",
            "Epoch 15 Batch 50 Loss 1.3681 Accuracy 0.5443\n",
            "Epoch 15 Batch 100 Loss 1.3618 Accuracy 0.5464\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/word-input_char-output_gen/ckpt-3\n",
            "Epoch 15 Loss 1.3609 Accuracy 0.5467\n",
            "Time taken for 1 epoch: 46.50 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.3543 Accuracy 0.5461\n",
            "Epoch 16 Batch 50 Loss 1.3308 Accuracy 0.5560\n",
            "Epoch 16 Batch 100 Loss 1.3248 Accuracy 0.5583\n",
            "Epoch 16 Loss 1.3236 Accuracy 0.5589\n",
            "Time taken for 1 epoch: 45.77 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.3014 Accuracy 0.5586\n",
            "Epoch 17 Batch 50 Loss 1.2932 Accuracy 0.5689\n",
            "Epoch 17 Batch 100 Loss 1.2894 Accuracy 0.5702\n",
            "Epoch 17 Loss 1.2878 Accuracy 0.5707\n",
            "Time taken for 1 epoch: 45.81 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.2675 Accuracy 0.5724\n",
            "Epoch 18 Batch 50 Loss 1.2583 Accuracy 0.5803\n",
            "Epoch 18 Batch 100 Loss 1.2553 Accuracy 0.5815\n",
            "Epoch 18 Loss 1.2534 Accuracy 0.5821\n",
            "Time taken for 1 epoch: 45.86 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.2209 Accuracy 0.5918\n",
            "Epoch 19 Batch 50 Loss 1.2244 Accuracy 0.5919\n",
            "Epoch 19 Batch 100 Loss 1.2221 Accuracy 0.5928\n",
            "Epoch 19 Loss 1.2215 Accuracy 0.5931\n",
            "Time taken for 1 epoch: 45.63 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.1924 Accuracy 0.5986\n",
            "Epoch 20 Batch 50 Loss 1.1959 Accuracy 0.6026\n",
            "Epoch 20 Batch 100 Loss 1.1911 Accuracy 0.6036\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/word-input_char-output_gen/ckpt-4\n",
            "Epoch 20 Loss 1.1903 Accuracy 0.6039\n",
            "Time taken for 1 epoch: 46.42 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.1432 Accuracy 0.6188\n",
            "Epoch 21 Batch 50 Loss 1.1634 Accuracy 0.6128\n",
            "Epoch 21 Batch 100 Loss 1.1592 Accuracy 0.6144\n",
            "Epoch 21 Loss 1.1578 Accuracy 0.6150\n",
            "Time taken for 1 epoch: 45.72 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.1663 Accuracy 0.6191\n",
            "Epoch 22 Batch 50 Loss 1.1295 Accuracy 0.6240\n",
            "Epoch 22 Batch 100 Loss 1.1269 Accuracy 0.6250\n",
            "Epoch 22 Loss 1.1263 Accuracy 0.6252\n",
            "Time taken for 1 epoch: 45.67 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.1005 Accuracy 0.6286\n",
            "Epoch 23 Batch 50 Loss 1.1015 Accuracy 0.6327\n",
            "Epoch 23 Batch 100 Loss 1.1025 Accuracy 0.6321\n",
            "Epoch 23 Loss 1.1017 Accuracy 0.6322\n",
            "Time taken for 1 epoch: 45.70 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.0674 Accuracy 0.6466\n",
            "Epoch 24 Batch 50 Loss 1.0811 Accuracy 0.6389\n",
            "Epoch 24 Batch 100 Loss 1.0823 Accuracy 0.6385\n",
            "Epoch 24 Loss 1.0814 Accuracy 0.6389\n",
            "Time taken for 1 epoch: 45.73 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.0628 Accuracy 0.6449\n",
            "Epoch 25 Batch 50 Loss 1.0629 Accuracy 0.6453\n",
            "Epoch 25 Batch 100 Loss 1.0627 Accuracy 0.6453\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/word-input_char-output_gen/ckpt-5\n",
            "Epoch 25 Loss 1.0625 Accuracy 0.6452\n",
            "Time taken for 1 epoch: 46.33 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.0542 Accuracy 0.6446\n",
            "Epoch 26 Batch 50 Loss 1.0409 Accuracy 0.6516\n",
            "Epoch 26 Batch 100 Loss 1.0439 Accuracy 0.6507\n",
            "Epoch 26 Loss 1.0452 Accuracy 0.6503\n",
            "Time taken for 1 epoch: 45.69 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.0246 Accuracy 0.6545\n",
            "Epoch 27 Batch 50 Loss 1.0254 Accuracy 0.6564\n",
            "Epoch 27 Batch 100 Loss 1.0275 Accuracy 0.6556\n",
            "Epoch 27 Loss 1.0274 Accuracy 0.6557\n",
            "Time taken for 1 epoch: 45.71 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.0172 Accuracy 0.6581\n",
            "Epoch 28 Batch 50 Loss 1.0097 Accuracy 0.6610\n",
            "Epoch 28 Batch 100 Loss 1.0120 Accuracy 0.6605\n",
            "Epoch 28 Loss 1.0125 Accuracy 0.6603\n",
            "Time taken for 1 epoch: 45.59 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.9816 Accuracy 0.6695\n",
            "Epoch 29 Batch 50 Loss 0.9942 Accuracy 0.6660\n",
            "Epoch 29 Batch 100 Loss 0.9972 Accuracy 0.6653\n",
            "Epoch 29 Loss 0.9977 Accuracy 0.6651\n",
            "Time taken for 1 epoch: 45.58 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.9982 Accuracy 0.6677\n",
            "Epoch 30 Batch 50 Loss 0.9773 Accuracy 0.6709\n",
            "Epoch 30 Batch 100 Loss 0.9818 Accuracy 0.6693\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/word-input_char-output_gen/ckpt-6\n",
            "Epoch 30 Loss 0.9820 Accuracy 0.6694\n",
            "Time taken for 1 epoch: 46.30 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.9402 Accuracy 0.6839\n",
            "Epoch 31 Batch 50 Loss 0.9615 Accuracy 0.6758\n",
            "Epoch 31 Batch 100 Loss 0.9673 Accuracy 0.6744\n",
            "Epoch 31 Loss 0.9680 Accuracy 0.6742\n",
            "Time taken for 1 epoch: 45.69 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.9365 Accuracy 0.6857\n",
            "Epoch 32 Batch 50 Loss 0.9509 Accuracy 0.6792\n",
            "Epoch 32 Batch 100 Loss 0.9570 Accuracy 0.6775\n",
            "Epoch 32 Loss 0.9579 Accuracy 0.6772\n",
            "Time taken for 1 epoch: 45.64 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.9196 Accuracy 0.6899\n",
            "Epoch 33 Batch 50 Loss 0.9396 Accuracy 0.6831\n",
            "Epoch 33 Batch 100 Loss 0.9437 Accuracy 0.6815\n",
            "Epoch 33 Loss 0.9447 Accuracy 0.6813\n",
            "Time taken for 1 epoch: 45.56 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.9465 Accuracy 0.6794\n",
            "Epoch 34 Batch 50 Loss 0.9257 Accuracy 0.6874\n",
            "Epoch 34 Batch 100 Loss 0.9316 Accuracy 0.6855\n",
            "Epoch 34 Loss 0.9324 Accuracy 0.6854\n",
            "Time taken for 1 epoch: 45.52 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.9119 Accuracy 0.6938\n",
            "Epoch 35 Batch 50 Loss 0.9127 Accuracy 0.6913\n",
            "Epoch 35 Batch 100 Loss 0.9203 Accuracy 0.6889\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/word-input_char-output_gen/ckpt-7\n",
            "Epoch 35 Loss 0.9217 Accuracy 0.6885\n",
            "Time taken for 1 epoch: 46.32 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.8677 Accuracy 0.7003\n",
            "Epoch 36 Batch 50 Loss 0.8962 Accuracy 0.6966\n",
            "Epoch 36 Batch 100 Loss 0.9080 Accuracy 0.6929\n",
            "Epoch 36 Loss 0.9084 Accuracy 0.6928\n",
            "Time taken for 1 epoch: 45.50 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.8907 Accuracy 0.6948\n",
            "Epoch 37 Batch 50 Loss 0.8879 Accuracy 0.6994\n",
            "Epoch 37 Batch 100 Loss 0.8948 Accuracy 0.6972\n",
            "Epoch 37 Loss 0.8958 Accuracy 0.6969\n",
            "Time taken for 1 epoch: 45.52 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.8812 Accuracy 0.6965\n",
            "Epoch 38 Batch 50 Loss 0.8707 Accuracy 0.7045\n",
            "Epoch 38 Batch 100 Loss 0.8773 Accuracy 0.7027\n",
            "Epoch 38 Loss 0.8790 Accuracy 0.7022\n",
            "Time taken for 1 epoch: 45.58 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.8737 Accuracy 0.7026\n",
            "Epoch 39 Batch 50 Loss 0.8531 Accuracy 0.7099\n",
            "Epoch 39 Batch 100 Loss 0.8624 Accuracy 0.7072\n",
            "Epoch 39 Loss 0.8637 Accuracy 0.7067\n",
            "Time taken for 1 epoch: 45.59 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.8440 Accuracy 0.7162\n",
            "Epoch 40 Batch 50 Loss 0.8407 Accuracy 0.7148\n",
            "Epoch 40 Batch 100 Loss 0.8466 Accuracy 0.7127\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/word-input_char-output_gen/ckpt-8\n",
            "Epoch 40 Loss 0.8480 Accuracy 0.7124\n",
            "Time taken for 1 epoch: 46.32 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.8005 Accuracy 0.7297\n",
            "Epoch 41 Batch 50 Loss 0.8220 Accuracy 0.7210\n",
            "Epoch 41 Batch 100 Loss 0.8297 Accuracy 0.7181\n",
            "Epoch 41 Loss 0.8317 Accuracy 0.7174\n",
            "Time taken for 1 epoch: 45.70 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.8035 Accuracy 0.7299\n",
            "Epoch 42 Batch 50 Loss 0.8044 Accuracy 0.7260\n",
            "Epoch 42 Batch 100 Loss 0.8130 Accuracy 0.7237\n",
            "Epoch 42 Loss 0.8146 Accuracy 0.7232\n",
            "Time taken for 1 epoch: 45.84 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.7969 Accuracy 0.7302\n",
            "Epoch 43 Batch 50 Loss 0.7899 Accuracy 0.7303\n",
            "Epoch 43 Batch 100 Loss 0.7992 Accuracy 0.7277\n",
            "Epoch 43 Loss 0.8003 Accuracy 0.7274\n",
            "Time taken for 1 epoch: 45.52 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.7869 Accuracy 0.7290\n",
            "Epoch 44 Batch 50 Loss 0.7773 Accuracy 0.7350\n",
            "Epoch 44 Batch 100 Loss 0.7841 Accuracy 0.7328\n",
            "Epoch 44 Loss 0.7856 Accuracy 0.7323\n",
            "Time taken for 1 epoch: 45.72 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.7458 Accuracy 0.7446\n",
            "Epoch 45 Batch 50 Loss 0.7592 Accuracy 0.7415\n",
            "Epoch 45 Batch 100 Loss 0.7676 Accuracy 0.7382\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/word-input_char-output_gen/ckpt-9\n",
            "Epoch 45 Loss 0.7693 Accuracy 0.7376\n",
            "Time taken for 1 epoch: 46.43 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.7324 Accuracy 0.7505\n",
            "Epoch 46 Batch 50 Loss 0.7428 Accuracy 0.7462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a92218fc0397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/deepcomedy/models/transformer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz4YwsF04YEI"
      },
      "source": [
        "## 4. Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQ7PnRmidiS"
      },
      "source": [
        "def generate_greedy(encoder_input, decoder_input):\n",
        "\n",
        "    # encoder_input = tf.convert_to_tensor(encoder_input)\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # decoder_input = tf.convert_to_tensor(decoder_input)\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    result = \"<GO> \"\n",
        "    tokenized_result = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # concatenate the predicted_id to the output which is given to the decoder as its input.\n",
        "        output = tf.concat(\n",
        "            [tf.cast(output, dtype=tf.int32), tf.cast(predicted_id, dtype=tf.int32)],\n",
        "            axis=-1,\n",
        "        )\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
        "\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result, tokenized_result"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2lr385ystg"
      },
      "source": [
        "def generate_topk(encoder_input, decoder_input, k=5, temperature=0.5):\n",
        "\n",
        "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
        "\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    result = \"<GO> \"\n",
        "    tokenized_result = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "    for i in range(200):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output\n",
        "        )\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask,\n",
        "        )\n",
        "\n",
        "        # select the last character from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predictions, indices = tf.math.top_k(predictions, k=k)\n",
        "\n",
        "        predictions /= temperature\n",
        "        predictions = np.squeeze(predictions, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        indices = np.squeeze(indices, axis=0)\n",
        "        pred = tf.random.categorical(predictions, num_samples=1)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        predicted_id = indices[predicted_id]\n",
        "\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        result += target_tokenizer.index_word[predicted_id.numpy()[0][0]] + \" \"\n",
        "        tokenized_result.append(predicted_id.numpy()[0][0])\n",
        "\n",
        "        if predicted_id == target_tokenizer.word_index[\"<EOT>\"]:\n",
        "            return result, tokenized_result"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5LRTshv0uG"
      },
      "source": [
        "Abbiamo provato due modi per generare:\n",
        "1. Dare all'encoder in input una terzina e ottenere la terzina successiva (come abbiamo allenato la rete a fare fondamentalmente), poi passare la terzina generata sempre all'encoder per ottenere la successiva e così via.\n",
        "1. Dare all'encoder in input uno start symbol e al decoder gli ultimi due versi della terzina generata. Il risultato dovrebbe tenere in considerazione esclusivamente il verso che ne esce fuori (TODO modificare generate greedy in modo tale che restituisca esclusivamente il next verse).\n",
        "1. TODO next provare a dare qualcosa all'encoder e al decoder contemporaneamente (es. contesto di generazione per il decoder generato dall'encoder?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtSP1D-nroO2"
      },
      "source": [
        "## Feeding the encoder the last output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRQPvAX4dlxv"
      },
      "source": [
        "def clean(x):\n",
        "  x = re.sub(r'\\| \\b', '', x)\n",
        "  x = re.sub(r'\\b \\|', '', x)\n",
        "  x = re.sub(r'\\|', '', x)\n",
        "  x = re.sub(r'[ ]+', ' ', x)\n",
        "  x = re.sub(r'\\b \\b', '', x)\n",
        "  return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Qv4LFhkdJK",
        "outputId": "8bb25d62-c9a9-464d-994f-4ad26ea8796b"
      },
      "source": [
        "encoder_input = [input_tokenizer.word_index[\"<GO>\"]]\n",
        "decoder_input = [target_tokenizer.word_index[\"<GO>\"]]\n",
        "\n",
        "generated_text, generated_tokenized = generate_greedy(encoder_input, decoder_input)\n",
        "print(generated_text)\n",
        "print(clean(generated_text))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<GO> | E <SEP> | i o <SEP> | a <SEP> | l u i : « <SEP> | S e <SEP> | t u <SEP> | v u o ’ <SEP> | c h ’ <SEP> i o <SEP> | t i <SEP> | r i e | d i <GO> | l a <SEP> | p r i | m a <SEP> | c h e <SEP> | t a n | t o <SEP> | d i | s c o r | d e | r e , <GO> | p e r <SEP> | l o <SEP> | s e | g n o r <SEP> | d e l <SEP> | m o n | d o <SEP> | s i <SEP> | r i | c h i e | d i » . <EOT> \n",
            "<GO> E <SEP> io <SEP> a <SEP> lui : « <SEP> Se <SEP> tu <SEP> vuo ’ <SEP> ch ’ <SEP> io <SEP> ti <SEP> riedi <GO> la <SEP> prima <SEP> che <SEP> tanto <SEP> discordere , <GO> per <SEP> lo <SEP> segnor <SEP> del <SEP> mondo <SEP> si <SEP> richiedi » . <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNfWEiSXminc",
        "outputId": "55afad8b-29f9-4674-a9fc-c9c1af938338"
      },
      "source": [
        "tokenized_generated = input_tokenizer.texts_to_sequences([clean(generated_text)])\n",
        "print(tokenized_generated)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 30, 1, 26, 1, 8, 1, 70, 1, 281, 1, 29, 1, 216, 1, 216, 1, 26, 1, 37, 1, 2, 6, 1, 83, 1, 5, 1, 49, 1, 2, 10, 1, 27, 1, 748, 1, 25, 1, 95, 1, 12, 1, 643, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963qqWf1nCvP",
        "outputId": "125862c4-bf8d-43b4-e63c-501d0a6ef705"
      },
      "source": [
        "generated_text_2, _ = generate_greedy(tokenized_generated[0], decoder_input)\n",
        "print(generated_text_2)\n",
        "print(clean(generated_text_2))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<GO> | E <SEP> | i o <SEP> | a <SEP> | l u i : « <SEP> | S e <SEP> | t u <SEP> | t i <SEP> | t i <SEP> | t i <SEP> | c o | t a <GO> | d i m | m i , <SEP> | s e <SEP> | t u <SEP> | l a <SEP> | m e | m o | r i a <SEP> | t e m | p o <SEP> | c e n | n o <GO> | c h e <SEP> | l ’ <SEP> a | n i | m a <SEP> | t u a <SEP> | q u e | s t i o n <SEP> | t i <SEP> | r i | t o | t a » . <EOT> \n",
            "<GO> E <SEP> io <SEP> a <SEP> lui : « <SEP> Se <SEP> tu <SEP> ti <SEP> ti <SEP> ti <SEP> cota <GO> dimmi , <SEP> se <SEP> tu <SEP> la <SEP> memoria <SEP> tempo <SEP> cenno <GO> che <SEP> l ’ <SEP> anima <SEP> tua <SEP> question <SEP> ti <SEP> ritota » . <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uj0IR3Yrrsp"
      },
      "source": [
        "## Feeding the decoder the last output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj0U2x4oLRV",
        "outputId": "f2cf5718-32da-4d43-a391-e52ce24ec547"
      },
      "source": [
        "print(generated_tokenized)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14, 1, 39, 2, 1, 5, 6, 2, 1, 4, 2, 1, 9, 15, 5, 34, 32, 2, 1, 42, 3, 2, 1, 10, 15, 2, 1, 19, 15, 6, 20, 2, 1, 12, 22, 20, 2, 5, 6, 2, 1, 10, 5, 2, 1, 8, 5, 3, 1, 13, 5, 14, 1, 9, 4, 2, 1, 17, 8, 5, 1, 16, 4, 2, 1, 12, 22, 3, 2, 1, 10, 4, 7, 1, 10, 6, 2, 1, 13, 5, 1, 11, 12, 6, 8, 1, 13, 3, 1, 8, 3, 18, 14, 1, 17, 3, 8, 2, 1, 9, 6, 2, 1, 11, 3, 1, 21, 7, 6, 8, 2, 1, 13, 3, 9, 2, 1, 16, 6, 7, 1, 13, 6, 2, 1, 11, 5, 2, 1, 8, 5, 1, 12, 22, 5, 3, 1, 13, 5, 33, 25, 24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKg4wbRSn6oJ",
        "outputId": "d98f6925-b91e-44f5-8769-17b79d61112f"
      },
      "source": [
        "generated_text_3, _ = generate_greedy(encoder_input, generated_tokenized)\n",
        "print(generated_text_2)\n",
        "print(clean(generated_text_2))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<GO> | E <SEP> | i o <SEP> | a <SEP> | l u i : « <SEP> | S e <SEP> | t u <SEP> | t i <SEP> | t i <SEP> | t i <SEP> | c o | t a <GO> | d i m | m i , <SEP> | s e <SEP> | t u <SEP> | l a <SEP> | m e | m o | r i a <SEP> | t e m | p o <SEP> | c e n | n o <GO> | c h e <SEP> | l ’ <SEP> a | n i | m a <SEP> | t u a <SEP> | q u e | s t i o n <SEP> | t i <SEP> | r i | t o | t a » . <EOT> \n",
            "<GO> E <SEP> io <SEP> a <SEP> lui : « <SEP> Se <SEP> tu <SEP> ti <SEP> ti <SEP> ti <SEP> cota <GO> dimmi , <SEP> se <SEP> tu <SEP> la <SEP> memoria <SEP> tempo <SEP> cenno <GO> che <SEP> l ’ <SEP> anima <SEP> tua <SEP> question <SEP> ti <SEP> ritota » . <EOT> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}